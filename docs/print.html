<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="getstarted/grid3_howitworks.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="getstarted/tfgrid3_getstarted.html"><strong aria-hidden="true">2.</strong> Getting Started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dashboard/portal/dashboard_portal_polkadot_widget.html"><strong aria-hidden="true">2.1.</strong> Installing Polkadot extension</a></li><li class="chapter-item expanded "><a href="dashboard/portal/dashboard_portal_polkadot_create_account.html"><strong aria-hidden="true">2.2.</strong> Create Account</a></li><li class="chapter-item expanded "><a href="dashboard/portal/dashboard_portal_ui_activation.html"><strong aria-hidden="true">2.3.</strong> Activate Account</a></li><li class="chapter-item expanded "><a href="dashboard/portal/dashboard_portal_transfer.html"><strong aria-hidden="true">2.4.</strong> Transfer TFT</a></li><li class="chapter-item expanded "><a href="dashboard/explorer/explorer_find_capacity.html"><strong aria-hidden="true">2.5.</strong> Tutorial: Find Node</a></li><li class="chapter-item expanded "><a href="weblets/weblets_vm.html"><strong aria-hidden="true">2.6.</strong> Tutorial: Deploy Your First VM</a></li></ol></li><li class="chapter-item expanded "><a href="terraform/terraform_readme.html"><strong aria-hidden="true">3.</strong> Terraform</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="terraform/terraform_install.html"><strong aria-hidden="true">3.1.</strong> Installing Terraform</a></li><li class="chapter-item expanded "><a href="terraform/terraform_basics.html"><strong aria-hidden="true">3.2.</strong> Terraform Basics</a></li><li class="chapter-item expanded "><a href="terraform/terraform_get_started.html"><strong aria-hidden="true">3.3.</strong> Tutorial</a></li><li class="chapter-item expanded "><a href="terraform/terraform_delete.html"><strong aria-hidden="true">3.4.</strong> Delete</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.5.</strong> Resources</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="terraform/resources/terraform_vm.html"><strong aria-hidden="true">3.5.1.</strong> Virtual Machine</a></li><li class="chapter-item expanded "><a href="terraform/resources/terraform_vm_gateway.html"><strong aria-hidden="true">3.5.2.</strong> Web Gateway</a></li><li class="chapter-item expanded "><a href="terraform/resources/terraform_k8s.html"><strong aria-hidden="true">3.5.3.</strong> Kubernetes cluster</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="terraform/resources/terraform_k8s_demo.html"><strong aria-hidden="true">3.5.3.1.</strong> Demo</a></li></ol></li><li class="chapter-item expanded "><a href="terraform/resources/terraform_zdb.html"><strong aria-hidden="true">3.5.4.</strong> ZDB</a></li><li class="chapter-item expanded "><a href="terraform/resources/terraform_qsfs.html"><strong aria-hidden="true">3.5.5.</strong> Quantum Filesystem</a></li><li class="chapter-item expanded "><a href="terraform/resources/terraform_caprover.html"><strong aria-hidden="true">3.5.6.</strong> CapRover</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.6.</strong> Advanced</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="terraform/advanced/terraform_provider.html"><strong aria-hidden="true">3.6.1.</strong> Terraform Provider</a></li><li class="chapter-item expanded "><a href="terraform/advanced/terraform_provisioners.html"><strong aria-hidden="true">3.6.2.</strong> Terraform Provisioners</a></li><li class="chapter-item expanded "><a href="terraform/advanced/terraform_mounts.html"><strong aria-hidden="true">3.6.3.</strong> Mounts</a></li><li class="chapter-item expanded "><a href="terraform/advanced/terraform_capacity_planning.html"><strong aria-hidden="true">3.6.4.</strong> Capacity planning</a></li><li class="chapter-item expanded "><a href="terraform/advanced/terraform_updates.html"><strong aria-hidden="true">3.6.5.</strong> Updates</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_readme.html"><strong aria-hidden="true">4.</strong> Javascript Client</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="javascript/grid3_javascript_installation.html"><strong aria-hidden="true">4.1.</strong> Installation</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_loadclient.html"><strong aria-hidden="true">4.2.</strong> Loading client</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_vm.html"><strong aria-hidden="true">4.3.</strong> Deploy a VM</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_capacity_planning.html"><strong aria-hidden="true">4.4.</strong> Capacity planning</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_vms.html"><strong aria-hidden="true">4.5.</strong> Deploy multiple VMs</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_caprover.html"><strong aria-hidden="true">4.6.</strong> Deploy CapRover</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_vm_gateways.html"><strong aria-hidden="true">4.7.</strong> Gateways</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_kubernetes.html"><strong aria-hidden="true">4.8.</strong> Deploy a Kubernetes cluster</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_zdb.html"><strong aria-hidden="true">4.9.</strong> Deploy a ZDB</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_qsfs.html"><strong aria-hidden="true">4.10.</strong> QSFS</a></li><li class="chapter-item expanded "><a href="javascript/grid3_javascript_kvstore.html"><strong aria-hidden="true">4.11.</strong> Key Value Store</a></li></ol></li><li class="chapter-item expanded "><a href="weblets/weblets_home.html"><strong aria-hidden="true">5.</strong> Weblets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="weblets/weblets_profile_manager.html"><strong aria-hidden="true">5.1.</strong> Profile Manager</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="weblets/profile_manager_unlock.html"><strong aria-hidden="true">5.1.1.</strong> Profile Manager Unlock</a></li></ol></li><li class="chapter-item expanded "><a href="weblets/weblets_deployments_list.html"><strong aria-hidden="true">5.2.</strong> Deployments List</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.3.</strong> Basic Environments</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="weblets/weblets_vm.html"><strong aria-hidden="true">5.3.1.</strong> Virtual Machine</a></li><li class="chapter-item expanded "><a href="weblets/weblets_k8s.html"><strong aria-hidden="true">5.3.2.</strong> Kubernetes</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.4.</strong> Ready Community Solutions</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="weblets/weblets_caprover.html"><strong aria-hidden="true">5.4.1.</strong> Caprover</a></li><li class="chapter-item expanded "><a href="weblets/weblets_funkwhale.html"><strong aria-hidden="true">5.4.2.</strong> Funkwhale</a></li><li class="chapter-item expanded "><a href="weblets/weblets_peertube.html"><strong aria-hidden="true">5.4.3.</strong> Peertube</a></li><li class="chapter-item expanded "><a href="weblets/weblets_taiga.html"><strong aria-hidden="true">5.4.4.</strong> Taiga</a></li><li class="chapter-item expanded "><a href="weblets/weblets_owncloud.html"><strong aria-hidden="true">5.4.5.</strong> Owncloud</a></li><li class="chapter-item expanded "><a href="weblets/weblets_discourse.html"><strong aria-hidden="true">5.4.6.</strong> Discourse</a></li><li class="chapter-item expanded "><a href="weblets/weblets_mattermost.html"><strong aria-hidden="true">5.4.7.</strong> Mattermost</a></li><li class="chapter-item expanded "><a href="weblets/weblets_presearch.html"><strong aria-hidden="true">5.4.8.</strong> Presearch</a></li><li class="chapter-item expanded "><a href="weblets/weblets_casper.html"><strong aria-hidden="true">5.4.9.</strong> CasperLabs</a></li><li class="chapter-item expanded "><a href="weblets/weblets_nodepilot.html"><strong aria-hidden="true">5.4.10.</strong> Node Pilot</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="dashboard/dashboard.html"><strong aria-hidden="true">6.</strong> Dashboard</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dashboard/explorer/explorer_home.html"><strong aria-hidden="true">6.1.</strong> Dasboard Explorer</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dashboard/explorer/explorer_find_capacity.html"><strong aria-hidden="true">6.1.1.</strong> find capacity</a></li><li class="chapter-item expanded "><a href="dashboard/explorer/explorer_graphql_intro.html"><strong aria-hidden="true">6.1.2.</strong> A Glimpse into GraphQL</a></li></ol></li><li class="chapter-item expanded "><a href="dashboard/portal/dashboard_portal_home.html"><strong aria-hidden="true">6.2.</strong> TFChain Portal</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dashboard/portal/dashboard_portal_ui_farming.html"><strong aria-hidden="true">6.2.1.</strong> Activate your farmer account and nodes on TFChain</a></li><li class="chapter-item expanded "><a href="dashboard/portal/dashboard_portal_dedicated_nodes.html"><strong aria-hidden="true">6.2.2.</strong> Dedicated Nodes</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="flist_hub/flist_hub.html"><strong aria-hidden="true">7.</strong> Flist Hub</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="manual3_iac/grid3_supported_flists.html"><strong aria-hidden="true">7.1.</strong> Supported FLists</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h2 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h2>
<p>Here we will try to describe the platform in a nutshell, so we understand the mechanics and clear the terminology a little bit.</p>
<h3 id="capacity"><a class="header" href="#capacity">Capacity</a></h3>
<p>Threefold grid is providing the <a href="https://dashboard.grid.tf/explorer/statistics">largest</a> decentralized, peer 2 peer infrastructure. </p>
<h3 id="the-process"><a class="header" href="#the-process">The process</a></h3>
<p>The idea is similar to electricity grid. People consume electricity, and sometimes they can provide electricity. It's almost the same. </p>
<h4 id="tfchain"><a class="header" href="#tfchain">TFchain</a></h4>
<p>TFChain is a blockchain orchestrating the interactions on the grid</p>
<ul>
<li>Users registration</li>
<li>Farms Managment (registration and IP management)</li>
<li>Money transfers</li>
<li>Billing and consumptions reports</li>
</ul>
<p>And more.</p>
<h4 id="the-farmers"><a class="header" href="#the-farmers">The farmers</a></h4>
<p>People (we call them farmers) provide internet capacity using one or mode 3Nodes. these nodes are registered on <code>TFChain</code> in what we call a &quot;farm&quot;. </p>
<p>The farm is the logical grouping of nodes. e.g you can have 2 3Nodes in Belgium, and 5 in France. so to logically group them you can create a farm in Belgium with a specific ID that you can use in for the nodes in Belgium and create another farm with another ID for the nodes in france. </p>
<blockquote>
<p>Note: most of the time the grouping is also done by the physical place</p>
</blockquote>
<h4 id="3node"><a class="header" href="#3node">3Node</a></h4>
<p>it's a computer really. That 3Node runs a very specific software <code>zero-os</code> sometimes called <code>zos</code>. <code>Zero-OS</code> is an autonomous operating system designed to expose raw compute, storage and network capacity.</p>
<p>The <code>Zero-OS</code> handles 
-The workloads provisioning, e.g starting a new container or a VM, starting a <code>0-db</code>,</p>
<ul>
<li>The networking for the workloads</li>
<li>The lifecycle management of the workloads running on </li>
<li>The system upgrades</li>
<li>Consumption reporting for the billing to happen on the <code>TFChain</code></li>
</ul>
<h4 id="provisioning"><a class="header" href="#provisioning">Provisioning</a></h4>
<p>Now that's very cool, we now already know about <code>TFChain</code>, <code>Farmers</code>, <code>3Nodes</code> and <code>Zero-OS</code>, but still one major thing is still missing is how can we send a provisioning request to <code>Zero-OS</code>?</p>
<p>The provisioning process is done mainly using two tools which you can read about in this manual; </p>
<ul>
<li><a href="getstarted/../terraform/terraform_readme.html">Terraform</a></li>
<li><a href="getstarted/../javascript/grid3_javascript_readme.html">Typescript Client</a></li>
</ul>
<p>So what happens is we build the deployment information (what workloads we want to deploy) and create a contract(s) on the blockchain between us and a 3Node that will fulfill the provisioning of the resources needed in the contract for us, and automatically the node reports to the blockchain periodically to deduct the money from the user account based on the consumption.</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="getstarted/././img/endlessscalable.png" alt=" " /></p>
<h2 id="this-guide-will-walk-you-through"><a class="header" href="#this-guide-will-walk-you-through">This guide will walk you through:</a></h2>
<ul>
<li><a href="getstarted/../dashboard/portal/dashboard_portal_polkadot_widget.html">Install Polkadot Widget</a></li>
<li><a href="getstarted/../dashboard/portal/dashboard_polkadot_create_account.html">Create and Activate account</a></li>
<li><a href="getstarted/../dashboard/portal/dashboard_ui_activation.html">Activate Twin</a></li>
<li><a href="getstarted/../dashboard/portal/dashboard_ui_tokens.html">TFT to TFChain</a></li>
<li><a href="getstarted/../dashboard/explorer/explorer_find_capacity.html">Find Node</a></li>
<li><a href="getstarted/../weblets/weblets_vm.html">Deploy Your First VM</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="polkadot-extension"><a class="header" href="#polkadot-extension">Polkadot Extension</a></h1>
<p><img src="dashboard/portal/../img/polkadot_extension_.png" alt=" " /></p>
<h2 id="what-does-it-do"><a class="header" href="#what-does-it-do">What does it do</a></h2>
<p>This browser extension manages accounts and allows the signing of transactions with those accounts. It does not inject providers for use by dapps at this early point, nor does it perform wallet functions, e.g send funds.</p>
<h2 id="install"><a class="header" href="#install">Install</a></h2>
<p>Before using the tfchain_portal, you need to install a Polkadot extension.</p>
<p>It can be installed from <a href="https://polkadot.js.org/extension/">here</a>.</p>
<p>Extensions are available for the Google Chrome and Firefox browsers.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-an-account-for-tfchain-using-polkadot-ui"><a class="header" href="#create-an-account-for-tfchain-using-polkadot-ui">Create an Account for TFChain using Polkadot UI</a></h1>
<blockquote>
<p>make sure <a href="dashboard/portal/./dashboard_portal_polkadot_widget.html">extension is installed</a></p>
</blockquote>
<p>You will be asked to create a new account or to recover an existing account.</p>
<p>When creating a new account, the mnemonic seed words will be shown, which can be used to recover your wallet.</p>
<blockquote>
<p>Keep them in a safe place, they give access to the TFTs that you will hold on the TFGrid.</p>
</blockquote>
<p><img src="dashboard/portal/../img/dashboard_portal_create_account_1.png" alt=" " title=":size=400" /></p>
<p>In the next screen, you can choose a network. Select <code>Allow use on any chain</code>. Give your account a name and a password for easy access from your local hardware. With this information, the account can be generated.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_create_account_2.png" alt=" " title=":size=400" /></p>
<p>Once the account is generated, activate it by accepting the Terms &amp; Conditions. You will get prompted to sign in the Polkadot extension.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_terms_conditions.png" alt=" " title=":size=600" /></p>
<blockquote>
<blockquote>
<p>PLEASE DO NOT FORGET TO REMEMBER YOUR MNEMONIC WORDS, YOU WILL NEED THEM IN NEXT STEPS.</p>
</blockquote>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tf-chain-portal-activation"><a class="header" href="#tf-chain-portal-activation">TF-Chain Portal Activation</a></h1>
<h2 id="go-to-the-portal"><a class="header" href="#go-to-the-portal">Go to the Portal</a></h2>
<ul>
<li><a href="https://dashboard.dev.grid.tf/">Devnet</a></li>
<li><a href="https://dashboard.qa.grid.tf/">Qanet</a></li>
<li><a href="https://dashboard.test.grid.tf/">Testnet</a></li>
<li><a href="https://dashboard.grid.tf/">Mainnet</a></li>
</ul>
<h2 id="create-a-twin"><a class="header" href="#create-a-twin">Create a Twin</a></h2>
<p>A twin is a unique identifier of where you can be found digitally. Click on <code>CREATE</code> to register your Twin on TF-Chain.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_create_twin.png" alt=" " title=":size=600" /></p>
<p>If you want to connect your TwinID to the planetary network, fill in the IPv6 address you get from Yggdrasil.
If not, no need to fill in the ip address, use <code>127.0.0.1</code>.</p>
<p>This address is reserved for future usage once the digital twin will launch.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_fill_ipv6.png" alt=" " title=":size=400" /></p>
<p>Submit the transaction using the password selected when creating the account.</p>
<p>The Twin IP can be modified at any moment.</p>
<h2 id="required-result"><a class="header" href="#required-result">Required result</a></h2>
<p>Shoud see something like:</p>
<p><img src="dashboard/portal/../img/dashboard_portal_twin_created.png" alt=" " title=":size=600" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tf-chain-portal-transfer"><a class="header" href="#tf-chain-portal-transfer">TF-Chain Portal Transfer</a></h1>
<h2 id="transfer-tft-between-tfchain-accounts"><a class="header" href="#transfer-tft-between-tfchain-accounts">Transfer TFT between TFChain accounts</a></h2>
<p>The portal can be used to transfer TFTs between 2 accounts that exist on the same chain.</p>
<blockquote>
<p>Remark: testnet and mainnet both have the same TFTs but as the 2 chains are different, there is no way to do a direct transfer between accounts on testnet and on mainnet.</p>
</blockquote>
<p><img src="dashboard/portal/../img/dashboard_portal_transfer.png" alt=" " title=":size=600" /></p>
<p>Click on <code>Transfer</code> in the sidebar to get the transfer screen.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_transfer_detail.png" alt=" " title=":size=300" /></p>
<p>Fill in the recipient address, the amount of tokens to transfer, and click on <code>Submit</code>. Sign in the extension screen, and you're done !</p>
<p>There is no transfer fee, just a signing fee of 0.001 TFT.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="find-capacity-on-the-grid"><a class="header" href="#find-capacity-on-the-grid">Find Capacity on the Grid</a></h1>
<p><img src="dashboard/explorer/../img/explorer_basics_.png" alt=" " />
<img src="dashboard/explorer/../img/explorer_basics_2.png" alt=" " /></p>
<p>Before you can deploy on the TFGrid you need to find a node which fits your requirements, in terms of location, available resources, ... .</p>
<h2 id="use-the-explorer-you-need-for-your-chosen-tfgrid-net"><a class="header" href="#use-the-explorer-you-need-for-your-chosen-tfgrid-net">Use the explorer you need for your chosen tfgrid net</a></h2>
<p>!!!include:explorer_list</p>
<h2 id="use-filter-to-select-country--requirement-for-public-ips"><a class="header" href="#use-filter-to-select-country--requirement-for-public-ips">Use filter to select Country &amp; Requirement for Public IP's</a></h2>
<p><img src="dashboard/explorer/../img/explorer_find_country_pubip.png" alt=" " /></p>
<p>In this case 17 nodes are found with that requirement.</p>
<p>Do note you have to put</p>
<ul>
<li><code>&gt;=1</code> in the publicip's part, means at least one public ip address.</li>
</ul>
<p>Remember the ID, this is the number you will need to specify your node.</p>
<h2 id="important"><a class="header" href="#important">Important</a></h2>
<ul>
<li>make sure there is enough capacity on the node for your workload requirements</li>
<li>make sure you use the right explorer, each net has a different explorer.</li>
</ul>
<h2 id="node-details"><a class="header" href="#node-details">Node details</a></h2>
<p><img src="dashboard/explorer/../img/node_detail_.png" alt=" " />
<img src="dashboard/explorer/../img/node_detail_1.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtual-machine"><a class="header" href="#virtual-machine">Virtual Machine</a></h1>
<p>Deploy a new virtual machine on the Threefold Grid</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a> </li>
<li>Click on the <strong>Virtual Machine</strong> tab</li>
</ul>
<p><strong>Process</strong> : </p>
<p><img src="weblets/../weblets/img/new_vm1.png" alt=" " /></p>
<ul>
<li>Fill in the instance name: it's used to reference the VM in the future.</li>
<li>Choose the image from the drop down (e.g Alpine, Ubuntu) or you can click on <code>Other</code> and manually specify the flist URL and the entrypoint.</li>
<li><code>Public IPv4</code> flag gives the virtual machine a Public IPv4</li>
<li><code>Public IPv6</code> flag gives the virtual machine a Public IPv6</li>
<li><code>Planetary Network</code> to connect the Virtual Machine to Planetary network</li>
<li>Choose the node to deploy on which can be
<ul>
<li>Manual: where you specify the node id yourself</li>
<li>Automatic: Suggests nodes list based on search criteria e.g <code>country</code>, <code>farm</code>, capacity..</li>
</ul>
</li>
</ul>
<p><img src="weblets/../weblets/img/new_vm2.png" alt=" " />
Clicking on enviornment allows you to define environment variables to pass to the virtual machine. </p>
<blockquote>
<p>Note the Public SSH key in the profile is automatically used as variable <code>SSH_KEY</code> passed to all Virtual Machines </p>
</blockquote>
<p><img src="weblets/../weblets/img/new_vm3.png" alt=" " />
You can attach one or more disks to the Virtual Machine by clicking on the Disks tab and the plus <code>+</code> sign and specify the following parameters</p>
<ul>
<li>Disk name </li>
<li>Disk size</li>
<li>Mount point</li>
</ul>
<p>in the bottom of the page you can see a list of all of the virual machines you deployed. you can click on <code>Show details</code> for more details</p>
<p><img src="weblets/../weblets/img/weblet_vm5.png" alt=" " />
You can also go to JSON tab for full details
<img src="weblets/../weblets/img/weblet_vm6.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/./advanced/img/terraform_.png" alt=" " /></p>
<h1 id="terraform-intro"><a class="header" href="#terraform-intro">Terraform Intro</a></h1>
<p><img src="terraform/../terraform/img//terraform_works.png" alt=" " /></p>
<p>Threefold loves Open Source! In v3.0 we are integrating one of the most popular 'Infrastructure as Code' (IaC) tools of the cloud industry, <a href="https://terraform.io">Terraform</a>. Utilizing the Threefold grid v3 using Terraform gives a consistent workflow and a familiar experience for everyone coming from different background. Terraform describes the state desired of how the deployment should look like instead of imperatively describing the low level details and the mechanics of how things should be glued together.</p>
<h2 id="features"><a class="header" href="#features">Features</a></h2>
<ul>
<li>All basic primitives from ThreeFold grid can be deployed, which is a lot.</li>
<li>Terraform can destroy a deployment</li>
<li>Terraform shows all the outputs</li>
</ul>
<h2 id="what-is-not-supported"><a class="header" href="#what-is-not-supported">What is not supported.</a></h2>
<ul>
<li>we don't support updates/upgrades, if you want a change you need to destroy a deployment &amp; re-create</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/./img//terraform_install.png" alt=" " /></p>
<h3 id="terraform-install"><a class="header" href="#terraform-install">Terraform Install</a></h3>
<p>Please note that for now our <a href="https://github.com/threefoldtech/terraform-provider-grid">Terraform plugin</a> is supported on Linux and MacOS. Windows</p>
<p>Get Terraform from the <a href="https://www.terraform.io/downloads.html">download page</a> on the official site, or install using you system's package manager. please check the <a href="https://learn.hashicorp.com/tutorials/terraform/install-cli">installation manual</a></p>
<p>There's no need to specifically install the ThreeFold Terraform plugin. Terraform will automatically load it from an online directory according to instruction within the deployment file.</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/./advanced/img//terraform_.png" alt=" " /></p>
<h1 id="terraform-basics"><a class="header" href="#terraform-basics">terraform basics</a></h1>
<h2 id="requirements"><a class="header" href="#requirements">Requirements</a></h2>
<!-- please make sure to read [What you need to know before getting started](grid3_developer_basics) -->
<p>Make sure following is done:</p>
<ul>
<li><a href="terraform/../getstarted/tfgrid3_getstarted.html">Get started with your account on TFGrid</a></li>
<li><a href="terraform/../terraform/terraform_install.html">Install Terraform</a></li>
</ul>
<h2 id="prepare"><a class="header" href="#prepare">Prepare</a></h2>
<ul>
<li>
<p>make a directory for your project <code>mkdir myfirstproject</code></p>
</li>
<li>
<p><code>cd myfirstproject</code></p>
</li>
<li>
<p>create <code>main.tf</code>  &lt;- creates the terraform main file </p>
</li>
</ul>
<pre><code class="language-terraform">terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
    mnemonics = &quot;FROM THE CREATE TWIN STEP&quot;
    network = &quot;dev&quot; # or test to use testnet
}

</code></pre>
<ul>
<li>to initialize the repo <code>terraform init</code></li>
</ul>
<h2 id="basic-commands"><a class="header" href="#basic-commands">basic commands</a></h2>
<ul>
<li>to execute a terraform file <code>terraform apply -parallelism=1</code></li>
<li>to see the output <code>terraform output</code>
<ul>
<li>can be used to get the relevant output variables e.g public ip, planetary network ip, wireguard configurations .. </li>
</ul>
</li>
<li>to see the state <code>terraform show</code></li>
<li>to destroy <code>terraform destroy -parallelism=1</code></li>
</ul>
<h2 id="find-your-node"><a class="header" href="#find-your-node">Find your Node</a></h2>
<p>The choice of the node is up to the user. They need to do the capacity planning.</p>
<p>Make sure you choose a node which has enough capacity and is available (up and running).</p>
<blockquote>
<p>Check <a href="terraform/../dashboard/explorer/explorer_home.html">Exploring Capacity</a> to know which nodes fits your deployment criteria.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/./advanced/img//terraform_.png" alt=" " /></p>
<h2 id="using-terraform"><a class="header" href="#using-terraform">Using Terraform</a></h2>
<ul>
<li>make a directory for your project <code>mkdir myfirstproject</code></li>
<li><code>cd myfirstproject</code></li>
<li>create <code>main.tf</code>  &lt;- creates the terraform main file </li>
</ul>
<h2 id="create"><a class="header" href="#create">Create</a></h2>
<p>to start the deployment <code>terraform init &amp;&amp; terraform apply -parallelism=1</code></p>
<h2 id="destroying"><a class="header" href="#destroying">Destroying</a></h2>
<p>can be done using <code>terraform destroy -parallelism=1</code></p>
<p>And that's it!! you managed to deploy 2 VMs on the threefold grid v3</p>
<h2 id="how-to-use-a-terraform-file"><a class="header" href="#how-to-use-a-terraform-file">How to use a Terraform File</a></h2>
<h3 id="initializing-the-provider"><a class="header" href="#initializing-the-provider">Initializing the provider</a></h3>
<p>In terraform's global section </p>
<pre><code class="language-terraform">terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

</code></pre>
<p>Providers can have different arguments e.g using which identity when deploying, which substrate network to create contracts on, .. etc. This can be done in the provider section</p>
<pre><code class="language-terraform">provider &quot;grid&quot; {
    mnemonics = &quot;FROM THE CREATE TWIN STEP&quot; 
    network = &quot;dev&quot; # or test to use testnet

}
</code></pre>
<p>Please note you can leave its content empty and export everything as environment variables</p>
<pre><code>export MNEMONICS=&quot;.....&quot;
export NETWORK=&quot;.....&quot;

</code></pre>
<p>For more info see <a href="terraform/./advanced/terraform_provider.html">Provider Manual</a></p>
<h3 id="output-section"><a class="header" href="#output-section">output section</a></h3>
<pre><code class="language-terraform">output &quot;wg_config&quot; {
    value = grid_network.net1.access_wg_config
}
output &quot;node1_vm1_ip&quot; {
    value = grid_deployment.d1.vms[0].ip
}
output &quot;node1_vm2_ip&quot; {
    value = grid_deployment.d1.vms[1].ip
}
output &quot;public_ip&quot; {
    value = grid_deployment.d1.vms[0].computedip
}

</code></pre>
<p>Output parameters show what has been done:</p>
<ul>
<li>the overlay wireguard network configurations</li>
<li>the private IPs of the VMs</li>
<li>the public IP of the VM <code>exposed under computedip</code> </li>
</ul>
<h3 id="which-flists-to-use-in-vm"><a class="header" href="#which-flists-to-use-in-vm">Which flists to use in VM</a></h3>
<p>see <a href="terraform/manual3_iac/grid3_supported_flists.html">list of flists</a></p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/./advanced/img/terraform_.png" alt=" " /></p>
<h1 id="terraform-delete"><a class="header" href="#terraform-delete">Terraform Delete</a></h1>
<h2 id="use-terraform"><a class="header" href="#use-terraform">Use Terraform</a></h2>
<p>go in directory of your terraform deployment</p>
<p>do:</p>
<pre><code>terraform destroy
</code></pre>
<h2 id="use-weblets-to-delete-from-blockchains"><a class="header" href="#use-weblets-to-delete-from-blockchains">Use Weblets To Delete From Blockchains</a></h2>
<p>go to <a href="https://play.dev.grid.tf/#/contractslist">https://play.dev.grid.tf/#/contractslist</a></p>
<p>or</p>
<p><a href="https://play.grid.tf/#/contractslist">https://play.grid.tf/#/contractslist</a></p>
<p><img src="terraform/./img/weblets_contracts.png" alt=" " /></p>
<p>select the contracts you want to cancel.</p>
<p>There is a select all button this makes sure your account you use has nothing left</p>
<blockquote>
<p>Its not always easy to see what to delete, this feature is there as a last resort. We will improve visibility in future.</p>
</blockquote>
<h2 id="easier-management-of-resources-tip"><a class="header" href="#easier-management-of-resources-tip">Easier management of resources (TIP)</a></h2>
<p>Use multiple accounts on TFChain, group your resources per account. </p>
<p>This gives you following benefits</p>
<ul>
<li>more control over TFT spending</li>
<li>easier to delete all your contracts</li>
<li>less chance to make mistakes</li>
<li>can use an account to share access with multiple people</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/resources/../advanced/img/terraform_.png" alt=" " /></p>
<h2 id="vm-deployment"><a class="header" href="#vm-deployment">VM Deployment</a></h2>
<pre><code class="language-terraform">terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
    mnemonics = &quot;FROM THE CREATE TWIN STEP&quot; 
    network = &quot;dev&quot; # or test to use testnet
}

resource &quot;grid_network&quot; &quot;net1&quot; {
    nodes = [2, 4]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network&quot;
    description = &quot;newer network&quot;
    add_wg_access = true
}
resource &quot;grid_deployment&quot; &quot;d1&quot; {
  node = 2
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, 2, &quot;&quot;)
  vms {
    name = &quot;vm1&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 1
    publicip = true
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    env_vars = {
      SSH_KEY =&quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCeq1MFCQOv3OCLO1HxdQl8V0CxAwt5AzdsNOL91wmHiG9ocgnq2yipv7qz+uCS0AdyOSzB9umyLcOZl2apnuyzSOd+2k6Cj9ipkgVx4nx4q5W1xt4MWIwKPfbfBA9gDMVpaGYpT6ZEv2ykFPnjG0obXzIjAaOsRthawuEF8bPZku1yi83SDtpU7I0pLOl3oifuwPpXTAVkK6GabSfbCJQWBDSYXXM20eRcAhIMmt79zo78FNItHmWpfPxPTWlYW02f7vVxTN/LUeRFoaNXXY+cuPxmcmXp912kW0vhK9IvWXqGAEuSycUOwync/yj+8f7dRU7upFGqd6bXUh67iMl7 ahmed@ahmedheaven&quot;
    }
  }
  vms {
    name = &quot;anothervm&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 1
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    env_vars = {
      SSH_KEY =&quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCeq1MFCQOv3OCLO1HxdQl8V0CxAwt5AzdsNOL91wmHiG9ocgnq2yipv7qz+uCS0AdyOSzB9umyLcOZl2apnuyzSOd+2k6Cj9ipkgVx4nx4q5W1xt4MWIwKPfbfBA9gDMVpaGYpT6ZEv2ykFPnjG0obXzIjAaOsRthawuEF8bPZku1yi83SDtpU7I0pLOl3oifuwPpXTAVkK6GabSfbCJQWBDSYXXM20eRcAhIMmt79zo78FNItHmWpfPxPTWlYW02f7vVxTN/LUeRFoaNXXY+cuPxmcmXp912kW0vhK9IvWXqGAEuSycUOwync/yj+8f7dRU7upFGqd6bXUh67iMl7 ahmed@ahmedheaven&quot;
    }
  }
}
output &quot;wg_config&quot; {
    value = grid_network.net1.access_wg_config
}
output &quot;node1_vm1_ip&quot; {
    value = grid_deployment.d1.vms[0].ip
}
output &quot;node1_vm2_ip&quot; {
    value = grid_deployment.d1.vms[1].ip
}
output &quot;public_ip&quot; {
    value = grid_deployment.d1.vms[0].computedip
}


</code></pre>
<h3 id="describing-the-overlay-network-for-the-project"><a class="header" href="#describing-the-overlay-network-for-the-project">describing the overlay network for the project</a></h3>
<pre><code class="language-terraform">resource &quot;grid_network&quot; &quot;net1&quot; {
    nodes = [2, 4]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network&quot;
    description = &quot;some network&quot;
    add_wg_access = true
}
</code></pre>
<p>We tell terraform we will have a network spanning two nodes <code>having the node IDs 2 and 4</code> using the IP Range <code>10.1.0.0/16</code> and add wireguard access for this network</p>
<h3 id="describing-the-deployment"><a class="header" href="#describing-the-deployment">describing the deployment</a></h3>
<pre><code class="language-terraform">resource &quot;grid_deployment&quot; &quot;d1&quot; {
  node = 2
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, 2, &quot;&quot;)
  vms {
    name = &quot;vm1&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 1
    publicip = true
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    env_vars = {
      SSH_KEY =&quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCeq1MFCQOv3OCLO1HxdQl8V0CxAwt5AzdsNOL91wmHiG9ocgnq2yipv7qz+uCS0AdyOSzB9umyLcOZl2apnuyzSOd+2k6Cj9ipkgVx4nx4q5W1xt4MWIwKPfbfBA9gDMVpaGYpT6ZEv2ykFPnjG0obXzIjAaOsRthawuEF8bPZku1yi83SDtpU7I0pLOl3oifuwPpXTAVkK6GabSfbCJQWBDSYXXM20eRcAhIMmt79zo78FNItHmWpfPxPTWlYW02f7vVxTN/LUeRFoaNXXY+cuPxmcmXp912kW0vhK9IvWXqGAEuSycUOwync/yj+8f7dRU7upFGqd6bXUh67iMl7 ahmed@ahmedheaven&quot;
    }
  }
  vms {
    name = &quot;anothervm&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 1
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    env_vars = {
      SSH_KEY =&quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCeq1MFCQOv3OCLO1HxdQl8V0CxAwt5AzdsNOL91wmHiG9ocgnq2yipv7qz+uCS0AdyOSzB9umyLcOZl2apnuyzSOd+2k6Cj9ipkgVx4nx4q5W1xt4MWIwKPfbfBA9gDMVpaGYpT6ZEv2ykFPnjG0obXzIjAaOsRthawuEF8bPZku1yi83SDtpU7I0pLOl3oifuwPpXTAVkK6GabSfbCJQWBDSYXXM20eRcAhIMmt79zo78FNItHmWpfPxPTWlYW02f7vVxTN/LUeRFoaNXXY+cuPxmcmXp912kW0vhK9IvWXqGAEuSycUOwync/yj+8f7dRU7upFGqd6bXUh67iMl7 ahmed@ahmedheaven&quot;
    }
  }
}
</code></pre>
<p>It's bit long for sure but let's try to dissect it a bit </p>
<pre><code class="language-terraform">  node = 2
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, 2, &quot;&quot;)
</code></pre>
<ul>
<li><code>node=2</code> means this deployment will happen on node with id <code>2</code>. Please note the choice of the node is completely up to the user at this point. They need to do the capacity planning. Check <a href="terraform/resources/../../dashboard/explorer/explorer_home.html">Exploring Capacity</a> to know which nodes fits your deployment criteria.</li>
<li><code>network_name</code> which network to deploy our project on, and here  we choose the <code>name</code> of network <code>net1</code></li>
<li><code>ip_range</code> here we <a href="https://www.terraform.io/docs/language/functions/lookup.html">lookup</a> the iprange of node <code>2</code> and initially load it with <code>&quot;&quot;</code></li>
</ul>
<blockquote>
<p>Advannced note: Direct map access fails during the planning if the key doesn't exist which happens in cases like adding a node to the network and a new deployment on this node. So it's replaced with this to make a default empty value to pass the planning validation and it's validated anyway inside the plugin.</p>
</blockquote>
<h2 id="which-flists-to-use"><a class="header" href="#which-flists-to-use">Which flists to use</a></h2>
<p>see <a href="terraform/resources/../../manual3_iac/grid3_supported_flists.html">list of flists</a></p>
<h2 id="remark-multiple-vms"><a class="header" href="#remark-multiple-vms">remark multiple VMs</a></h2>
<p>in terraform you can define items of a list like the following</p>
<pre><code>listname {

}
listname {

}
</code></pre>
<p>So to add a VM </p>
<pre><code class="language-terraform">  vms {
    name = &quot;vm1&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 1
    publicip = true
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    env_vars = {
      SSH_KEY =&quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCeq1MFCQOv3OCLO1HxdQl8V0CxAwt5AzdsNOL91wmHiG9ocgnq2yipv7qz+uCS0AdyOSzB9umyLcOZl2apnuyzSOd+2k6Cj9ipkgVx4nx4q5W1xt4MWIwKPfbfBA9gDMVpaGYpT6ZEv2ykFPnjG0obXzIjAaOsRthawuEF8bPZku1yi83SDtpU7I0pLOl3oifuwPpXTAVkK6GabSfbCJQWBDSYXXM20eRcAhIMmt79zo78FNItHmWpfPxPTWlYW02f7vVxTN/LUeRFoaNXXY+cuPxmcmXp912kW0vhK9IvWXqGAEuSycUOwync/yj+8f7dRU7upFGqd6bXUh67iMl7 ahmed@ahmedheaven&quot;
    }
  }
</code></pre>
<ul>
<li>We give it a name within our deployment <code>vm1</code></li>
<li><code>flist</code> is used to  define the <a href="threefold:zos_fs">flist</a> to run within the VM. Check the <a href="terraform/resources/grid3_supported_flists">supported flists</a></li>
<li><code>cpu</code> and <code>memory</code> are used to define the cpu and memory</li>
<li><code>publicip</code> is usued to define if it requires a public IP or not</li>
<li><code>entrypoint</code> is used define the entrypoint which in most of the cases in <code>/sbin/zinit init</code>, but in case of flists based on vms it can be specific to each flist</li>
<li><code>env_vars</code> are used to define te environment variables, in this example we define <code>SSH_KEY</code> to authorize me accessing the machine</li>
</ul>
<p>Here we say we will have this deployment on node with <code>twin ID 2</code> using the overlay network defined from before <code>grid_network.net1.name</code> and use the ip range allocated to that specific node <code>2</code></p>
<p>The file describes only the desired state which is <code>a deployment of two VMs and their specifications in terms of cpu and memory, and some environment variables e.g sshkey to ssh into the machine</code></p>
<h2 id="reference"><a class="header" href="#reference">Reference</a></h2>
<p>A complete list of VM workload parameters can be found <a href="https://github.com/threefoldtech/terraform-provider-grid/blob/development/docs/resources/deployment.md#nested-schema-for-vms">here</a>.</p>
<pre><code>terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
}

resource &quot;grid_network&quot; &quot;net1&quot; {
    nodes = [8]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network&quot;
    description = &quot;newer network&quot;
    add_wg_access = true
}
resource &quot;grid_deployment&quot; &quot;d1&quot; {
  node = 8
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, 8, &quot;&quot;)
  vms {
    name = &quot;vm1&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 2 
    publicip = true
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    env_vars = {
      SSH_KEY = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;
    }
    planetary = true
  }
  vms {
    name = &quot;anothervm&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 1
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    env_vars = {
      SSH_KEY = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;
    }
  }
}
output &quot;wg_config&quot; {
    value = grid_network.net1.access_wg_config
}
output &quot;node1_zmachine1_ip&quot; {
    value = grid_deployment.d1.vms[0].ip
}
output &quot;node1_zmachine2_ip&quot; {
    value = grid_deployment.d1.vms[1].ip
}
output &quot;public_ip&quot; {
    value = grid_deployment.d1.vms[0].computedip
}

output &quot;ygg_ip&quot; {
    value = grid_deployment.d1.vms[0].ygg_ip
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/resources/../advanced/img/terraform_.png" alt=" " /></p>
<h1 id="terraform-web-gateway-with-vm"><a class="header" href="#terraform-web-gateway-with-vm">Terraform Web Gateway With VM</a></h1>
<h2 id="expose-with-prefix"><a class="header" href="#expose-with-prefix">expose with prefix</a></h2>
<p>A complete list of gateway name workload parameters can be found <a href="https://github.com/threefoldtech/terraform-provider-grid/blob/development/docs/resources/name_proxy.md">here</a>.</p>
<pre><code> terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
}

# this data source is used to break circular dependency in cases similar to the following:
# vm: needs to know the domain in its init script
# gateway_name: needs the ip of the vm to use as backend.
# - the fqdn can be computed from grid_gateway_domain for the vm
# - the backend can reference the vm ip directly 
data &quot;grid_gateway_domain&quot; &quot;domain&quot; {
  node = 7 
  name = &quot;ashraf&quot;
}
resource &quot;grid_network&quot; &quot;net1&quot; {
    nodes = [8]
    ip_range = &quot;10.1.0.0/24&quot;
    name = &quot;network&quot;
    description = &quot;newer network&quot;
    add_wg_access = true
}
resource &quot;grid_deployment&quot; &quot;d1&quot; {
  node = 8
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, 8, &quot;&quot;)
  vms {
    name = &quot;vm1&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/strm-helloworld-http-latest.flist&quot;
    cpu = 2 
    publicip = true
    memory = 1024
    env_vars = {
      SSH_KEY = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDTwULSsUubOq3VPWL6cdrDvexDmjfznGydFPyaNcn7gAL9lRxwFbCDPMj7MbhNSpxxHV2+/iJPQOTVJu4oc1N7bPP3gBCnF51rPrhTpGCt5pBbTzeyNweanhedkKDsCO2mIEh/92Od5Hg512dX4j7Zw6ipRWYSaepapfyoRnNSriW/s3DH/uewezVtL5EuypMdfNngV/u2KZYWoeiwhrY/yEUykQVUwDysW/xUJNP5o+KSTAvNSJatr3FbuCFuCjBSvageOLHePTeUwu6qjqe+Xs4piF1ByO/6cOJ8bt5Vcx0bAtI8/MPApplUU/JWevsPNApvnA/ntffI+u8DCwgP ashraf@thinkpad&quot;
    }
    planetary = true
  }
}
resource &quot;grid_name_proxy&quot; &quot;p1&quot; {
  node = 7
  name = &quot;ashraf&quot;
  backends = [format(&quot;http://%s&quot;, split(&quot;/&quot;, grid_deployment.d1.vms[0].computedip)[0])]
  tls_passthrough = false
}
output &quot;fqdn&quot; {
    value = data.grid_gateway_domain.domain.fqdn
}
output &quot;node1_zmachine1_ip&quot; {
    value = grid_deployment.d1.vms[0].ip
}
output &quot;public_ip&quot; {
    value = split(&quot;/&quot;,grid_deployment.d1.vms[0].computedip)[0]
}

output &quot;ygg_ip&quot; {
    value = grid_deployment.d1.vms[0].ygg_ip
}

</code></pre>
<p>please note to use grid_name_proxy you should choose a node that has public config and has a domain in its public config like node 7 in the following example
<img src="terraform/resources/./terraform/img//graphql_publicconf.png" alt=" " /></p>
<p>Here </p>
<ul>
<li>we created a grid domain resource <code>ashraf</code> to be deployed on gateway node <code>7</code> to end up with a domain <code>ashraf.ghent01.devnet.grid.tf</code></li>
<li>we create a proxy for the gateway to send the traffic coming to <code>ashraf.ghent01.devnet.grid.tf</code> to the vm as a backend, we say <code>tls_passthrough is false</code> to let the gateway terminate the traffic, if you replcae it with <code>true</code> your backend service needs to be able to do the TLS termination</li>
</ul>
<h2 id="expose-with-fulldomain"><a class="header" href="#expose-with-fulldomain">expose with fulldomain</a></h2>
<p>A complete list of gateway fqdn workload parameters can be found <a href="https://github.com/threefoldtech/terraform-provider-grid/blob/development/docs/resources/fqdn_proxy.md">here</a>.</p>
<p>it is more like the above example the only difference is you need to create an <code>A record</code> on your name provider for <code>remote.omar.grid.tf</code>  to gateway node <code>7</code> IPv4.</p>
<pre><code>
resource &quot;grid_fqdn_proxy&quot; &quot;p1&quot; {
  node = 7
  name = &quot;workloadname&quot;
  fqdn = &quot;remote.omar.grid.tf&quot;
  backends = [format(&quot;http://%s&quot;, split(&quot;/&quot;, grid_deployment.d1.vms[0].computedip)[0])]
  tls_passthrough = true
}

output &quot;fqdn&quot; {
    value = grid_fqdn_proxy.p1.fqdn
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/resources/../advanced/img//terraform_.png" alt=" " /></p>
<h1 id="terraform-kubernetes-cluster"><a class="header" href="#terraform-kubernetes-cluster">Terraform Kubernetes cluster</a></h1>
<p>Kubernetes deployment can be quite difficult and requiring lots of experience, but I think we provided a very simple way to provision Kubernetes cluster on grid 3</p>
<pre><code class="language-terraform">terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
}

resource &quot;grid_network&quot; &quot;net1&quot; {
    nodes = [2, 4]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network12346&quot;
    description = &quot;newer network&quot;
    add_wg_access = true
}

resource &quot;grid_kubernetes&quot; &quot;k8s1&quot; {
  network_name = grid_network.net1.name
  nodes_ip_range = grid_network.net1.nodes_ip_range 
  token = &quot;12345678910122&quot;
  ssh_key = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;

  master {
    disk_size = 23
    node = 2
    name = &quot;mr&quot;
    cpu = 2
    publicip = true
    memory = 2048
  }
  workers {
    disk_size = 15
    node = 2
    name = &quot;w0&quot;
    cpu = 2
    memory = 2048
  }
  workers {
    disk_size = 14
    node = 4
    name = &quot;w2&quot;
    cpu = 1 
    memory = 2048
  }
  workers {
    disk_size = 13
    node = 4
    name = &quot;w3&quot;
    cpu = 1
    memory = 2048
  }
}


output &quot;master_public_ip&quot; {
    value = grid_kubernetes.k8s1.master[0].computedip
}

output &quot;wg_config&quot; {
    value = grid_network.net1.access_wg_config
}

</code></pre>
<p>Everything looks similar to our first example, the global terraform section, the provider section and the network section.</p>
<h3 id="grid-kubernetes-resource"><a class="header" href="#grid-kubernetes-resource">grid kubernetes resource</a></h3>
<pre><code class="language-terraform">resource &quot;grid_kubernetes&quot; &quot;k8s1&quot; {
  network_name = grid_network.net1.name
  nodes_ip_range = grid_network.net1.nodes_ip_range 
  token = &quot;12345678910122&quot;
  ssh_key = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;

  master {
    disk_size = 23
    node = 2
    name = &quot;mr&quot;
    cpu = 2
    publicip = true
    memory = 2048
  }
  workers {
    disk_size = 15
    node = 2
    name = &quot;w0&quot;
    cpu = 2
    memory = 2048
  }
  workers {
    disk_size = 14
    node = 4
    name = &quot;w2&quot;
    cpu = 1 
    memory = 2048
  }
  workers {
    disk_size = 13
    node = 4
    name = &quot;w3&quot;
    cpu = 1
    memory = 2048
  }
}

</code></pre>
<p>it requires</p>
<ul>
<li>
<p>the network name</p>
</li>
<li>
<p>nodes ip range </p>
</li>
<li>
<p>token is the cluster token</p>
</li>
<li>
<p>ssh_key to access the cluster VMs</p>
</li>
</ul>
<p>then we describe the VMs master and workers section in terms of </p>
<ul>
<li>name within the deployment</li>
<li>disk size</li>
<li>node to deploy it on </li>
<li>cpu </li>
<li>memory</li>
</ul>
<h3 id="kubernetes-outputs"><a class="header" href="#kubernetes-outputs">Kubernetes outputs</a></h3>
<pre><code class="language-terraform">output &quot;master_public_ip&quot; {
    value = grid_kubernetes.k8s1.master[0].computedip
}

output &quot;wg_config&quot; {
    value = grid_network.net1.access_wg_config
}

</code></pre>
<p>We will be mainly interested in the master node public ip <code>computed IP</code> and the wireguard configurations</p>
<p>You can check the examples repo <a href="https://github.com/threefoldtech/terraform-provider-grid/tree/development/examples">here</a></p>
<h3 id="current-limitations"><a class="header" href="#current-limitations">Current limitations</a></h3>
<ul>
<li><a href="https://github.com/threefoldtech/terraform-provider-grid/issues/12">parallelism=1</a></li>
<li><a href="https://github.com/threefoldtech/terraform-provider-grid/issues/15">increasing IPs in active deployment</a></li>
<li><a href="https://github.com/threefoldtech/terraform-provider-grid/issues/13">introducing new nodes to kuberentes deployment</a></li>
<li><a href="https://github.com/threefoldtech/terraform-provider-grid/issues/11">Multiple deployments on the same node</a></li>
</ul>
<h2 id="more-info"><a class="header" href="#more-info">More Info</a></h2>
<p>A complete list of k8s resource parameters can be found <a href="https://github.com/threefoldtech/terraform-provider-grid/blob/development/docs/resources/kubernetes.md">here</a>.</p>
<pre><code>terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
}

resource &quot;grid_network&quot; &quot;net1&quot; {
    nodes = [2, 4]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network12346&quot;
    description = &quot;newer network&quot;
    add_wg_access = true
}

resource &quot;grid_kubernetes&quot; &quot;k8s1&quot; {
  network_name = grid_network.net1.name
  nodes_ip_range = grid_network.net1.nodes_ip_range 
  token = &quot;12345678910122&quot;
  ssh_key = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;

  master {
    disk_size = 23
    node = 2
    name = &quot;mr&quot;
    cpu = 2
    publicip = true
    memory = 2048
  }
  workers {
    disk_size = 15
    node = 2
    name = &quot;w0&quot;
    cpu = 2
    memory = 2048
  }
  workers {
    disk_size = 14
    node = 4
    name = &quot;w2&quot;
    cpu = 1 
    memory = 2048
  }
  workers {
    disk_size = 13
    node = 4
    name = &quot;w3&quot;
    cpu = 1
    memory = 2048
  }
}


output &quot;master_public_ip&quot; {
    value = grid_kubernetes.k8s1.master[0].computedip
}

output &quot;wg_config&quot; {
    value = grid_network.net1.access_wg_config
}
</code></pre>
<h2 id="demo"><a class="header" href="#demo">Demo</a></h2>
<ul>
<li>see <a href="terraform/resources/./terraform_k8s_demo.html">Demo</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/resources/../advanced/img//terraform_.png" alt=" " /></p>
<h2 id="demo-video-showing-deploying-k8s-with-terraform"><a class="header" href="#demo-video-showing-deploying-k8s-with-terraform">Demo video showing deploying k8s with terraform</a></h2>
<div class="aspect-w-16 aspect-h-9">
<iframe src="https://player.vimeo.com/video/654552300?h=c61feb579b" width="640" height="564" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
</div>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/resources/../advanced/img//terraform_.png" alt=" " /></p>
<h1 id="deploying-a-zdb-with-terraform"><a class="header" href="#deploying-a-zdb-with-terraform">Deploying a ZDB with terraform</a></h1>
<p>A brief description of zdb fields can be found <a href="https://github.com/threefoldtech/terraform-provider-grid/blob/development/docs/resources/deployment.md#nested-schema-for-zdbs">here</a>. </p>
<p>A more thorough description of zdb operation can be found in its parent <a href="https://github.com/threefoldtech/0-db">repo</a>.</p>
<pre><code>terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
}

resource &quot;grid_deployment&quot; &quot;d1&quot; {
  node = 4 
  
  zdbs{
    name = &quot;zdb1&quot;
    size = 10 
    description = &quot;zdb1 description&quot;
    password = &quot;zdbpasswd1&quot;
    mode = &quot;user&quot;
  }
  zdbs{
    name = &quot;zdb2&quot;
    size = 2
    description = &quot;zdb2 description&quot;
    password = &quot;zdbpasswd2&quot;
    mode = &quot;seq&quot;
  }
}

output &quot;deployment_id&quot; {
    value = grid_deployment.d1.id
}

output &quot;zdb1_endpoint&quot; {
    value = format(&quot;[%s]:%d&quot;, grid_deployment.d1.zdbs[0].ips[0], grid_deployment.d1.zdbs[0].port)
}

output &quot;zdb1_namespace&quot; {
    value = grid_deployment.d1.zdbs[0].namespace
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/resources/../advanced/img/terraform_.png" alt=" " /></p>
<h2 id="terraform-quantum-safe-filesystem"><a class="header" href="#terraform-quantum-safe-filesystem">Terraform Quantum Safe Filesystem</a></h2>
<p>A complete list of QSFS workload parameters can be found <a href="https://github.com/threefoldtech/terraform-provider-grid/blob/development/docs/resources/deployment.md#nested-schema-for-qsfs">here</a>. </p>
<p>The <a href="https://github.com/threefoldtech/quantum-storage">quantum-storage</a> repo contains a more thorough description of QSFS operation.</p>
<pre><code>terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
}

locals {
  metas = [&quot;meta1&quot;, &quot;meta2&quot;, &quot;meta3&quot;, &quot;meta4&quot;]
  datas = [&quot;data1&quot;, &quot;data2&quot;, &quot;data3&quot;, &quot;data4&quot;]
}

resource &quot;grid_network&quot; &quot;net1&quot; {
    nodes = [7]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network&quot;
    description = &quot;newer network&quot;
}

resource &quot;grid_deployment&quot; &quot;d1&quot; {
    node = 7
    dynamic &quot;zdbs&quot; {
        for_each = local.metas
        content {
            name = zdbs.value
            description = &quot;description&quot;
            password = &quot;password&quot;
            size = 10
            mode = &quot;user&quot;
        }
    }
    dynamic &quot;zdbs&quot; {
        for_each = local.datas
        content {
            name = zdbs.value
            description = &quot;description&quot;
            password = &quot;password&quot;
            size = 10
            mode = &quot;seq&quot;
        }
    }
}

resource &quot;grid_deployment&quot; &quot;qsfs&quot; {
  node = 7
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, 7, &quot;&quot;)
  qsfs {
    name = &quot;qsfs&quot;
    description = &quot;description6&quot;
    cache = 10240 # 10 GB
    minimal_shards = 2
    expected_shards = 4
    redundant_groups = 0
    redundant_nodes = 0
    max_zdb_data_dir_size = 512 # 512 MB
    encryption_algorithm = &quot;AES&quot;
    encryption_key = &quot;4d778ba3216e4da4231540c92a55f06157cabba802f9b68fb0f78375d2e825af&quot;
    compression_algorithm = &quot;snappy&quot;
    metadata {
      type = &quot;zdb&quot;
      prefix = &quot;hamada&quot;
      encryption_algorithm = &quot;AES&quot;
      encryption_key = &quot;4d778ba3216e4da4231540c92a55f06157cabba802f9b68fb0f78375d2e825af&quot;
      dynamic &quot;backends&quot; {
          for_each = [for zdb in grid_deployment.d1.zdbs : zdb if zdb.mode != &quot;seq&quot;]
          content {
              address = format(&quot;[%s]:%d&quot;, backends.value.ips[1], backends.value.port)
              namespace = backends.value.namespace
              password = backends.value.password
          }
      }
    }
    groups {
      dynamic &quot;backends&quot; {
          for_each = [for zdb in grid_deployment.d1.zdbs : zdb if zdb.mode == &quot;seq&quot;]
          content {
              address = format(&quot;[%s]:%d&quot;, backends.value.ips[1], backends.value.port)
              namespace = backends.value.namespace
              password = backends.value.password
          }
      }
    }
  }
  vms {
    name = &quot;vm&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 2
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    planetary = true
    env_vars = {
      SSH_KEY = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;
    }
    mounts {
        disk_name = &quot;qsfs&quot;
        mount_point = &quot;/qsfs&quot;
    }
  }
}
output &quot;metrics&quot; {
    value = grid_deployment.qsfs.qsfs[0].metrics_endpoint
}
output &quot;ygg_ip&quot; {
    value = grid_deployment.qsfs.vms[0].ygg_ip
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/resources/../advanced/img/terraform_.png" alt=" " /></p>
<h1 id="terraform-caprover"><a class="header" href="#terraform-caprover">Terraform Caprover</a></h1>
<h3 id="what-is-caprover"><a class="header" href="#what-is-caprover">What is CapRover?</a></h3>
<p><a href="https://caprover.com/">CapRover</a> is an easy-to-use app/database deployment and web server manager that works for a variety of applications such as Node.js, Ruby, PHP, Postgres, and MongoDB. It runs fast and is very robust, as it uses Docker, Nginx, LetsEncrypt, and NetData under the hood behind its user-friendly interface.
Heres a link to CapRover's open source repository on <a href="https://github.com/caprover/caprover">GitHub</a>.</p>
<h3 id="features-of-caprover"><a class="header" href="#features-of-caprover">Features of Caprover:</a></h3>
<ul>
<li>CLI for automation and scripting</li>
<li>Web GUI for ease of access and convenience</li>
<li>No lock-in: Remove CapRover and your apps keep working !</li>
<li>Docker Swarm under the hood for containerization and clustering.</li>
<li>Nginx (fully customizable template) under the hood for load-balancing.</li>
<li>Lets Encrypt under the hood for free SSL (HTTPS).</li>
<li><strong>One-Click Apps</strong> : Deploying one-click apps is a matter of seconds! MongoDB, Parse, MySQL, WordPress, Postgres and many more.</li>
<li><strong>Fully Customizable</strong> : Optionally fully customizable nginx config allowing you to enable HTTP2, specific caching logic, custom SSL certs and etc.</li>
<li><strong>Cluster Ready</strong> : Attach more nodes and create a cluster in seconds! CapRover automatically configures nginx to load balance.</li>
<li><strong>Increase Productivity</strong> : Focus on your apps ! Not the bells and whistles, just to run your apps.</li>
<li><strong>Easy Deploy</strong> : Many ways to deploy. You can upload your source from dashboard, use command line caprover deploy, use webhooks and build upon git push</li>
</ul>
<h3 id="pre-requisites"><a class="header" href="#pre-requisites">Pre-requisites</a></h3>
<ul>
<li>Domain Name:
after installation, you will need to point a wildcard DNS entry to your CapRover IP Address.
Note that you can use CapRover without a domain too. But you won't be able to setup HTTPS or add <code>Self hosted Docker Registry</code>.</li>
<li>TerraForm installed to provision, adjust and tear down infrastructure using the tf configuration files provided here.</li>
<li>Yggdrasil installed and enabled for End-to-end encrypted IPv6 networking.</li>
<li>account created on <a href="https://polkadot.js.org/apps/?rpc=wss://tfchain.dev.threefold.io/ws#/accounts">Polkadot</a> and got an twin id, and saved you mnemonics.</li>
<li>TFTs in your account balance (in development, Transferer some test TFTs from ALICE account).</li>
</ul>
<h3 id="how-to-run-caprover-on-threefold-grid-3"><a class="header" href="#how-to-run-caprover-on-threefold-grid-3">How to run CapRover on ThreeFold Grid 3:</a></h3>
<p>In this guide, we will use Caprover to setup your own private Platform as a service (PaaS) on TFGrid 3 infrastructure.</p>
<h4 id="clone-the-project-repo"><a class="header" href="#clone-the-project-repo">Clone the project repo</a></h4>
<pre><code class="language-sh">git clone https://github.com/freeflowuniverse/freeflow_caprover.git
</code></pre>
<h4 id="a-leader-node-deploymentsetup"><a class="header" href="#a-leader-node-deploymentsetup">A) leader node deployment/setup:</a></h4>
<h5 id="step-1-deploy-a-leader-node"><a class="header" href="#step-1-deploy-a-leader-node">step 1: Deploy a Leader Node</a></h5>
<p>Create a leader caprover node using terraform, here's an example :</p>
<pre><code>terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
    mnemonics = &quot;&lt;your-mnemonics&gt;&quot; 
    network = &quot;dev&quot; # or test to use testnet
}

resource &quot;grid_network&quot; &quot;net0&quot; {
    nodes = [4]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network&quot;
    description = &quot;newer network&quot;
    add_wg_access = true
}

resource &quot;grid_deployment&quot; &quot;d0&quot; {
  node = 4
  network_name = grid_network.net0.name
  ip_range = lookup(grid_network.net0.nodes_ip_range, 4, &quot;&quot;)
  disks {
    name        = &quot;data0&quot;
    # will hold images, volumes etc. modify the size according to your needs
    size        = 20
    description = &quot;volume holding docker data&quot;
  }
  disks {
    name        = &quot;data1&quot;
    # will hold data reltaed to caprover conf, nginx stuff, lets encrypt stuff.
    size        = 5
    description = &quot;volume holding captain data&quot;
  }

  vms {
    name = &quot;caprover&quot;
    flist = &quot;https://hub.grid.tf/samehabouelsaad.3bot/abouelsaad-caprover-tf_10.0.1_v1.0.flist&quot;
    # modify the cores according to your needs
    cpu = 4
    publicip = true
    # modify the memory according to your needs
    memory = 8192
    entrypoint = &quot;/sbin/zinit init&quot;
    mounts {
      disk_name   = &quot;data0&quot;
      mount_point = &quot;/var/lib/docker&quot;
    }
    mounts {
      disk_name   = &quot;data1&quot;
      mount_point = &quot;/captain&quot;
    }
    env_vars = {
      &quot;PUBLIC_KEY&quot; = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9MI7fh4xEOOEKL7PvLvXmSeRWesToj6E26bbDASvlZnyzlSKFLuYRpnVjkr8JcuWKZP6RQn8+2aRs6Owyx7Tx+9kmEh7WI5fol0JNDn1D0gjp4XtGnqnON7d0d5oFI+EjQQwgCZwvg0PnV/2DYoH4GJ6KPCclPz4a6eXrblCLA2CHTzghDgyj2x5B4vB3rtoI/GAYYNqxB7REngOG6hct8vdtSndeY1sxuRoBnophf7MPHklRQ6EG2GxQVzAOsBgGHWSJPsXQkxbs8am0C9uEDL+BJuSyFbc/fSRKptU1UmS18kdEjRgGNoQD7D+Maxh1EbmudYqKW92TVgdxXWTQv1b1+3dG5+9g+hIWkbKZCBcfMe4nA5H7qerLvoFWLl6dKhayt1xx5mv8XhXCpEC22/XHxhRBHBaWwSSI+QPOCvs4cdrn4sQU+EXsy7+T7FIXPeWiC2jhFd6j8WIHAv6/rRPsiwV1dobzZOrCxTOnrqPB+756t7ANxuktsVlAZaM= sameh@sameh-inspiron-3576&quot;
    # SWM_NODE_MODE env var is required, should be &quot;leader&quot; or &quot;worker&quot;
    # leader: will run sshd, containerd, dockerd as zinit services plus caprover service in leader mode which start caprover, lets encrypt, nginx containers.
    # worker: will run sshd, containerd, dockerd as zinit services plus caprover service in orker mode which only join the swarm cluster. check the wroker terrafrom file example.
      &quot;SWM_NODE_MODE&quot; = &quot;leader&quot;
    # CAPROVER_ROOT_DOMAIN is optional env var, by providing it you can access the captain dashboard after vm initilization by visiting http://captain.your-root-domain
    # otherwise you will have to add the root domain manually from the captain dashboard by visiting http://{publicip}:3000 to access the dashboard
      &quot;CAPROVER_ROOT_DOMAIN&quot; = &quot;roverapps.grid.tf&quot;
  }
}

output &quot;wg_config&quot; {
    value = grid_network.net0.access_wg_config
}
output &quot;ygg_ip&quot; {
    value = grid_deployment.d0.vms[0].ygg_ip
}
output &quot;vm_ip&quot; {
    value = grid_deployment.d0.vms[0].ip
}
output &quot;vm_public_ip&quot; {
    value = grid_deployment.d0.vms[0].computedip
}
</code></pre>
<pre><code class="language-bash">cd freeflow_caprover/terraform/leader/
vim main.tf
</code></pre>
<ul>
<li>
<p>In <code>provider</code> Block, add your <code>mnemonics</code> and specify the grid network to deploy on.</p>
</li>
<li>
<p>In <code>resource</code> Block, update the disks size, memory size, and cores number to fit your needs or leave as it is for testing.</p>
</li>
<li>
<p>In the <code>PUBLIC_KEY</code> env var value put your ssh public key .</p>
</li>
<li>
<p>In the <code>CAPROVER_ROOT_DOMAIN</code> env var value put your root domain, this is optional and you can add it later from the dashboard put it will save you the extra step and allow you to access your dashboard using your domain name directly after the deployment.</p>
</li>
<li>
<p>save the file, and execute the following commands:</p>
<pre><code class="language-bash">terraform init
terraform apply -parallelism=1
</code></pre>
</li>
<li>
<p>wait till you see <code>apply complete</code>, and note the VM public ip in the final output.</p>
</li>
<li>
<p>verify the status of the VM</p>
<pre><code class="language-bash">ssh root@{public_ip_address}
zinit list
zinit log caprover
</code></pre>
<p>You will see output like this:</p>
<pre><code class="language-bash">root@caprover:~ # zinit list
sshd: Running
containerd: Running
dockerd: Running
sshd-init: Success
caprover: Running
root@caprover:~ # zinit log caprover
[+] caprover: CapRover Root Domain: newapps.grid.tf
[+] caprover: {
[+] caprover:             &quot;namespace&quot;: &quot;captain&quot;,
[+] caprover:             &quot;customDomain&quot;: &quot;newapps.grid.tf&quot;
[+] caprover:     }
[+] caprover: CapRover will be available at http://captain.newapps.grid.    tf after installation
[-] caprover: docker: Cannot connect to the Docker daemon at unix:///var/   run/docker.sock. Is the docker daemon running?.
[-] caprover: See 'docker run --help'.
[-] caprover: Unable to find image 'caprover/caprover:latest' locally
[-] caprover: latest: Pulling from caprover/caprover
[-] caprover: af4c2580c6c3: Pulling fs layer
[-] caprover: 4ea40d27a2cf: Pulling fs layer
[-] caprover: 523d612e9cd2: Pulling fs layer
[-] caprover: 8fee6a1847b0: Pulling fs layer
[-] caprover: 60cce3519052: Pulling fs layer
[-] caprover: 4bae1011637c: Pulling fs layer
[-] caprover: ecf48b6c1f43: Pulling fs layer
[-] caprover: 856f69196742: Pulling fs layer
[-] caprover: e86a512b6f8c: Pulling fs layer
[-] caprover: cecbd06d956f: Pulling fs layer
[-] caprover: cdd679ff24b0: Pulling fs layer
[-] caprover: d60abbe06609: Pulling fs layer
[-] caprover: 0ac0240c1a59: Pulling fs layer
[-] caprover: 52d300ad83da: Pulling fs layer
[-] caprover: 8fee6a1847b0: Waiting
[-] caprover: e86a512b6f8c: Waiting
[-] caprover: 60cce3519052: Waiting
[-] caprover: cecbd06d956f: Waiting
[-] caprover: cdd679ff24b0: Waiting
[-] caprover: 4bae1011637c: Waiting
[-] caprover: d60abbe06609: Waiting
[-] caprover: 0ac0240c1a59: Waiting
[-] caprover: 52d300ad83da: Waiting
[-] caprover: 856f69196742: Waiting
[-] caprover: ecf48b6c1f43: Waiting
[-] caprover: 523d612e9cd2: Verifying Checksum
[-] caprover: 523d612e9cd2: Download complete
[-] caprover: 4ea40d27a2cf: Verifying Checksum
[-] caprover: 4ea40d27a2cf: Download complete
[-] caprover: af4c2580c6c3: Verifying Checksum
[-] caprover: af4c2580c6c3: Download complete
[-] caprover: 4bae1011637c: Verifying Checksum
[-] caprover: 4bae1011637c: Download complete
[-] caprover: 8fee6a1847b0: Verifying Checksum
[-] caprover: 8fee6a1847b0: Download complete
[-] caprover: 856f69196742: Verifying Checksum
[-] caprover: 856f69196742: Download complete
[-] caprover: ecf48b6c1f43: Verifying Checksum
[-] caprover: ecf48b6c1f43: Download complete
[-] caprover: e86a512b6f8c: Verifying Checksum
[-] caprover: e86a512b6f8c: Download complete
[-] caprover: cdd679ff24b0: Verifying Checksum
[-] caprover: cdd679ff24b0: Download complete
[-] caprover: d60abbe06609: Verifying Checksum
[-] caprover: d60abbe06609: Download complete
[-] caprover: cecbd06d956f: Download complete
[-] caprover: 0ac0240c1a59: Verifying Checksum
[-] caprover: 0ac0240c1a59: Download complete
[-] caprover: 60cce3519052: Verifying Checksum
[-] caprover: 60cce3519052: Download complete
[-] caprover: af4c2580c6c3: Pull complete
[-] caprover: 52d300ad83da: Download complete
[-] caprover: 4ea40d27a2cf: Pull complete
[-] caprover: 523d612e9cd2: Pull complete
[-] caprover: 8fee6a1847b0: Pull complete
[-] caprover: 60cce3519052: Pull complete
[-] caprover: 4bae1011637c: Pull complete
[-] caprover: ecf48b6c1f43: Pull complete
[-] caprover: 856f69196742: Pull complete
[-] caprover: e86a512b6f8c: Pull complete
[-] caprover: cecbd06d956f: Pull complete
[-] caprover: cdd679ff24b0: Pull complete
[-] caprover: d60abbe06609: Pull complete
[-] caprover: 0ac0240c1a59: Pull complete
[-] caprover: 52d300ad83da: Pull complete
[-] caprover: Digest:   sha256:39c3f188a8f425775cfbcdc4125706cdf614cd38415244ccf967cd1a4e692b4f
[-] caprover: Status: Downloaded newer image for caprover/caprover:latest
[+] caprover: Captain Starting ...
[+] caprover: Overriding skipVerifyingDomains from /captain/data/   config-override.json
[+] caprover: Installing Captain Service ...
[+] caprover:
[+] caprover:  Installation of CapRover is starting...
[+] caprover: For troubleshooting, please see: https://caprover.com/docs/   troubleshooting.html
[+] caprover:
[+] caprover:
[+] caprover:
[+] caprover:
[+] caprover:
[+] caprover:  &gt;&gt;&gt; Checking System Compatibility &lt;&lt;&lt;
[+] caprover:    Docker Version passed.
[+] caprover:    Ubuntu detected.
[+] caprover:    X86 CPU detected.
[+] caprover:    Total RAM 8339 MB
[+] caprover: Pulling: nginx:1
[+] caprover: Pulling: caprover/caprover-placeholder-app:latest
[+] caprover: Pulling: caprover/certbot-sleeping:v1.6.0
[+] caprover: October 12th 2021, 12:49:26.301 pm    Fresh installation!
[+] caprover: October 12th 2021, 12:49:26.309 pm    Starting swarm at   185.206.122.32:2377
[+] caprover: Swarm started: z06ymksbcoren9cl7g2xzw9so
[+] caprover: *** CapRover is initializing ***
[+] caprover: Please wait at least 60 seconds before trying to access   CapRover.
[+] caprover: ===================================
[+] caprover:  **** Installation is done! *****
[+] caprover: CapRover is available at http://captain.newapps.grid.tf
[+] caprover: Default password is: captain42
[+] caprover: ===================================
</code></pre>
<p>Wait until you see *<strong>* Installation is done! ***</strong> in the caprover service log.</p>
</li>
</ul>
<h5 id="step-2-connect-root-domain"><a class="header" href="#step-2-connect-root-domain">Step 2: Connect Root Domain</a></h5>
<p>After the container runs, you will now need to connect your CapRover instance to a Root Domain.</p>
<p>Lets say you own example.com. You can set *.something.example.com as an A-record in your DNS settings to point to the IP address of the server where you installed CapRover. To do this, go to the DNS settings in your domain provider website, and set a wild card A record entry.</p>
<p>For example: Type: A, Name (or host): *.something.example.com, IP (or Points to): <code>110.122.131.141</code> where this is the IP address of your CapRover machine.</p>
<pre><code class="language-yaml">TYPE: A record
HOST: \*.something.example.com
POINTS TO: (IP Address of your server)
TTL: (doesnt really matter)
</code></pre>
<p>To confirm, go to https://mxtoolbox.com/DNSLookup.aspx and enter <code>somethingrandom.something.example.com</code> and check if IP address resolves to the IP you set in your DNS.</p>
<h6 id="note"><a class="header" href="#note">Note</a></h6>
<p><code>somethingrandom</code> is needed because you set a wildcard entry in your DNS by setting <code>*.something.example.com</code> as your host, not <code>something.example.com</code>.</p>
<h5 id="step-3-caprover-root-domain-configurations"><a class="header" href="#step-3-caprover-root-domain-configurations">Step 3: CapRover Root Domain Configurations</a></h5>
<p>skip this step if you provided your root domain in the TerraFrom configuration file</p>
<p>Once the CapRover is initialized, you can visit <code>http://[IP_OF_YOUR_SERVER]:3000</code> in your browser and login to CapRover using the default password <code>captain42</code>. You can change your password later.</p>
<p>In the UI enter you root domain and press Update Domain button.</p>
<h5 id="step-4-access-the-captain-dashboard"><a class="header" href="#step-4-access-the-captain-dashboard">Step 4: Access the Captain Dashboard</a></h5>
<p>Once you set your root domain as caprover.example.com, you will be redirected to captain.caprover.example.com.</p>
<p>Now CapRover is ready and running in a single node.</p>
<h6 id="to-allow-cluster-mode"><a class="header" href="#to-allow-cluster-mode">To allow cluster mode</a></h6>
<ul>
<li>
<p>Enable HTTPS</p>
<ul>
<li>Go to CapRover <code>Dashboard</code> tab, then in <code>CapRover Root Domain Configurations</code> press on <code>Enable HTTPS</code> then you will asked to enter your email address</li>
</ul>
</li>
<li>
<p>Docker Registry Configuration</p>
<ul>
<li>Go to CapRover <code>Cluster</code> tab, then in <code>Docker Registry Configuration</code> section, press on <code>Self hosted Docker Registry</code> or add your <code>Remote Docker Registry</code></li>
</ul>
</li>
<li>
<p>Run the following command in the ssh session:</p>
<pre><code class="language-bash">    docker swarm join-token worker
</code></pre>
<p>It will output something like this:</p>
<pre><code class="language-bash">docker swarm join --token   SWMTKN-1-0892ds1ney7pa0hymi3qwph7why1d9r3z6bvwtin51r14hcz3t-cjsephnu4f2ez fpdd6svnnbq7 185.206.122.33:2377
</code></pre>
</li>
<li>
<p>To add a worker node to this swarm, you need:</p>
<ul>
<li>Generated token <code>SWMTKN-1-0892ds1ney7pa0hymi3qwph7why1d9r3z6bvwtin51r14hcz3t-cjsephnu4f2ezfpdd6svnnbq7</code></li>
<li>Leader node public ip <code>185.206.122.33</code></li>
</ul>
</li>
</ul>
<p>This information is required in the next section to run CapRover in cluster mode.</p>
<h4 id="b-worker-node-deploymentsetup"><a class="header" href="#b-worker-node-deploymentsetup">B) Worker Node Deployment/setup:</a></h4>
<h5 id="step-1-deploy-a-worker-node"><a class="header" href="#step-1-deploy-a-worker-node">Step 1: Deploy a Worker Node</a></h5>
<p>example worker terraform file</p>
<pre><code>terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
    mnemonics = &quot;&lt;your-mnemonics&gt;&quot;
    network = &quot;dev&quot; # or test to use testnet 
}

resource &quot;grid_network&quot; &quot;net2&quot; {
    nodes = [4]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network&quot;
    description = &quot;newer network&quot;
}

resource &quot;grid_deployment&quot; &quot;d2&quot; {
  node = 4
  network_name = grid_network.net2.name
  ip_range = lookup(grid_network.net2.nodes_ip_range, 4, &quot;&quot;)
  disks {
    name        = &quot;data2&quot;
    # will hold images, volumes etc. modify the size according to your needs
    size        = 20
    description = &quot;volume holding docker data&quot;
  }

  vms {
    name = &quot;caprover&quot;
    flist = &quot;https://hub.grid.tf/samehabouelsaad.3bot/abouelsaad-caprover-tf_10.0.1_v1.0.flist&quot;
    # modify the cores according to your needs
    cpu = 2
    publicip = true
    # modify the memory according to your needs
    memory = 2048
    entrypoint = &quot;/sbin/zinit init&quot;
    mounts {
      disk_name   = &quot;data2&quot;
      mount_point = &quot;/var/lib/docker&quot;
    }
    env_vars = {
      &quot;PUBLIC_KEY&quot; = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9MI7fh4xEOOEKL7PvLvXmSeRWesToj6E26bbDASvlZnyzlSKFLuYRpnVjkr8JcuWKZP6RQn8+2aRs6Owyx7Tx+9kmEh7WI5fol0JNDn1D0gjp4XtGnqnON7d0d5oFI+EjQQwgCZwvg0PnV/2DYoH4GJ6KPCclPz4a6eXrblCLA2CHTzghDgyj2x5B4vB3rtoI/GAYYNqxB7REngOG6hct8vdtSndeY1sxuRoBnophf7MPHklRQ6EG2GxQVzAOsBgGHWSJPsXQkxbs8am0C9uEDL+BJuSyFbc/fSRKptU1UmS18kdEjRgGNoQD7D+Maxh1EbmudYqKW92TVgdxXWTQv1b1+3dG5+9g+hIWkbKZCBcfMe4nA5H7qerLvoFWLl6dKhayt1xx5mv8XhXCpEC22/XHxhRBHBaWwSSI+QPOCvs4cdrn4sQU+EXsy7+T7FIXPeWiC2jhFd6j8WIHAv6/rRPsiwV1dobzZOrCxTOnrqPB+756t7ANxuktsVlAZaM= sameh@sameh-inspiron-3576&quot;
    }
    # SWM_NODE_MODE env var is required, should be &quot;leader&quot; or &quot;worker&quot;
    # leader: check the wroker terrafrom file example.
    # worker: will run sshd, containerd, dockerd as zinit services plus caprover service in orker mode which only join the swarm cluster. 

      &quot;SWM_NODE_MODE&quot; = &quot;worker&quot;
    # from the leader node (the one running caprover) run `docker swarm join-token worker`
    # you must add the generated token to SWMTKN env var and the leader public ip to LEADER_PUBLIC_IP env var

      &quot;SWMTKN&quot;=&quot;SWMTKN-1-522cdsyhknmavpdok4wi86r1nihsnipioc9hzfw9dnsvaj5bed-8clrf4f2002f9wziabyxzz32d&quot;
    &quot;LEADER_PUBLIC_IP&quot; = &quot;185.206.122.38&quot;

  }
}

output &quot;wg_config&quot; {
    value = grid_network.net2.access_wg_config
}
output &quot;ygg_ip&quot; {
    value = grid_deployment.d2.vms[0].ygg_ip
}
output &quot;vm_ip&quot; {
    value = grid_deployment.d2.vms[0].ip
}
output &quot;vm_public_ip&quot; {
    value = grid_deployment.d2.vms[0].computedip
}
</code></pre>
<pre><code class="language-bash">cd freeflow_caprover/terraform/worker/
vim main.tf
</code></pre>
<ul>
<li>
<p>In <code>provider</code> Block, add your <code>mnemonics</code> and specify the grid network to deploy on.</p>
</li>
<li>
<p>In <code>resource</code> Block, update the disks size, memory size, and cores number to fit your needs or leave as it is for testing.</p>
</li>
<li>
<p>In the <code>PUBLIC_KEY</code> env var value put your ssh public key.</p>
</li>
<li>
<p>In the <code>SWMTKN</code> env var value put the previously generated token.</p>
</li>
<li>
<p>In the <code>LEADER_PUBLIC_IP</code> env var value put the leader node public ip.</p>
</li>
<li>
<p>Save the file, and execute the following commands:</p>
<pre><code class="language-bash">terraform init
terraform apply -parallelism=1
</code></pre>
</li>
<li>
<p>Wait till you see <code>apply complete</code>, and note the VM public ip in the final output.</p>
</li>
<li>
<p>Verify the status of the VM.</p>
<pre><code class="language-bash">ssh root@{public_ip_address}
zinit list
zinit log caprover
</code></pre>
<p>You will see output like this:</p>
<pre><code class="language-bash">root@caprover:~# zinit list
caprover: Success
dockerd: Running
containerd: Running
sshd: Running
sshd-init: Success
root@caprover:~# zinit log caprover
[-] caprover: Cannot connect to the Docker daemon at unix:///var/run/   docker.sock. Is the docker daemon running?
[+] caprover: This node joined a swarm as a worker.
</code></pre>
</li>
</ul>
<p>This means that your worker node is now ready and have joined the cluster successfully.</p>
<p>You can also verify this from CapRover dashboard in <code>Cluster</code> tab. Check <code>Nodes</code> section, you should be able to see the new worker node added there.</p>
<p>Now CapRover is ready in cluster mode (more than one server).</p>
<p>To run One-Click Apps please follow this <a href="https://caprover.com/docs/one-click-apps.html">tutorial</a></p>
<h3 id="implementations-details"><a class="header" href="#implementations-details">Implementations Details:</a></h3>
<ul>
<li>
<p>we use Ubuntu 18.04 to minimize the production issues as CapRover is tested on Ubuntu 18.04 and Docker 19.03.</p>
</li>
<li>
<p>In standard installation, CapRover has to be installed on a machine with a public IP address.</p>
</li>
<li>
<p>Services are managed by <code>Zinit</code> service manager to bring these processes up and running in case of any failure:</p>
<ul>
<li>sshd-init : service used to add user public key in vm ssh authorized keys (it run once).</li>
<li>containerd: service to maintain container runtime needed by docker.</li>
<li>caprover: service to run caprover container(it run once).</li>
<li>dockerd: service to run docker daemon.</li>
<li>sshd: service to maintain ssh server daemon.</li>
</ul>
</li>
<li>
<p>we adjusting the OOM priority on the Docker daemon so that it is less likely to be killed than other processes on the system</p>
<pre><code class="language-bash">echo -500 &gt;/proc/self/oom_score_adj
</code></pre>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/advanced/./img/terraform_.png" alt=" " /></p>
<h1 id="terraform-provider"><a class="header" href="#terraform-provider">Terraform Provider</a></h1>
<pre><code>terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}
provider &quot;grid&quot; {
    mnemonics = &quot;FROM THE CREATE TWIN STEP&quot;
    network = &quot;dev&quot; # or test to use testnet
}
</code></pre>
<h3 id="environment-variables"><a class="header" href="#environment-variables">environment variables</a></h3>
<p>should be recognizable as Env variables too</p>
<ul>
<li><code>MNEMONICS</code></li>
<li><code>NETWORK</code></li>
<li><code>SUBSTRATE_URL</code> </li>
<li><code>RMB_PROXY_URL</code></li>
<li><code>GRAPHQL_URL</code></li>
</ul>
<p>The *_URL variables can be used to override the dafault urls associated with the specified network</p>
<h3 id="credential-file"><a class="header" href="#credential-file">credential file</a></h3>
<pre><code>provider &quot;threefold&quot; {
    creds_file = &quot;...&quot;
}

</code></pre>
<h3 id="remarks"><a class="header" href="#remarks">Remarks</a></h3>
<ul>
<li>Grid terraform provider is hosted on terraform registry <a href="https://registry.terraform.io/providers/threefoldtech/grid/latest/docs?pollNotifications=true">here</a> </li>
<li>all provider input variables and their description can be found <a href="https://github.com/threefoldtech/terraform-provider-grid/blob/development/docs/index.md">here</a></li>
<li>capitalized environment variables can be used instead of writing them in the provider (e.g. MNEMONICS)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/advanced/./img//terraform_.png" alt=" " /></p>
<h1 id="deploying-a-vm-and-apply-provisioner-commands-after-deployment"><a class="header" href="#deploying-a-vm-and-apply-provisioner-commands-after-deployment">Deploying a VM and apply provisioner commands after deployment</a></h1>
<p>In this example we will see how to deploy a VM and apply provisioner commands on it on the Threefold grid v3</p>
<p>!!!code url:'https://github.com/threefoldtech/terraform-provider-grid/blob/development/examples/resources/external_provisioner/remote-exec_hello-world/main.tf'</p>
<h2 id="params-docs"><a class="header" href="#params-docs">Params docs</a></h2>
<h3 id="requirements-1"><a class="header" href="#requirements-1">Requirements</a></h3>
<ul>
<li>the machine should have <code>ssh server</code> running</li>
<li>the machine should have <code>scp</code> installed</li>
</ul>
<h3 id="connection-block"><a class="header" href="#connection-block">Connection Block</a></h3>
<ul>
<li>defines how we will connect to the deployed machine </li>
</ul>
<pre><code>   connection {
    type     = &quot;ssh&quot;
    user     = &quot;root&quot;
    agent    = true
    host     = grid_deployment.d1.vms[0].ygg_ip
  }
</code></pre>
<p>type: defines the used service to connect to
user: the connecting users
agent: if used the provisoner will use the default key to connect to the remote machine
host: the ip/host of the remote machine</p>
<h3 id="provisioner-block"><a class="header" href="#provisioner-block">Provisioner Block</a></h3>
<ul>
<li>defines the actual provisioner behaviour</li>
</ul>
<pre><code>   provisioner &quot;remote-exec&quot; {
    inline = [
      &quot;echo 'Hello world!' &gt; /root/readme.txt&quot;
    ]
  }
</code></pre>
<ul>
<li>remote-exec: the provisoner type we are willing to use can be remote, local or another type</li>
<li>inline: This is a list of command strings. They are executed in the order they are provided. This cannot be provided with script or scripts.</li>
<li>script: This is a path (relative or absolute) to a local script that will be copied to the remote resource and then executed. This cannot be provided with inline or scripts.</li>
<li>scripts: his is a list of paths (relative or absolute) to local scripts that will be copied to the remote resource and then executed. They are executed in the order they are provided. This cannot be provided with inline or script.</li>
</ul>
<h2 id="more-info-1"><a class="header" href="#more-info-1">More Info</a></h2>
<p>A complete list of provisioner parameters can be found <a href="https://www.terraform.io/language/resources/provisioners/remote-exec">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/advanced/./img//terraform_.png" alt=" " /></p>
<h1 id="deploying-a-vm-with-mounts-using-terraform"><a class="header" href="#deploying-a-vm-with-mounts-using-terraform">Deploying a VM with Mounts using terraform</a></h1>
<p>In this example we will see how to deploy a VM and mount disks on it on the Threefold grid v3</p>
<pre><code>terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtech/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
}

resource &quot;grid_network&quot; &quot;net1&quot; {
    nodes = [2, 4]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network&quot;
    description = &quot;newer network&quot;
}
resource &quot;grid_deployment&quot; &quot;d1&quot; {
  node = 2
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, 2, &quot;&quot;)
  disks {
    name = &quot;data&quot;
    size = 10
    description = &quot;volume holding app data&quot;
  }
  vms {
    name = &quot;vm1&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 1
    publicip = true
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    mounts {
        disk_name = &quot;data&quot;
        mount_point = &quot;/app&quot;
    }
    env_vars = {
      SSH_KEY = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;
    }
  }
  vms {
    name = &quot;anothervm&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;
    cpu = 1
    memory = 1024
    entrypoint = &quot;/sbin/zinit init&quot;
    env_vars = {
      SSH_KEY = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;
    }
  }
}
output &quot;wg_config&quot; {
    value = grid_network.net1.access_wg_config
}
output &quot;node1_zmachine1_ip&quot; {
    value = grid_deployment.d1.vms[0].ip
}
output &quot;node1_zmachine2_ip&quot; {
    value = grid_deployment.d1.vms[1].ip
}
output &quot;public_ip&quot; {
    value = grid_deployment.d1.vms[0].computedip
}
</code></pre>
<h2 id="more-info-2"><a class="header" href="#more-info-2">More Info</a></h2>
<p>A complete list of Mount workload parameters can be found <a href="https://github.com/threefoldtech/terraform-provider-grid/blob/development/docs/resources/deployment.md#nested-schema-for-vmsmounts">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/advanced/./img//terraform_.png" alt=" " /></p>
<h2 id="capacity-planning"><a class="header" href="#capacity-planning">Capacity planning</a></h2>
<p>In this example we will discuss capacity planning on top of Threefold grid v3</p>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<pre><code>terraform {
  required_providers {
    grid = {
      source = &quot;threefoldtechdev.com/providers/grid&quot;
    }
  }
}

provider &quot;grid&quot; {
}

resource &quot;grid_scheduler&quot; &quot;sched&quot; {
  # a machine for the first server instance
  requests {
    name = &quot;server1&quot;
    cru = 1
    sru = 256
    mru = 256
  }
  # a machine for the second server instance
  requests {
    name = &quot;server2&quot;
    cru = 1
    sru = 256
    mru = 256
  }
  # a name workload
  requests {
    name = &quot;gateway&quot;
    ipv4 = true
    farm = &quot;Freefarm&quot;
  }
}

resource &quot;grid_network&quot; &quot;net1&quot; {
    nodes = distinct([
      grid_scheduler.sched.nodes[&quot;server1&quot;],
      grid_scheduler.sched.nodes[&quot;server2&quot;]
    ])
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network2&quot;
    description = &quot;newer network&quot;
}

resource &quot;grid_deployment&quot; &quot;server1&quot; {
  node = grid_scheduler.sched.nodes[&quot;server1&quot;]
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, grid_scheduler.sched.nodes[&quot;server1&quot;], &quot;&quot;)
  vms {
    name = &quot;firstserver&quot;
    flist = &quot;https://hub.grid.tf/omar0.3bot/omarelawady-simple-http-server-latest.flist&quot;
    cpu = 1
    memory = 256
    rootfs_size = 256
    entrypoint = &quot;/main.sh&quot;
    env_vars {
      key = &quot;SSH_KEY&quot;
      value = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;
    }
    env_vars {
        key = &quot;PATH&quot;
        value = &quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;
    }

    planetary = true
  }
}

resource &quot;grid_deployment&quot; &quot;server2&quot; {
  node = grid_scheduler.sched.nodes[&quot;server2&quot;]
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, grid_scheduler.sched.nodes[&quot;server2&quot;], &quot;&quot;)
  vms {
    name = &quot;secondserver&quot;
    flist = &quot;https://hub.grid.tf/tf-official-apps/simple-http-server-latest.flist&quot;
    cpu = 1
    memory = 256
    rootfs_size = 256
    entrypoint = &quot;/main.sh&quot;
    env_vars {
      key = &quot;SSH_KEY&quot;
      value = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;
    }
    env_vars {
        key = &quot;PATH&quot;
        value = &quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;
    }

    planetary = true
  }
}

resource &quot;grid_fqdn_proxy&quot; &quot;balancer&quot; {
  node = grid_scheduler.sched.nodes[&quot;gateway&quot;]
  name = &quot;balancer&quot;
  fqdn = &quot;remote.omar.grid.tf&quot;
  backends = [format(&quot;http://[%s]&quot;, grid_deployment.server1.vms[0].ygg_ip), format(&quot;http://[%s]&quot;, grid_deployment.server2.vms[0].ygg_ip)]
  tls_passthrough = false
}


output &quot;load_balancer_domain&quot; {
    value = grid_fqdn_proxy.balancer.fqdn
}
</code></pre>
<h3 id="preparing-the-requests"><a class="header" href="#preparing-the-requests">preparing the requests</a></h3>
<pre><code class="language-terraform">resource &quot;grid_scheduler&quot; &quot;sched&quot; {
  # a machine for the first server instance
  requests {
    name = &quot;server1&quot;
    cru = 1
    sru = 256
    mru = 256
  }
  # a machine for the second server instance
  requests {
    name = &quot;server2&quot;
    cru = 1
    sru = 256
    mru = 256
  }
  # a name workload
  requests {
    name = &quot;gateway&quot;
    ipv4 = true
    farm = &quot;Freefarm&quot;
  }
}
</code></pre>
<p>Here we define a <code>list</code> of requests, each request has a name and filter options e.g <code>cru</code>, <code>sru</code>, <code>mru</code>, having <code>ipv4</code> or not </p>
<p>And after that in our code we can reference the grid_scheduler object with the request name to be used instead of node_id </p>
<p>for example</p>
<pre><code class="language-terraform">resource &quot;grid_deployment&quot; &quot;server1&quot; {
  node = grid_scheduler.sched.nodes[&quot;server1&quot;]
  network_name = grid_network.net1.name
  ip_range = lookup(grid_network.net1.nodes_ip_range, grid_scheduler.sched.nodes[&quot;server1&quot;], &quot;&quot;)
  vms {
    name = &quot;firstserver&quot;
    flist = &quot;https://hub.grid.tf/omar0.3bot/omarelawady-simple-http-server-latest.flist&quot;
    cpu = 1
    memory = 256
    rootfs_size = 256
    entrypoint = &quot;/main.sh&quot;
    env_vars = {
      SSH_KEY = &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtCuUUCZGLZ4NoihAiUK8K0kSoTR1WgIaLQKqMdQ/99eocMLqJgQMRIp8lueFG7SpcgXVRzln8KNKZX1Hm8lcrXICr3dnTW/0bpEnF4QOGLYZ/qTLF5WmoCgKyJ6WO96GjWJBsZPads+RD0WeiijV7jj29lALsMAI8CuOH0pcYUwWsRX/I1z2goMPNRY+PBjknMYFXEqizfUXqUnpzF3w/bKe8f3gcrmOm/Dxh1nHceJDW52TJL/sPcl6oWnHZ3fY4meTiAS5NZglyBF5oKD463GJnMt/rQ1gDNl8E4jSJUArN7GBJntTYxFoFo6zxB1OsSPr/7zLfPG420+9saBu9yN1O9DlSwn1ZX+Jg0k7VFbUpKObaCKRmkKfLiXJdxkKFH/+qBoCCnM5hfYxAKAyQ3YCCP/j9wJMBkbvE1QJMuuoeptNIvSQW6WgwBfKIK0shsmhK2TDdk0AHEnzxPSkVGV92jygSLeZ4ur/MZqWDx/b+gACj65M3Y7tzSpsR76M= omar@omar-Predator-PT315-52&quot;
    }
    env_vars = {
        PATH = &quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;
    }

    planetary = true
  }
}
</code></pre>
<blockquote>
<p>Note: you need to call <code>distinct</code> while specifying the nodes in the network, because the scheduler may assign server1, server2 on the same node.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p><img src="terraform/advanced/./img//terraform_.png" alt=" " /></p>
<h2 id="updating-not-supported"><a class="header" href="#updating-not-supported">Updating (NOT SUPPORTED)</a></h2>
<p>Some of the updates are working, but the code is not finished, use on your own risk</p>
<p>Updates are triggered by changing the deployments fields.</p>
<pre><code>resource &quot;grid_network&quot; &quot;net&quot; {
    nodes = [2, 4]
    ip_range = &quot;10.1.0.0/16&quot;
    name = &quot;network&quot;
    description = &quot;newer network&quot;
}

</code></pre>
<ul>
<li>add node 4 to the network</li>
<li>the version of the workload needs to get updated internally</li>
<li>the version of the deployment needs to get updated internally</li>
<li>update the hash in the contract (the contract id will stay the same)</li>
<li>there are workloads that doesn't support in-place updates (e.g. Zmachines). To change them there are a couple of options (all performs destroy/create so data can be lost):</li>
</ul>
<ol>
<li><code>terraform taint grid_deployment.d1</code> (next apply will destroy ALL workloads within grid_deployment.d1 and create a new deployment)</li>
<li><code>terraform destroy --target grid_deployment.d1 --parallelism=1 &amp;&amp; terraform apply --target grid_deployment.d1 --parallelism=1</code> (same as above)</li>
<li>remove the vm, then execute a <code>terraform apply</code>, then add the vm with the new config (this performs two updates but keeps neighboring workloads inside the same deployment intact)
dispatching the updated deployment object to <code>zos.deployment.update</code> of the node </li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p>This section covers developing projects on top of Threefold Grid using Javascript language.</p>
<p>Javascript has a huge ecosystem, and first class citzen when it comes to blockchain technologies like substrate and that was one of the reasons for it to become one the very first supported languages on the grid</p>
<p>Please make sure to check the <a href="javascript/../getstarted/tfgrid3_getstarted.html">basics</a> before continuing</p>
<p>all code examples can be found <a href="https://github.com/threefoldtech/grid3_client_ts/tree/development/scripts">here</a> </p>
<div style="break-before: page; page-break-before: always;"></div><p>The <a href="https://github.com/threefoldtech/grid3_client_ts">client</a> is written using typescript to provide more convinient, type-checked code and it is used to deploy workloads like Virtual machines, kubernetes clusters, quantum storage, .. etc</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<ul>
<li>node 14.4.0 or higher </li>
<li>npm 6.14.5 or higher</li>
</ul>
<blockquote>
<p><a href="https://nvm.sh/">nvm</a> is the recommended way for installing node.</p>
</blockquote>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<h3 id="external-package"><a class="header" href="#external-package">External package</a></h3>
<pre><code class="language-bash">npm install grid3_client
</code></pre>
<p>or</p>
<pre><code class="language-bash">yarn add grid3_client
</code></pre>
<h3 id="local-usage"><a class="header" href="#local-usage">Local usage</a></h3>
<ul>
<li>Clone the repository</li>
</ul>
<pre><code class="language-bash">git clone https://github.com/threefoldtech/grid3_client_ts.git
</code></pre>
<ul>
<li>Install it</li>
</ul>
<pre><code class="language-bash">npm install
</code></pre>
<p>or</p>
<pre><code class="language-bash">yarn install
</code></pre>
<p>From now on in the document we will assume you already have</p>
<ul>
<li>account </li>
<li>twin</li>
<li>mnemonics</li>
</ul>
<p>If you don't, please visit the <a href="javascript/../getstarted/tfgrid3_getstarted.html">Get started section</a></p>
<p>We provided set of scripts to play around with in the repository in the <code>scripts</code> directory.</p>
<h2 id="how-to-run-the-scripts"><a class="header" href="#how-to-run-the-scripts">How to run the scripts</a></h2>
<ul>
<li>Set your grid3 client configuration in <code>scripts/client_loader.ts</code> or easily use one of <code>config.json</code></li>
<li>update your customized deployments specs</li>
<li>Run using <a href="https://www.npmjs.com/ts-node">ts-node</a></li>
</ul>
<pre><code class="language-bash">npx ts-node --project tsconfig-node.json scripts/zdb.ts
</code></pre>
<p>or</p>
<pre><code class="language-bash">yarn run ts-node --project tsconfig-node.json scripts/zdb.ts
</code></pre>
<h2 id="reference-api"><a class="header" href="#reference-api">Reference API</a></h2>
<p>Still in progress, but you can check always <a href="https://threefoldtech.github.io/grid3_client_ts/api/">here</a></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="client-configurations"><a class="header" href="#client-configurations">client configurations</a></h2>
<p>grid 3 client supports communication over RMB <code>MessageBusClient</code> or HTTP <code>HTTPMessageBusClient</code> using one of the deployed grid3 proxies.</p>
<pre><code class="language-typescript">import fs from &quot;fs&quot;;
import path from &quot;path&quot;;
import { GridClient } from &quot;../src/client&quot;;

import { MessageBusClientInterface } from &quot;ts-rmb-client-base&quot;;
import { HTTPMessageBusClient } from &quot;ts-rmb-http-client&quot;;
import { MessageBusClient } from &quot;ts-rmb-redis-client&quot;;

</code></pre>
<p>So according to your scenario you choose the communication method</p>
<h2 id="example-configuration-object"><a class="header" href="#example-configuration-object">example configuration object</a></h2>
<pre><code class="language-json">{
    &quot;network&quot;: &quot;dev&quot;,
    &quot;mnemonic&quot;: &quot;&quot;,
    &quot;rmb_proxy&quot;: true,
    &quot;storeSecret&quot;: &quot;secret&quot;
}
</code></pre>
<p>So all configurations that is needed are</p>
<ul>
<li>network: <code>dev</code> for devnet, <code>test</code> for testnet</li>
<li>mnemonics</li>
<li>rmb_proxy: to use the https RMB proxy to reduce the dependencies on having <code>redis</code> and <code>yggrassil</code> on the same host, and also to be usable too from the browser if needed.</li>
</ul>
<h2 id="creating-a-client"><a class="header" href="#creating-a-client">creating a client</a></h2>
<pre><code class="language-typescript">async function getClient(): Promise&lt;GridClient&gt; {
    let rmb: MessageBusClientInterface;
    if (config.rmb_proxy) {
        rmb = new HTTPMessageBusClient(0, &quot;&quot;);
    } else {
        rmb = new MessageBusClient();
    }
    const gridClient = new GridClient(config.network, config.mnemonic, config.storeSecret, rmb, &quot;&quot;, BackendStorageType.auto, KeypairType.sr25519);
    await gridClient.connect();
    return gridClient;
}
</code></pre>
<p>The grid client requires a communication transport, the availlable options are <code>MessageBusClient</code> that works over the RMB, and <code>HTTPMessageBusClient</code> using relay proxies </p>
<blockquote>
<p>HTTPMessageBusClient is very highlevel, and allows easier integration with many languages because it's http based, also it enables the whole space of web applications. It's safe to assume that we will be using it from now on.</p>
</blockquote>
<ul>
<li>communication transport: RMB or over RMB proxy via HTTP</li>
<li>network: <code>dev</code> for devnet, <code>test</code> for testnet</li>
<li>mnemonics: used for signing the requests.</li>
<li>storeSecret: used to encrypt data while storing in backeds</li>
<li>BackendStorage : can be <code>auto</code> which willl automatically adapt if running in node environment to use <code>filesystem backend</code> or the browser enviornment to use <code>localstorage backend</code>. Also you can set it to <code>kvstore</code> to use the tfchain keyvalue store module.</li>
<li>keypairType: is defaulted to <code>sr25519</code>, most likely you will never need to change it. <code>ed25519</code> is supported too.</li>
</ul>
<blockquote>
<p>Note: The choice of the node is completely up to the user at this point. They need to do the capacity planning. Check <a href="javascript/../dashboard/explorer/explorer_home.html">Exploring Capacity</a> to know which nodes fits your deployment criteria.</p>
</blockquote>
<p>Check the document for <a href="javascript/../javascript/grid3_javascript_capacity_planning.html">capacity planning using code</a> if you want to automate it</p>
<blockquote>
<p>Note: this feature is still experimental</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h2 id="deploying-a-vm"><a class="header" href="#deploying-a-vm">Deploying a VM</a></h2>
<h3 id="example-code"><a class="header" href="#example-code">Example code</a></h3>
<pre><code>import { DiskModel, MachineModel, MachinesModel, NetworkModel } from &quot;../src&quot;;
import { config, getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

// create network Object
const n = new NetworkModel();
n.name = &quot;wedtest&quot;;
n.ip_range = &quot;10.249.0.0/16&quot;;

// create disk Object
const disk = new DiskModel();
disk.name = &quot;wedDisk&quot;;
disk.size = 8;
disk.mountpoint = &quot;/testdisk&quot;;

// create vm node Object
const vm = new MachineModel();
vm.name = &quot;testvm&quot;;
vm.node_id = 17;
vm.disks = [disk];
vm.public_ip = false;
vm.planetary = true;
vm.cpu = 1;
vm.memory = 1024 * 2;
vm.rootfs_size = 0;
vm.flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;;
vm.entrypoint = &quot;/sbin/zinit init&quot;;
vm.env = {
    SSH_KEY: config.ssh_key,
};
// vm.ip = &quot;10.249.2.5&quot; // create a machine with specific private ip

// create VMs Object
const vms = new MachinesModel();
vms.name = &quot;newVMS&quot;;
vms.network = n;
vms.machines = [vm];
vms.metadata = &quot;{'testVMs': true}&quot;;
vms.description = &quot;test deploying VMs via ts grid3 client&quot;;

async function main() {
    const grid3 = await getClient();

    // deploy vms
    const res = await grid3.machines.deploy(vms);
    log(res);

    // get the deployment
    const l = await grid3.machines.getObj(vms.name);
    log(l);

    // // delete
    // const d = await grid3.machines.delete({ name: vms.name });
    // log(d);

    await grid3.disconnect();
}

main();
</code></pre>
<h3 id="detailed-explanation"><a class="header" href="#detailed-explanation">Detailed explanation</a></h3>
<h4 id="building-network"><a class="header" href="#building-network">building network</a></h4>
<pre><code class="language-typescript">// create network Object
const n = new NetworkModel();
n.name = &quot;montest&quot;;
n.ip_range = &quot;10.232.0.0/16&quot;;
</code></pre>
<p>Here we prepare the network model that is going to be used by specifying a name to our network and the range it will be spanning over</p>
<h3 id="building-the-disk-model"><a class="header" href="#building-the-disk-model">building the disk model</a></h3>
<pre><code class="language-typescript">// create disk Object
const disk = new DiskModel();
disk.name = &quot;newDisk&quot;;
disk.size = 10;
disk.mountpoint = &quot;/newDisk&quot;;
</code></pre>
<p>here we create the disk model specifying its name, size in GB and where it will be mounted eventually</p>
<h3 id="building-the-vm"><a class="header" href="#building-the-vm">building the VM</a></h3>
<pre><code class="language-typescript">// create vm node Object
const vm = new MachineModel();
vm.name = &quot;testvm&quot;;
vm.node_id = 4;
vm.disks = [disk];
vm.public_ip = false;
vm.planetary = true;
vm.cpu = 1;
vm.memory = 1024 * 2;
vm.rootfs_size = 1;
vm.flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;;
vm.entrypoint = &quot;/sbin/zinit init&quot;;
vm.env = {
    SSH_KEY:
        &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDWlguBuvfQikkRJZXkLPei7Scvo/OULUEvjWVR4tCZ5V85P2F4SsSghxpRGixCNc7pNtgvdwJegK06Tn7SkV2jYJ9kBJh8PA06CPSz1mnpco4cgktiWx/R8xBvLGlyO0BwUuD3/WFjrc6fzH9E7Bpkel/xTnacx14w1bZAC1R35hz7BaHu1WrXsfxEd0VH7gpMPoQ4+l+H38ULPTiC+JcOKJOqVafgcc0sU7otXbgCa1Frr4QE5bwiMYhOlsRfRv/hf08jYsVo+RUO3wD12ylLWR7a7sJDkBBwgir8SwAvtRlT6k9ew9cDMQ7H8iWNCOg2xqoTLpVag6RN9kGzA5LGL+qHEcBr6gd2taFEy9+mt+TWuKp6reUeJfTu9RD1UgB0HpcdgTHtoUTISW7Mz4KNkouci2DJFngDWrLRxRoz81ZwfI2hjFY0PYDzF471K7Nwwt3qKYF1Js9a6VO38tMxSU4mTO83bt+dUFozgpw2Y0KKJGHDwU66i2MvTPg3EGs= ayoub@ayoub-Inspiron-3576&quot;,
};

</code></pre>
<p>Now we go to the VM model, that will be used to build our <code>zmachine</code> object</p>
<p>We need to specify its</p>
<ul>
<li>name</li>
<li>node_id: where it will get deployed</li>
<li>disks: disks model collection</li>
<li>memory</li>
<li>root filesystem size</li>
<li>flist: the image it is going to start from. Check the <a href="javascript//manual3_iac/grid3_supported_flists.html">supported flists</a></li>
<li>entry point: entrypoint command / script to execute</li>
<li>env: has the environment variables needed e.g sshkeys used</li>
<li>public ip: if we want to have a public ip attached to the VM</li>
<li>planetary: to enable planetary network on VM</li>
</ul>
<h3 id="building-vms-collection"><a class="header" href="#building-vms-collection">building VMs collection</a></h3>
<pre><code class="language-typescript">// create VMs Object
const vms = new MachinesModel();
vms.name = &quot;monVMS&quot;;
vms.network = n;
vms.machines = [vm];
vms.metadata = &quot;{'testVMs': true}&quot;;
vms.description = &quot;test deploying VMs via ts grid3 client&quot;;
</code></pre>
<p>Here it's quite simple we can add one or more VM to the <code>machines</code> property to have them deployed as part of our project</p>
<h3 id="deployment"><a class="header" href="#deployment">deployment</a></h3>
<pre><code class="language-typescript">// deploy vms
const res = await grid3.machines.deploy(vms);
log(res);
</code></pre>
<h3 id="getting-deployment-information"><a class="header" href="#getting-deployment-information">getting deployment information</a></h3>
<p>can do so based on the name you gave to the <code>vms</code> collection</p>
<pre><code class="language-typescript">// get the deployment
const l = await grid3.machines.getObj(vms.name);
log(l);
</code></pre>
<h3 id="deleting-a-deployment"><a class="header" href="#deleting-a-deployment">deleting a deployment</a></h3>
<pre><code class="language-typescript">// delete
const d = await grid3.machines.delete({ name: vms.name });
log(d);
</code></pre>
<p>In the underlying layer we cancel the contracts that were created on the chain and as a result all of the workloads tied to his project will get deleted.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="capacity-planning-1"><a class="header" href="#capacity-planning-1">capacity planning</a></h2>
<p>It's almost the same as in <a href="javascript/../javascript/grid3_javascript_vm.html">deploying a single VM</a> the only difference is you can automate the choice of the node to deploy on using code. We now support <code>FilterOptions</code> to filter nodes based on specific criteria e.g the node resources (CRU, SRU, HRU, MRU) or being part of a specific farm or located in some country, or being a gateway or not </p>
<pre><code>FilterOptions: { accessNodeV4?: boolean; accessNodeV6?: boolean; city?: string; country?: string; cru?: number; farm?: number; gateway?: boolean; hru?: number; mru?: number; sru?: number }
</code></pre>
<pre><code>import { DiskModel, FilterOptions, MachineModel, MachinesModel, NetworkModel } from &quot;../src&quot;;
import { config, getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

async function main() {
    const grid3 = await getClient();

    // create network Object
    const n = new NetworkModel();
    n.name = &quot;dynamictest&quot;;
    n.ip_range = &quot;10.249.0.0/16&quot;;

    // create disk Object
    const disk = new DiskModel();
    disk.name = &quot;dynamicDisk&quot;;
    disk.size = 8;
    disk.mountpoint = &quot;/testdisk&quot;;

    const vmQueryOptions: FilterOptions = {
        cru: 1,
        mru: 2, // GB
        country: &quot;Belgium&quot;,
    };

    // create vm node Object
    const vm = new MachineModel();
    vm.name = &quot;testvm&quot;;
    vm.node_id = +(await grid3.capacity.filterNodes(vmQueryOptions))[0].nodeId; // TODO: allow random choise
    vm.disks = [disk];
    vm.public_ip = false;
    vm.planetary = true;
    vm.cpu = 1;
    vm.memory = 1024 * 2;
    vm.rootfs_size = 0;
    vm.flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;;
    vm.entrypoint = &quot;/sbin/zinit init&quot;;
    vm.env = {
        SSH_KEY: config.ssh_key,
    };

    // create VMs Object
    const vms = new MachinesModel();
    vms.name = &quot;dynamicVMS&quot;;
    vms.network = n;
    vms.machines = [vm];
    vms.metadata = &quot;{'testVMs': true}&quot;;
    vms.description = &quot;test deploying VMs via ts grid3 client&quot;;

    // deploy vms
    const res = await grid3.machines.deploy(vms);
    log(res);

    // get the deployment
    const l = await grid3.machines.getObj(vms.name);
    log(l);

    // // delete
    // const d = await grid3.machines.delete({ name: vms.name });
    // log(d);

    await grid3.disconnect();
}

main();
</code></pre>
<p>In this example you can notice the criteria for <code>server1</code></p>
<pre><code class="language-typescript">const server1_options: FilterOptions = {
    cru: 1,
    mru: 2, // GB
    country: &quot;Belgium&quot;,
};

</code></pre>
<p>Here we want all the nodes with <code>CRU:20</code> and <code>MRU:100</code> and located in <code>Belgium</code></p>
<blockquote>
<p>Note some libraries allow reverse lookup of countries codes by name e.g <a href="https://www.npmjs.com/package/i18n-iso-countries">i18n-iso-countries</a></p>
</blockquote>
<p>and then in the MachineModel, we specified the <code>node_id</code> to be the first value of our filteration </p>
<pre><code class="language-typescript">vm.node_id = +(await nodes.filterNodes(server1_options))[0].nodeId;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="deploying-multiple-vms"><a class="header" href="#deploying-multiple-vms">Deploying multiple VMs</a></h2>
<h3 id="example-code-1"><a class="header" href="#example-code-1">Example code</a></h3>
<pre><code>import { DiskModel, FilterOptions, MachineModel, MachinesModel, NetworkModel } from &quot;../src&quot;;
import { config, getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

async function main() {
    const grid3 = await getClient();

    // create network Object
    const n = new NetworkModel();
    n.name = &quot;monNetwork&quot;;
    n.ip_range = &quot;10.238.0.0/16&quot;;

    // create disk Object
    const disk1 = new DiskModel();
    disk1.name = &quot;newDisk1&quot;;
    disk1.size = 10;
    disk1.mountpoint = &quot;/newDisk1&quot;;

    const vmQueryOptions: FilterOptions = {
        cru: 1,
        mru: 2, // GB
        sru: 10,
        farmId: 1,
    };

    // create vm node Object
    const vm1 = new MachineModel();
    vm1.name = &quot;testvm1&quot;;
    vm1.node_id = +(await grid3.capacity.filterNodes(vmQueryOptions))[0].nodeId;
    vm1.disks = [disk1];
    vm1.public_ip = false;
    vm1.planetary = true;
    vm1.cpu = 1;
    vm1.memory = 1024 * 2;
    vm1.rootfs_size = 0;
    vm1.flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;;
    vm1.entrypoint = &quot;/sbin/zinit init&quot;;
    vm1.env = {
        SSH_KEY: config.ssh_key,
    };

    // create disk Object
    const disk2 = new DiskModel();
    disk2.name = &quot;newDisk2&quot;;
    disk2.size = 10;
    disk2.mountpoint = &quot;/newDisk2&quot;;

    // create another vm node Object
    const vm2 = new MachineModel();
    vm2.name = &quot;testvm2&quot;;
    vm2.node_id = +(await grid3.capacity.filterNodes(vmQueryOptions))[1].nodeId;
    vm2.disks = [disk2];
    vm2.public_ip = false;
    vm2.planetary = true;
    vm2.cpu = 1;
    vm2.memory = 1024 * 2;
    vm2.rootfs_size = 0;
    vm2.flist = &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;;
    vm2.entrypoint = &quot;/sbin/zinit init&quot;;
    vm2.env = {
        SSH_KEY: config.ssh_key,
    };

    // create VMs Object
    const vms = new MachinesModel();
    vms.name = &quot;monVMS&quot;;
    vms.network = n;
    vms.machines = [vm1, vm2];
    vms.metadata = &quot;{'testVMs': true}&quot;;
    vms.description = &quot;test deploying VMs via ts grid3 client&quot;;

    // deploy vms
    const res = await grid3.machines.deploy(vms);
    log(res);

    // get the deployment
    const l = await grid3.machines.getObj(vms.name);
    log(l);

    // // delete
    // const d = await grid3.machines.delete({ name: vms.name });
    // log(d);

    await grid3.disconnect();
}

main();
</code></pre>
<p>It's similiar to the previous section of <a href="javascript/../javascript/grid3_javascript_vm.html">deploying a single VM</a>, but just adds more vm objects to vms collection. </p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="deploying-caprover-leader-node"><a class="header" href="#deploying-caprover-leader-node">Deploying Caprover Leader Node</a></h2>
<h3 id="example-code-2"><a class="header" href="#example-code-2">Example code</a></h3>
<pre><code>import { DiskModel, FilterOptions, MachineModel, MachinesModel, NetworkModel } from &quot;../src&quot;;
import { config, getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

async function main() {
    const grid3 = await getClient();

    const vmQueryOptions: FilterOptions = {
        cru: 4,
        mru: 4, // GB
        sru: 10,
        farmId: 1,
    };

    const CAPROVER_FLIST = &quot;https://hub.grid.tf/tf-official-apps/tf-caprover-main.flist&quot;;
    // create network Object
    const n = new NetworkModel();
    n.name = &quot;wedtest&quot;;
    n.ip_range = &quot;10.249.0.0/16&quot;;

    // create disk Object
    const disk = new DiskModel();
    disk.name = &quot;wedDisk&quot;;
    disk.size = 10;
    disk.mountpoint = &quot;/var/lib/docker&quot;;

    // create vm node Object
    const vm = new MachineModel();
    vm.name = &quot;testvm&quot;;
    vm.node_id = +(await grid3.capacity.filterNodes(vmQueryOptions))[0].nodeId;
    vm.disks = [disk];
    vm.public_ip = true;
    vm.planetary = false;
    vm.cpu = 4;
    vm.memory = 1024 * 4;
    vm.rootfs_size = 0;
    vm.flist = CAPROVER_FLIST;
    vm.entrypoint = &quot;/sbin/zinit init&quot;;
    vm.env = {
        PUBLIC_KEY: config.ssh_key,
        SWM_NODE_MODE: &quot;leader&quot;,
        CAPROVER_ROOT_DOMAIN: &quot;rafy.grid.tf&quot;, // update me
        DEFAULT_PASSWORD: &quot;captain42&quot;,
        CAPTAIN_IMAGE_VERSION: &quot;v1.2.1&quot;,
    };

    // create VMs Object
    const vms = new MachinesModel();
    vms.name = &quot;newVMS5&quot;;
    vms.network = n;
    vms.machines = [vm];
    vms.metadata = &quot;{'testVMs': true}&quot;;
    vms.description = &quot;caprover leader machine/node&quot;;

    // deploy vms
    const res = await grid3.machines.deploy(vms);
    log(res);

    // get the deployment
    const l = await grid3.machines.getObj(vms.name);
    log(l);

    log(`You can access Caprover via the browser using: https://captain.${vm.env.CAPROVER_ROOT_DOMAIN}`);

    // // delete
    // const d = await grid3.machines.delete({ name: vms.name });
    // log(d);

    await grid3.disconnect();
}

main();
</code></pre>
<h3 id="detailed-explanation-1"><a class="header" href="#detailed-explanation-1">Detailed explanation</a></h3>
<p>So this deployment is almost similiar to what we have in the <a href="javascript/./grid3_javascript_vm.html">vm deployment section</a>. We only have different environment variables</p>
<h6 id="env-variables-in-leader-node"><a class="header" href="#env-variables-in-leader-node">Env. variables in Leader Node</a></h6>
<ul>
<li>PUBLIC_KEY: Your public IP to be able to access the VM.</li>
<li>SWM_NODE_MODE: Caprover Node type which must be <code>leader</code> as we are deploying a leader node.</li>
<li>CAPROVER_ROOT_DOMAIN: The domain which you we will use to bind the deployed VM.</li>
<li>DEFAULT_PASSWORD: Caprover default password you want to deploy with.</li>
</ul>
<p>For further details about Leader node deployment please <a href="https://github.com/freeflowuniverse/freeflow_caprover#a-leader-node-deploymentsetup">check</a></p>
<h2 id="deploying-caprover-worker-node"><a class="header" href="#deploying-caprover-worker-node">Deploying Caprover Worker Node</a></h2>
<h3 id="example-code-3"><a class="header" href="#example-code-3">Example code</a></h3>
<pre><code>import { DiskModel, FilterOptions, MachineModel, MachinesModel, NetworkModel } from &quot;../src&quot;;
import { config, getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

async function main() {
    const grid3 = await getClient();

    const vmQueryOptions: FilterOptions = {
        cru: 4,
        mru: 4, // GB
        sru: 10,
        farmId: 1,
    };

    const CAPROVER_FLIST = &quot;https://hub.grid.tf/tf-official-apps/tf-caprover-main.flist&quot;;
    // create network Object
    const n = new NetworkModel();
    n.name = &quot;wedtest&quot;;
    n.ip_range = &quot;10.249.0.0/16&quot;;

    // create disk Object
    const disk = new DiskModel();
    disk.name = &quot;wedDisk&quot;;
    disk.size = 10;
    disk.mountpoint = &quot;/var/lib/docker&quot;;

    // create vm node Object
    const vm = new MachineModel();
    vm.name = &quot;capworker1&quot;;
    vm.node_id = +(await grid3.capacity.filterNodes(vmQueryOptions))[0].nodeId;
    vm.disks = [disk];
    vm.public_ip = true;
    vm.planetary = false;
    vm.cpu = 4;
    vm.memory = 1024 * 4;
    vm.rootfs_size = 0;
    vm.flist = CAPROVER_FLIST;
    vm.entrypoint = &quot;/sbin/zinit init&quot;;
    vm.env = {
        // These env. vars needed to be changed based on the leader node.
        PUBLIC_KEY: config.ssh_key,
        SWM_NODE_MODE: &quot;worker&quot;,
        SWMTKN: &quot;SWMTKN-1-1eikxeyat4br9t4la1dnln11l1tvlnrngzwh5iq68m2vn7edi1-6lc6xtw3pzd99lrowyuayr5yv&quot;,
        LEADER_PUBLIC_IP: &quot;185.206.122.157&quot;,
        CAPTAIN_IMAGE_VERSION: &quot;v1.2.1&quot;,
    };

    // create VMs Object
    const vms = new MachinesModel();
    vms.name = &quot;newVMS6&quot;;
    vms.network = n;
    vms.machines = [vm];
    vms.metadata = &quot;{'testVMs': true}&quot;;
    vms.description = &quot;caprover worker machine/node&quot;;

    // deploy vms
    const res = await grid3.machines.deploy(vms);
    log(res);

    // get the deployment
    const l = await grid3.machines.getObj(vms.name);
    log(l);

    // // delete
    // const d = await grid3.machines.delete({ name: vms.name });
    // log(d);

    await grid3.disconnect();
}

main();
</code></pre>
<p>Before worker node deployment:</p>
<ul>
<li>Get token from the leader node</li>
<li>Get leader node public IP</li>
</ul>
<p>For futher inforamtion please <a href="https://github.com/freeflowuniverse/freeflow_caprover#step-4-access-the-captain-dashboard">check</a></p>
<p>to deploy a worker Node it has the same details as a leader node regarding the deployment details except environment variables.</p>
<h6 id="env-variables-in-worker-node"><a class="header" href="#env-variables-in-worker-node">Env. variables in worker Node</a></h6>
<ul>
<li>PUBLIC_KEY: Your public IP to be able to access the VM.</li>
<li>SWM_NODE_MODE: Caprover Node type which must be <code>worker</code> as we are deploying a worker node.</li>
<li>SWMTKN: Token generated on the leader node to allow the worker node to join the docker swarm network </li>
<li>LEADER_PUBLIC_IP: Leader node public IP.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>!!!include:grid3_javascript_loadclient</p>
<h2 id="deploying-a-vm-and-exposing-it-over-a-gateway-prefix"><a class="header" href="#deploying-a-vm-and-exposing-it-over-a-gateway-prefix">Deploying a VM and exposing it over a Gateway Prefix</a></h2>
<p>After the <a href="javascript/../javascript/grid3_javascript_vm.html">deployment of a VM</a>, now it's time to expose it to the world </p>
<h3 id="example-code-4"><a class="header" href="#example-code-4">Example code</a></h3>
<pre><code>import { FilterOptions, GatewayNameModel } from &quot;../src&quot;;
import { getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

// read more about the gateway types in this doc: https://github.com/threefoldtech/zos/tree/main/docs/gateway
async function main() {
    const grid3 = await getClient();

    const gatewayQueryOptions: FilterOptions = {
        gateway: true,
        farmId: 1,
    };

    const gw = new GatewayNameModel();
    gw.name = &quot;test&quot;;
    gw.node_id = +(await grid3.capacity.filterNodes(gatewayQueryOptions))[0].nodeId;
    gw.tls_passthrough = false;
    // the backends have to be in this format `http://ip:port` or `https://ip:port`, and the `ip` pingable from the node so using the ygg ip or public ip if available.
    gw.backends = [&quot;http://185.206.122.35:8000&quot;];

    // deploy
    const res = await grid3.gateway.deploy_name(gw);
    log(res);

    // get the deployment
    const l = await grid3.gateway.getObj(gw.name);
    log(l);

    // // delete
    // const d = await grid3.gateway.delete_name({ name: gw.name });
    // log(d);

    grid3.disconnect();
}

main();

</code></pre>
<h3 id="detailed-explanation-2"><a class="header" href="#detailed-explanation-2">Detailed explanation</a></h3>
<pre><code class="language-javascript">const gw = new GatewayNameModel();
gw.name = &quot;ayoubtest&quot;;
gw.node_id = 1;
gw.tls_passthrough = false;
gw.backends = [&quot;http://185.206.122.35:8000&quot;];
</code></pre>
<ul>
<li>we created a gateway name model and gave it a <code>name</code> -that's why it's called GatewayName- <code>ayoubtest</code> to be deployed on gateway node <code>1</code> to end up with a domain <code>ayoubtest.ghent01.devnet.grid.tf</code>, </li>
<li>we create a proxy for the gateway to send the traffic coming to <code>example2.ghent01.devnet.grid.tf</code> to the backend  <code>http://185.206.122.35</code>, we say <code>tls_passthrough is false</code> to let the gateway terminate the traffic, if you replace it with <code>true</code> your backend service needs to be able to do the TLS termination</li>
</ul>
<h4 id="deploying"><a class="header" href="#deploying">deploying</a></h4>
<pre><code class="language-typescript">// deploy
const res = await grid3.gateway.deploy_name(gw);
log(res);
</code></pre>
<p>this deploys <code>GatewayName</code> on the grid</p>
<h4 id="getting-deployment-object"><a class="header" href="#getting-deployment-object">getting deployment object</a></h4>
<pre><code class="language-typescript">const l = await grid3.gateway.getObj(gw.name);
log(l);
</code></pre>
<p>getting the deployment information can be done using <code>getObj</code></p>
<h4 id="deletion"><a class="header" href="#deletion">deletion</a></h4>
<pre><code class="language-typescript">const d = await grid3.gateway.delete_name({ name: gw.name });
log(d);
</code></pre>
<h2 id="deploying-a-vm-and-exposing-it-over-a-gateway-using-a-full-domain"><a class="header" href="#deploying-a-vm-and-exposing-it-over-a-gateway-using-a-full-domain">Deploying a VM and exposing it over a Gateway using a Full domain</a></h2>
<p>After the <a href="javascript/javascript/grid3_javascript_vm.html">deployment of a VM</a>, now it's time to expose it to the world </p>
<h3 id="example-code-5"><a class="header" href="#example-code-5">Example code</a></h3>
<pre><code>import { FilterOptions, GatewayFQDNModel } from &quot;../src&quot;;
import { getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

// read more about the gateway types in this doc: https://github.com/threefoldtech/zos/tree/main/docs/gateway
async function main() {
    const grid3 = await getClient();

    const gatewayQueryOptions: FilterOptions = {
        gateway: true,
        farmId: 1,
    };
    const gw = new GatewayFQDNModel();
    gw.name = &quot;applyFQDN&quot;;
    gw.node_id = +(await grid3.capacity.filterNodes(gatewayQueryOptions))[0].nodeId;
    gw.fqdn = &quot;test.hamada.grid.tf&quot;;
    gw.tls_passthrough = false;
    // the backends have to be in this format `http://ip:port` or `https://ip:port`, and the `ip` pingable from the node so using the ygg ip or public ip if available.
    gw.backends = [&quot;http://185.206.122.35:8000&quot;];

    // deploy
    const res = await grid3.gateway.deploy_fqdn(gw);
    log(res);

    // get the deployment
    const l = await grid3.gateway.getObj(gw.name);
    log(l);

    // // delete
    // const d = await grid3.gateway.delete_fqdn({ name: gw.name });
    // log(d);

    grid3.disconnect();
}

main();
</code></pre>
<h3 id="detailed-explanation-3"><a class="header" href="#detailed-explanation-3">Detailed explanation</a></h3>
<pre><code class="language-typescript">const gw = new GatewayFQDNModel();
gw.name = &quot;applyFQDN&quot;;
gw.node_id = 1;
gw.fqdn = &quot;test.hamada.grid.tf&quot;;
gw.tls_passthrough = false;
gw.backends = [&quot;my yggdrasil IP&quot;];
</code></pre>
<ul>
<li>we created a <code>GatewayFQDNModel</code> and gave it a name <code>applyFQDNN</code> to be deployed on gateway node <code>1</code> and specified the fully qualified domain <code>fqdn</code> to a domain we own <code>test.hamada.grid.tf</code></li>
<li>we created a record on our name provider for <code>test.hamada.grid.tf</code> to point to the IP of gateway node <code>1</code></li>
<li>we specified the backened would be an yggdrassil ip so once this is deployed when we go to <code>test.hamada.grid.tf</code> we go to the gateway server and from their our traffic goes to the backend.</li>
</ul>
<h4 id="deploying-1"><a class="header" href="#deploying-1">deploying</a></h4>
<pre><code class="language-typescript">// deploy
const res = await grid3.gateway.deploy_fqdn(gw);
log(res);
</code></pre>
<p>this deploys <code>GatewayName</code> on the grid</p>
<h4 id="get-deployment-object"><a class="header" href="#get-deployment-object">get deployment object</a></h4>
<pre><code class="language-typescript">const l = await grid3.gateway.getObj(gw.name);
log(l);
</code></pre>
<p>getting the deployment information can be done using <code>getObj</code></p>
<h4 id="deletion-1"><a class="header" href="#deletion-1">deletion</a></h4>
<pre><code class="language-typescript">const d = await grid3.gateway.delete_fqdn({ name: gw.name });
log(d);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>first, make sure you have your <a href="javascript/./grid3_javascript_loadclient.html">client</a> prepared</p>
<h2 id="deploying-a-kubernetes-cluster"><a class="header" href="#deploying-a-kubernetes-cluster">Deploying a kubernetes cluster</a></h2>
<h3 id="example-code-6"><a class="header" href="#example-code-6">Example code</a></h3>
<pre><code>import { FilterOptions, K8SModel, KubernetesNodeModel, NetworkModel } from &quot;../src&quot;;
import { config, getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

async function main() {
    const grid3 = await getClient();

    // create network Object
    const n = new NetworkModel();
    n.name = &quot;monNetwork&quot;;
    n.ip_range = &quot;10.238.0.0/16&quot;;
    n.addAccess = true;

    const masterQueryOptions: FilterOptions = {
        cru: 1,
        mru: 2, // GB
        sru: 10,
        farmId: 1,
    };

    const workerQueryOptions: FilterOptions = {
        cru: 2,
        mru: 4, // GB
        sru: 10,
        farmId: 1,
    };

    // create k8s node Object
    const master = new KubernetesNodeModel();
    master.name = &quot;master&quot;;
    master.node_id = +(await grid3.capacity.filterNodes(masterQueryOptions))[0].nodeId;
    master.cpu = 1;
    master.memory = 1024 * 2;
    master.rootfs_size = 0;
    master.disk_size = 8;
    master.public_ip = false;
    master.planetary = true;

    // create k8s node Object
    const worker = new KubernetesNodeModel();
    worker.name = &quot;worker&quot;;
    worker.node_id = +(await grid3.capacity.filterNodes(workerQueryOptions))[0].nodeId;
    worker.cpu = 2;
    worker.memory = 1024 * 4;
    worker.rootfs_size = 0;
    worker.disk_size = 8;
    worker.public_ip = false;
    worker.planetary = true;

    // create k8s Object
    const k = new K8SModel();
    k.name = &quot;testk8s&quot;;
    k.secret = &quot;secret&quot;;
    k.network = n;
    k.masters = [master];
    k.workers = [worker];
    k.metadata = &quot;{'testk8s': true}&quot;;
    k.description = &quot;test deploying k8s via ts grid3 client&quot;;
    k.ssh_key = config.ssh_key;

    // deploy
    const res = await grid3.k8s.deploy(k);
    log(res);

    // get the deployment
    const l = await grid3.k8s.getObj(k.name);
    log(l);

    // // delete
    // const d = await grid3.k8s.delete({ name: k.name });
    // log(d);

    await grid3.disconnect();
}

main();
</code></pre>
<h3 id="detailed-explanation-4"><a class="header" href="#detailed-explanation-4">Detailed explanation</a></h3>
<h4 id="building-network-1"><a class="header" href="#building-network-1">Building network</a></h4>
<pre><code class="language-typescript">// create network Object
const n = new NetworkModel();
n.name = &quot;monNetwork&quot;;
n.ip_range = &quot;10.238.0.0/16&quot;;

</code></pre>
<h4 id="building-nodes"><a class="header" href="#building-nodes">Building nodes</a></h4>
<pre><code class="language-typescript">// create k8s node Object
const master = new KubernetesNodeModel();
master.name = &quot;master&quot;;
master.node_id = 4;
master.cpu = 1;
master.memory = 1024 * 2;
master.rootfs_size = 1;
master.disk_size = 8;
master.public_ip = false;
master.planetary = true;

// create k8s node Object
const worker = new KubernetesNodeModel();
worker.name = &quot;worker&quot;;
worker.node_id = 4;
worker.cpu = 2;
worker.memory = 1024 * 4;
worker.rootfs_size = 1;
worker.disk_size = 8;
worker.public_ip = false;
worker.planetary = true;

</code></pre>
<h4 id="building-cluster"><a class="header" href="#building-cluster">Building cluster</a></h4>
<p>Here we specify the cluster project name, cluster secret, network model to be used, master and workers nodes and sshkey to access them</p>
<pre><code class="language-typescript">// create k8s Object
const k = new K8SModel();
k.name = &quot;testk8s&quot;;
k.secret = &quot;secret&quot;;
k.network = n;
k.masters = [master];
k.workers = [worker];
k.metadata = &quot;{'testk8s': true}&quot;;
k.description = &quot;test deploying k8s via ts grid3 client&quot;;
k.ssh_key =
    &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDWlguBuvfQikkRJZXkLPei7Scvo/OULUEvjWVR4tCZ5V85P2F4SsSghxpRGixCNc7pNtgvdwJegK06Tn7SkV2jYJ9kBJh8PA06CPSz1mnpco4cgktiWx/R8xBvLGlyO0BwUuD3/WFjrc6fzH9E7Bpkel/xTnacx14w1bZAC1R35hz7BaHu1WrXsfxEd0VH7gpMPoQ4+l+H38ULPTiC+JcOKJOqVafgcc0sU7otXbgCa1Frr4QE5bwiMYhOlsRfRv/hf08jYsVo+RUO3wD12ylLWR7a7sJDkBBwgir8SwAvtRlT6k9ew9cDMQ7H8iWNCOg2xqoTLpVag6RN9kGzA5LGL+qHEcBr6gd2taFEy9+mt+TWuKp6reUeJfTu9RD1UgB0HpcdgTHtoUTISW7Mz4KNkouci2DJFngDWrLRxRoz81ZwfI2hjFY0PYDzF471K7Nwwt3qKYF1Js9a6VO38tMxSU4mTO83bt+dUFozgpw2Y0KKJGHDwU66i2MvTPg3EGs= ayoub@ayoub-Inspiron-3576&quot;;

</code></pre>
<h4 id="deploying-2"><a class="header" href="#deploying-2">Deploying</a></h4>
<p>use <code>deploy</code> function to deploy the kubernetes project</p>
<pre><code class="language-typescript">const res = await grid3.k8s.deploy(k);
log(res);
</code></pre>
<h4 id="getting-deployment-information-1"><a class="header" href="#getting-deployment-information-1">Getting deployment information</a></h4>
<pre><code class="language-typescript">const l = await grid3.k8s.getObj(k.name);
log(l);
</code></pre>
<h4 id="deleting-deployment"><a class="header" href="#deleting-deployment">Deleting deployment</a></h4>
<pre><code class="language-typescript">const d = await grid3.k8s.delete({ name: k.name });
log(d);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>first, make sure you have your <a href="javascript/./grid3_javascript_loadclient.html">client</a> prepared</p>
<h2 id="deploying-zdb"><a class="header" href="#deploying-zdb">Deploying ZDB</a></h2>
<h3 id="example-code-7"><a class="header" href="#example-code-7">Example code</a></h3>
<pre><code>import { FilterOptions, ZDBModel, ZdbModes, ZDBSModel } from &quot;../src&quot;;
import { getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

async function main() {
    const grid3 = await getClient();

    const zdbQueryOptions: FilterOptions = {
        hru: 10,
        farmId: 1,
    };

    // create zdb object
    const zdb = new ZDBModel();
    zdb.name = &quot;hamada&quot;;
    zdb.node_id = +(await grid3.capacity.filterNodes(zdbQueryOptions))[0].nodeId;
    zdb.mode = ZdbModes.user;
    zdb.disk_size = 9;
    zdb.publicNamespace = false;
    zdb.password = &quot;testzdb&quot;;

    // create zdbs object
    const zdbs = new ZDBSModel();
    zdbs.name = &quot;tttzdbs&quot;;
    zdbs.zdbs = [zdb];
    zdbs.metadata = '{&quot;test&quot;: &quot;test&quot;}';

    // deploy zdb
    const res = await grid3.zdbs.deploy(zdbs);
    log(res);

    // get the deployment
    const l = await grid3.zdbs.getObj(zdbs.name);
    log(l);

    // // delete
    // const d = await grid3.zdbs.delete({ name: zdbs.name });
    // log(d);

    await grid3.disconnect();
}

main();
</code></pre>
<h3 id="detailed-explanation-5"><a class="header" href="#detailed-explanation-5">Detailed explanation</a></h3>
<h4 id="getting-the-client"><a class="header" href="#getting-the-client">Getting the client</a></h4>
<pre><code class="language-typescript">const grid3 = getClient();
</code></pre>
<h4 id="building-the-model"><a class="header" href="#building-the-model">Building the model</a></h4>
<pre><code class="language-typescript">// create zdb object
const zdb = new ZDBModel();
zdb.name = &quot;hamada&quot;;
zdb.node_id = 16;
zdb.mode = ZdbModes.user;
zdb.disk_size = 9;
zdb.publicNamespace = false;
zdb.password = &quot;testzdb&quot;;
</code></pre>
<p>Here we define a <code>ZDB model</code> and setting the relevant properties e.g </p>
<ul>
<li>name</li>
<li>node_id : where to deploy on</li>
<li>mode: <code>user</code> or <code>seq</code></li>
<li>disk_size: disk size in GB</li>
<li>publicNamespace: a public namespace can be read-only if a password is set</li>
<li>password: namespace password</li>
</ul>
<h4 id="preparing-zdbs-collection"><a class="header" href="#preparing-zdbs-collection">preparing ZDBs collection</a></h4>
<pre><code class="language-typescript">// create zdbs object
const zdbs = new ZDBSModel();
zdbs.name = &quot;tttzdbs&quot;;
zdbs.zdbs = [zdb];
zdbs.metadata = '{&quot;test&quot;: &quot;test&quot;}';

</code></pre>
<p>you can attach multiple ZDBs into the collection nand send it for deployment</p>
<h4 id="deployment-1"><a class="header" href="#deployment-1">Deployment</a></h4>
<pre><code class="language-typescript">const res = await grid3.zdbs.deploy(zdbs);
log(res);
</code></pre>
<h4 id="getting-deployment-information-2"><a class="header" href="#getting-deployment-information-2">Getting Deployment information</a></h4>
<p><code>getObj</code> gives detailed information about the workload.</p>
<pre><code class="language-typescript">// get the deployment
const l = await grid3.zdbs.getObj(zdbs.name);
log(l);
</code></pre>
<h4 id="deleting-a-deployment-1"><a class="header" href="#deleting-a-deployment-1">Deleting a deployment</a></h4>
<p><code>.delete</code> method helps cancelling the relevant contracts related to that ZDBs deployment</p>
<pre><code class="language-typescript">// delete
const d = await grid3.zdbs.delete({ name: zdbs.name });
log(d);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>first, make sure you have your <a href="javascript/./grid3_javascript_loadclient.html">client</a> prepared</p>
<h2 id="deploying-a-vm-with-qsfs"><a class="header" href="#deploying-a-vm-with-qsfs">Deploying a VM with QSFS</a></h2>
<h3 id="example-code-8"><a class="header" href="#example-code-8">Example code</a></h3>
<pre><code>import { FilterOptions, MachinesModel, QSFSZDBSModel } from &quot;../src&quot;;
import { config, getClient } from &quot;./client_loader&quot;;
import { log } from &quot;./utils&quot;;

async function main() {
    const grid3 = await getClient();

    const qsfs_name = &quot;wed2710q1&quot;;
    const machines_name = &quot;wed2710t1&quot;;

    const vmQueryOptions: FilterOptions = {
        cru: 2,
        mru: 2, // GB
        sru: 10,
        farmId: 1,
    };

    const qsfsQueryOptions: FilterOptions = {
        hru: 40,
        farmId: 1,
    };

    const qsfsNodes = [];

    const allNodes = await grid3.capacity.filterNodes(qsfsQueryOptions);
    if (allNodes.length &gt;= 2) {
        qsfsNodes.push(+allNodes[0].nodeId, +allNodes[1].nodeId);
    } else {
        throw Error(&quot;Couldn't find nodes for qsfs&quot;);
    }

    const vmNode = +(await grid3.capacity.filterNodes(vmQueryOptions))[0].nodeId;

    const qsfs: QSFSZDBSModel = {
        name: qsfs_name,
        count: 8,
        node_ids: qsfsNodes,
        password: &quot;mypassword&quot;,
        disk_size: 10,
        description: &quot;my qsfs test&quot;,
        metadata: &quot;&quot;,
    };

    const vms: MachinesModel = {
        name: machines_name,
        network: {
            name: &quot;wed2710n1&quot;,
            ip_range: &quot;10.201.0.0/16&quot;,
        },
        machines: [
            {
                name: &quot;wed2710v1&quot;,
                node_id: vmNode,
                disks: [
                    {
                        name: &quot;wed2710d1&quot;,
                        size: 10,
                        mountpoint: &quot;/mydisk&quot;,
                    },
                ],
                qsfs_disks: [
                    {
                        qsfs_zdbs_name: qsfs_name,
                        name: &quot;wed2710d2&quot;,
                        minimal_shards: 2,
                        expected_shards: 4,
                        encryption_key: &quot;hamada&quot;,
                        prefix: &quot;hamada&quot;,
                        cache: 1,
                        mountpoint: &quot;/myqsfsdisk&quot;,
                    },
                ],
                public_ip: false,
                public_ip6: false,
                planetary: true,
                cpu: 1,
                memory: 1024 * 2,
                rootfs_size: 0,
                flist: &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;,
                entrypoint: &quot;/sbin/zinit init&quot;,
                env: {
                    SSH_KEY: config.ssh_key,
                },
            },
        ],
        metadata: &quot;{'testVMs': true}&quot;,
        description: &quot;test deploying VMs via ts grid3 client&quot;,
    };

    async function cancel(grid3) {
        // delete
        const d = await grid3.machines.delete({ name: machines_name });
        log(d);
        const r = await grid3.qsfs_zdbs.delete({ name: qsfs_name });
        log(r);
    }
    //deploy qsfs
    const res = await grid3.qsfs_zdbs.deploy(qsfs);
    log(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;QSFS backend has been created&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&quot;);
    log(res);

    const vm_res = await grid3.machines.deploy(vms);
    log(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;vm has been created&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&quot;);
    log(vm_res);

    // get the deployment
    const l = await grid3.machines.getObj(vms.name);
    log(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;Deployment result&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&quot;);
    log(l);

    // await cancel(grid3);

    await grid3.disconnect();
}

main();
</code></pre>
<h3 id="detailed-explanation-6"><a class="header" href="#detailed-explanation-6">Detailed explanation</a></h3>
<h4 id="getting-the-client-1"><a class="header" href="#getting-the-client-1">Getting the client</a></h4>
<pre><code class="language-typescript">const grid3 = getClient();
</code></pre>
<h4 id="preparing-qsfs"><a class="header" href="#preparing-qsfs">preparing QSFS</a></h4>
<pre><code class="language-javascript">const qsfs_name = &quot;wed2710q1&quot;;
const machines_name = &quot;wed2710t1&quot;;
</code></pre>
<p>We prepare here some names to use across the client for the QSFS and the machines project</p>
<pre><code class="language-typescript">// deploy qsfs backend zdbs first
const qsfs = {
    name: qsfs_name,
    count: 8,
    node_ids: [16, 17],
    password: &quot;mypassword&quot;,
    disk_size: 10,
    description: &quot;my qsfs test&quot;,
    metadata: &quot;&quot;,
}
const res = await grid3.qsfs_zdbs.deploy(qsfs);
log(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;QSFS backend has been created&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&quot;);
log(res);
</code></pre>
<p>Here we deploy <code>8</code> ZDBs on nodes <code>2,3</code> with password <code>mypassword</code>, all of them having disk size of <code>10GB</code> </p>
<h4 id="deploying-a-vm-with-qsfs-1"><a class="header" href="#deploying-a-vm-with-qsfs-1">Deploying a VM with QSFS</a></h4>
<pre><code class="language-typescript"> // deploy vms
const vms = {
    name: machines_name,
    network: {
        name: &quot;wed2710n1&quot;,
        ip_range: &quot;10.201.0.0/16&quot;,
    },
    machines: [
        {
            name: &quot;wed2710v1&quot;,
            node_id: 17,
            disks: [
                {
                    name: &quot;wed2710d1&quot;,
                    size: 10,
                    mountpoint: &quot;/mydisk&quot;,
                },
            ],
            qsfs_disks: [
                {
                    qsfs_zdbs_name: qsfs_name,
                    name: &quot;wed2710d2&quot;,
                    minimal_shards: 2,
                    expected_shards: 4,
                    encryption_key: &quot;hamada&quot;,
                    prefix: &quot;hamada&quot;,
                    cache: 1,
                    mountpoint: &quot;/myqsfsdisk&quot;,
                },
            ],
            public_ip: false,
            planetary: true,
            cpu: 1,
            memory: 1024 * 2,
            rootfs_size: 1,
            flist: &quot;https://hub.grid.tf/tf-official-apps/base:latest.flist&quot;,
            entrypoint: &quot;/sbin/zinit init&quot;,
            env: {
                SSH_KEY:
                    &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCt1LYcIga3sgbip5ejiC6R7CCa34omOwUilR66ZEvUh/u4RpbZ9VjRryVHVDyYcd/qbUzpWMzqzFlfFmtVhPQ0yoGhxiv/owFwStqddKO2iNI7T3U2ytYLJqtPm0JFLB5n07XLyFRplq0W2/TjNrYl51DedDQqBJDq34lz6vTkECNmMKg9Ld0HpxnpHBLH0PsXMY+JMZ8keH9hLBK61Mx9cnNxcLV9N6oA6xRCtwqOdLAH08MMaItYcJ0UF/PDs1PusJvWkvsH5/olgayeAReI6JFGv/x4Eqq5vRJRQjkj9m+Q275gzf9Y/7M/VX7KOH7P9HmDbxwRtOq1F0bRutKF&quot;,
            },
        },
    ],
    metadata: &quot;{'testVMs': true}&quot;,
    description: &quot;test deploying VMs via ts grid3 client&quot;,
}
const vm_res = await grid3.machines.deploy(vms);
log(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;vm has been created&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&quot;);
log(vm_res);
</code></pre>
<p>So this deployment is almost similiar to what we have in the <a href="javascript/grid3_javascript_vm">vm deployment section</a>. We only have a new section <code>qsfs_disks</code></p>
<pre><code class="language-typescript">    qsfs_disks: [{
        qsfs_zdbs_name: qsfs_name,
        name: &quot;wed2710d2&quot;,
        minimal_shards: 2,
        expected_shards: 4,
        encryption_key: &quot;hamada&quot;,
        prefix: &quot;hamada&quot;,
        cache: 1,
        mountpoint: &quot;/myqsfsdisk&quot;
    }],
</code></pre>
<p><code>qsfs_disks</code> is a list, representing all of the QSFS disks used within that VM.</p>
<ul>
<li><code>qsfs_zdbs_name</code>: that's the backend ZDBs we defined in the beginning</li>
<li><code>expected_shards</code>: how many ZDBs that QSFS should be working with</li>
<li><code>minimal_shards</code>: the minimal possible amount of ZDBs to recover the data with when losing disks e.g due to failure</li>
<li><code>mountpoint</code>: where it will be mounted on the VM <code>/myqsfsdisk</code></li>
</ul>
<h4 id="getting-deployment-information-3"><a class="header" href="#getting-deployment-information-3">Getting deployment information</a></h4>
<pre><code class="language-typescript">const l = await grid3.machines.getObj(vms.name);
log(l);
</code></pre>
<h4 id="deleting-a-deployment-2"><a class="header" href="#deleting-a-deployment-2">Deleting a deployment</a></h4>
<pre><code class="language-typescript">// delete
const d = await grid3.machines.delete({ name: machines_name });
log(d);
const r = await grid3.qsfs_zdbs.delete({ name: qsfs_name });
log(r);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>first, make sure you have your <a href="javascript/./grid3_javascript_loadclient.html">client</a> prepared</p>
<h2 id="using-tfchain-kvstore"><a class="header" href="#using-tfchain-kvstore">Using tfchain KVStore</a></h2>
<p>As part of the tfchain, we support a keyvalue store module that can be used for any value within <code>2KB</code> range. practically it's used to save the user configurations state, so it can be built up again on any machine, given they used the same mnemonics and same secret. </p>
<h3 id="example-code-9"><a class="header" href="#example-code-9">Example code</a></h3>
<pre><code>import &quot;reflect-metadata&quot;;

import { getClient } from &quot;./client_loader&quot;;

const exampleObj = {
    key1: &quot;value1&quot;,
    key2: 2,
};

/*
KVStore example usage:
*/
async function main() {
    //For creating grid3 client with KVStore, you need to specify the KVStore storage type in the pram:

    const gridClient = await getClient();

    //then every module will use the KVStore to save its configuration and restore it.

    // also you can use it like this:
    const db = gridClient.kvstore;

    // set key
    const key = &quot;hamada&quot;;
    await db.set({ key, value: JSON.stringify(exampleObj) });

    // list all the keys
    const keys = await db.list();
    console.log(keys);

    // get the key
    const data = await db.get({ key });
    console.log(JSON.parse(data.toString()));

    // // remove the key
    // await db.remove({ key });

    // disconnect
    await gridClient.disconnect();
}

main();
</code></pre>
<h4 id="setting-values"><a class="header" href="#setting-values">setting values</a></h4>
<p><code>db.set</code> is used to set key to any value <code>serialized as string</code></p>
<pre><code class="language-typescript">await db.set({ key, value: JSON.stringify(exampleObj) });
</code></pre>
<h4 id="getting-key"><a class="header" href="#getting-key">getting key</a></h4>
<p><code>db.get</code> is used to get a specific key</p>
<pre><code class="language-typescript">const data = await db.get({ key });
log(JSON.parse(data));
</code></pre>
<h4 id="listing-keys"><a class="header" href="#listing-keys">listing keys</a></h4>
<p><code>db.list</code> is used to list all the keys.</p>
<pre><code class="language-typescript">const keys = await db.list();
log(keys);
</code></pre>
<h4 id="deleting-key"><a class="header" href="#deleting-key">deleting key</a></h4>
<p><code>db.remove</code> is used to delete a specific key.</p>
<pre><code class="language-typescript">await db.remove({ key });
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="weblets"><a class="header" href="#weblets">Weblets</a></h1>
<p>Weblets are a UI tool helps to deploy solutions on TF Grid v3.</p>
<p>A weblet is a compiled javascript web component which can be embedded in HTML page of a web app.</p>
<p>The backend for the weblets is introduced with <a href="weblets/../javascript/grid3_javascript_readme.html">grid client</a> which communicate to TF Chain and TF Grid over RMB.</p>
<p><strong>Advantages</strong> :</p>
<ul>
<li>It is a non-code easy way to deploy a whole solution on the Grid.</li>
<li>It is 100% decentralized, there is no server involved</li>
</ul>
<h2 id="weblets-list"><a class="header" href="#weblets-list">Weblets list</a></h2>
<p>There are several weblets you can use like: </p>
<ul>
<li>
<p>Basic Environments:</p>
<ul>
<li><a href="weblets/./weblets_vm.html">Virtual Machine</a></li>
<li><a href="weblets/./weblets_k8s.html">Kubernetes</a></li>
</ul>
</li>
<li>
<p>Ready Community Solutions:</p>
<ul>
<li><a href="weblets/./weblets_caprover.html">Caprover</a></li>
<li><a href="weblets/./weblets_funkwhale.html">Funkwhale</a></li>
<li><a href="weblets/./weblets_peertube.html">Peertube</a></li>
<li><a href="weblets/./weblets_taiga.html">Taiga</a></li>
<li><a href="weblets/./weblets_owncloud.html">Owncloud</a></li>
<li><a href="weblets/./weblets_discourse.html">Discourse</a></li>
<li><a href="weblets/./weblets_mattermost.html">Mattermost</a></li>
<li><a href="weblets/./weblets_presearch.html">Presearch</a></li>
<li><a href="weblets/./weblets_casper.html">CasperLabs</a></li>
<li><a href="weblets/./weblets_nodepilot.html">Node Pilot</a></li>
</ul>
</li>
<li>
<p>Some utils:</p>
<ul>
<li><a href="weblets/./weblets_profile_manager.html">Profile Manager</a></li>
<li><a href="weblets/./weblets_deployments_list.html">Deployment List</a></li>
</ul>
</li>
</ul>
<h2 id="playground"><a class="header" href="#playground">Playground</a></h2>
<p>Playground is a Vue app that has the weblets embedded. so you can try it out on different TF Chain networks.</p>
<ul>
<li><a href="https://play.dev.grid.tf">https://play.dev.grid.tf</a> for Devnet.</li>
<li><a href="https://play.qa.grid.tf">https://play.qa.grid.tf</a> for QAnet.</li>
<li><a href="https://play.test.grid.tf">https://play.test.grid.tf</a> for Testnet.</li>
<li><a href="https://play.grid.tf">https://play.grid.tf</a> for Mainnet.</li>
</ul>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<ul>
<li>Regarding browser support, we're only supporting Google Chrome browser at the moment with more browsers to be supported soon. </li>
<li>Deploys one thing at a time.</li>
<li>Might take sometime to deploy a solution like Peertube, so you should wait a little bit until it's fully running.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="profile-manager"><a class="header" href="#profile-manager">Profile Manager</a></h1>
<p>Currently, we're supporting four different networks.</p>
<ul>
<li>One for development purposes (Devnet) where you can find it at https://play.dev.grid.tf</li>
<li>One for internal testing and verifications (QAnet) where you can find it at https://play.qa.grid.tf</li>
<li>One for testing (Testnet) where you can find it at https://play.test.grid.tf</li>
<li>One for our mainnet and you can find it at https://play.grid.tf</li>
</ul>
<p><img src="weblets/./img/profile_manager1.png" alt=" " /></p>
<p>Start by creating a profile from the upper right button. This creates a profile, saved and encrypted locally in your browser.</p>
<h3 id="secure"><a class="header" href="#secure">Secure</a></h3>
<p><img src="weblets/./img/pro_manager5.png" alt=" " /></p>
<p>The <strong>Profile Manager Password</strong> is how you store your profile info in browser local storage.
Create a new profile by visiting the <strong>Create Profile Manager</strong> tab and enter your new password. After you're done, click on <strong>Create New Profile Manager</strong>. </p>
<p>You'll need that password to be able to load your profiles afterwards from the <strong>Activate Profile Manager</strong> tab.</p>
<p><img src="weblets/./img/pro_manager6.png" alt=" " /></p>
<h3 id="process"><a class="header" href="#process">Process</a></h3>
<p>Start entering the following information required to create your new profile.</p>
<p><img src="weblets/./img/dev_profile2.png" alt=" " /></p>
<ul>
<li><code>Profile Name</code>: Any chosen name, makes it easy for you to remember between sessions.</li>
<li><code>Mnemonics</code> are the secret words of your Polkadot account, <a href="weblets/tfchain_portal_polkadot_create_account">Generate yours here!</a>. </li>
<li>Your <code>Public SSH Key</code> is used to login into VM's, Kubernetes, ... </li>
</ul>
<p>After you finish typing your credentials, click on <strong>Activate</strong>. Once your profile gets activated, you should find your <strong>Twin ID</strong> and <strong>Address</strong> generated under your <em><strong>Mnemonics</strong></em> for verification. Also, your <strong>Account Balance</strong> will be available at the top right corner under your profile name. </p>
<p><img src="weblets/./img/dev_profile3.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unlock-profile-manager"><a class="header" href="#unlock-profile-manager">Unlock Profile Manager</a></h1>
<p>To unlock a previously created profile manager, do the following steps:</p>
<ul>
<li>
<p>Choose one of the networks:</p>
<ul>
<li>https://play.dev.grid.tf for Devnet.</li>
<li>https://play.qa.grid.tf for QAnet.</li>
<li>https://play.test.grid.tf for Testnet.</li>
<li>https://play.grid.tf for Mainnet.</li>
</ul>
</li>
<li>
<p>Go to the <strong>Activate Profile Manager</strong> tab.</p>
</li>
<li>
<p>Fill in the <strong>Profile Manager Password</strong> used when you created the profile</p>
</li>
<li>
<p>Click <code>Load Profiles</code></p>
</li>
</ul>
<p><img src="weblets/./img/pro_manager6.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deployments-list"><a class="header" href="#deployments-list">Deployments List</a></h1>
<p>Deployments List is a generic weblet for listing deployments on Threefold Grid</p>
<ul>
<li>make sure you have a <a href="weblets/./weblets_profile_manager.html">profile</a> activated</li>
<li>click on Deployments</li>
</ul>
<p><img src="weblets/./img/deplist1.png" alt=" " /></p>
<p>There you can see tabs representing the known solutions supported e.g <code>VM</code>, <code>Kubernetes</code>, <code>CapRover</code>.. etc</p>
<p>Click on a tab activates the listing of solutions of that tab. </p>
<blockquote>
<p>Note the information is saved on the blockchain Key value module.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtual-machine-1"><a class="header" href="#virtual-machine-1">Virtual Machine</a></h1>
<p>Deploy a new virtual machine on the Threefold Grid</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a> </li>
<li>Click on the <strong>Virtual Machine</strong> tab</li>
</ul>
<p><strong>Process</strong> : </p>
<p><img src="weblets/../weblets/img/new_vm1.png" alt=" " /></p>
<ul>
<li>Fill in the instance name: it's used to reference the VM in the future.</li>
<li>Choose the image from the drop down (e.g Alpine, Ubuntu) or you can click on <code>Other</code> and manually specify the flist URL and the entrypoint.</li>
<li><code>Public IPv4</code> flag gives the virtual machine a Public IPv4</li>
<li><code>Public IPv6</code> flag gives the virtual machine a Public IPv6</li>
<li><code>Planetary Network</code> to connect the Virtual Machine to Planetary network</li>
<li>Choose the node to deploy on which can be
<ul>
<li>Manual: where you specify the node id yourself</li>
<li>Automatic: Suggests nodes list based on search criteria e.g <code>country</code>, <code>farm</code>, capacity..</li>
</ul>
</li>
</ul>
<p><img src="weblets/../weblets/img/new_vm2.png" alt=" " />
Clicking on enviornment allows you to define environment variables to pass to the virtual machine. </p>
<blockquote>
<p>Note the Public SSH key in the profile is automatically used as variable <code>SSH_KEY</code> passed to all Virtual Machines </p>
</blockquote>
<p><img src="weblets/../weblets/img/new_vm3.png" alt=" " />
You can attach one or more disks to the Virtual Machine by clicking on the Disks tab and the plus <code>+</code> sign and specify the following parameters</p>
<ul>
<li>Disk name </li>
<li>Disk size</li>
<li>Mount point</li>
</ul>
<p>in the bottom of the page you can see a list of all of the virual machines you deployed. you can click on <code>Show details</code> for more details</p>
<p><img src="weblets/../weblets/img/weblet_vm5.png" alt=" " />
You can also go to JSON tab for full details
<img src="weblets/../weblets/img/weblet_vm6.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kubernetes"><a class="header" href="#kubernetes">Kubernetes</a></h1>
<p>Kubernetes is the standard container orchestration tool.</p>
<p>On the TF grid, Kubernetes clusters can be deployed out of the box. We have implemented <a href="https://k3s.io/">K3S</a>, a full-blown Kubernetes offering that uses only half of the memory footprint. It is packaged as a single binary and made more lightweight to run workloads in resource-constrained locations (fits e.g. IoT, edge, ARM workloads).</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a> </li>
<li>Click on the <strong>Kubernetes</strong> tab</li>
</ul>
<h3 id="configs-tab"><a class="header" href="#configs-tab"><strong>Configs</strong> tab</a></h3>
<p><img src="weblets/./img/new_k8s1.png" alt=" " /></p>
<ul>
<li><code>Name</code>: Your Kubernetes Cluster name.</li>
<li><code>Cluster Token</code>: It's used for authentication between your worker nodes and master node. You could use the auto-generated one or type your own.</li>
<li><code>Network Name</code>: It's used for Wireguard's private network.</li>
<li><code>Network IP Range</code>: It's a private subnet for Wireguard's network. You could use the auto-generated one or replace it with a <strong>private</strong> subnet.</li>
</ul>
<h3 id="master-and-worker-tabs"><a class="header" href="#master-and-worker-tabs"><strong>Master</strong> and <strong>Worker</strong> tabs</a></h3>
<p><img src="weblets/./img/new_k8s2.png" alt=" " />
<img src="weblets/./img/new_k8s3.png" alt=" " /></p>
<blockquote>
<p>Currently, we only support &quot;single-master-multi-worker&quot; k8s clusters. So you could always add more than one worker node by clicking on the <strong>+</strong> in the <em><strong>Worker</strong></em> tab.</p>
</blockquote>
<h3 id="kubeconfig"><a class="header" href="#kubeconfig">Kubeconfig</a></h3>
<p>Once the cluster is ready, you can SSH into the cluster using <code>ssh root@IP</code></p>
<blockquote>
<p>IP can be the public IP or the planetary network IP</p>
</blockquote>
<p>Onced connected via SSH, you can execute commands on the cluster like <code>kubectl get nodes</code>, and to get the kubeconfig, you can find it in <code>/root/.kube/config</code> </p>
<blockquote>
<p>if it doesn't exist in <code>/root/.kube/config</code> it can be in <code>/etc/rancher/k3s/k3s.yaml</code></p>
</blockquote>
<p>example:</p>
<pre><code>root@WR768dbf76:~# cat /root/.kube/config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkakNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUyTkRBeU5qWTBNVE13SGhjTk1qRXhNakl6TVRNek16TXpXaGNOTXpFeE1qSXhNVE16TXpNegpXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUyTkRBeU5qWTBNVE13V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFUcGNtZE1KaWg1eGFTa1JlelNKVU5mUkQ5NWV6cE12amhVeUc2bWU4bTkKY0lQWENoNUZ2ZU81Znk1d1VTSTlYOFlGV2JkOGtRcG9vaVdVbStwYjFvU3hvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVUtkL3VUU3FtWk12bHhtcWNYU3lxCmVhWERIbXd3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnSUQ0cGNQWDl2R0F6SC9lTkhCNndVdmNZRi9HbXFuQVIKR2dqT1RSdWVia1lDSUdRUmUwTGJzQXdwMWNicHlYRWljV3V0aG1RQ1dwRXY1NThWZ3BoMFpETFAKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://127.0.0.1:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: default
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJrVENDQVRlZ0F3SUJBZ0lJWnptV1A4ellKaGd3Q2dZSUtvWkl6ajBFQXdJd0l6RWhNQjhHQTFVRUF3d1kKYXpOekxXTnNhV1Z1ZEMxallVQXhOalF3TWpZMk5ERXpNQjRYRFRJeE1USXlNekV6TXpNek0xb1hEVEl5TVRJeQpNekV6TXpNek0xb3dNREVYTUJVR0ExVUVDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGVEFUQmdOVkJBTVRESE41CmMzUmxiVHBoWkcxcGJqQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJINTZaZGM5aTJ0azAyNGQKcXBDQ2NRMndMMjc1QWtPZUFxalIzQjFTTGFQeG1oOG9IcXd4SzY2RTc1ZWQya2VySFIySnBZbWwwNE5sa0grLwpSd2kvMDNDalNEQkdNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0RBakFmCkJnTlZIU01FR0RBV2dCVGhPakJSaExjeE53UDkzd0xtUzBYRUFUNjlSekFLQmdncWhrak9QUVFEQWdOSUFEQkYKQWlBcjdDcDR2dks4Y2s0Q0lROEM5em5zVkFUZVhDaHZsUmdvanZuVXU4REZld0loQUlwRVYyMWJZVXBpUEkzVQowa3QvQmJqRUtjV1poVXNHQ0g0YzVNWTFFS0JhCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0KLS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkekNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdFkyeHAKWlc1MExXTmhRREUyTkRBeU5qWTBNVE13SGhjTk1qRXhNakl6TVRNek16TXpXaGNOTXpFeE1qSXhNVE16TXpNegpXakFqTVNFd0h3WURWUVFEREJock0zTXRZMnhwWlc1MExXTmhRREUyTkRBeU5qWTBNVE13V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFUY3NlakN3TDQ5VkZvQnJhWVRyR3ByR2lMajNKeEw4ZVcwYnpTVDBWRGUKeFlrb3hDbDlnR0N6R2p1Q2Q0ZmZmRXV0QWdFMjU5MDFBWGJCU2VnOHdlSkJvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVTRUb3dVWVMzTVRjRC9kOEM1a3RGCnhBRSt2VWN3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUloQU5CYWRhcFFZbnlYOEJDUllNODZtYWtMNkFDM0hSenMKL2l3Ukp6TnV6YytaQWlCZm14YytDTVZHQnBrblAzR2dWSWlFMFVQWkUrOFRnRUdkTTgrdCt4V2Ywdz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSURXQURoZUl0RVdHWlFCc0tCSUpZTTZPeDB5TmRHQ1JjTDBTMUtvYjRTZ25vQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZm5wbDF6MkxhMlRUYmgycWtJSnhEYkF2YnZrQ1E1NENxTkhjSFZJdG8vR2FIeWdlckRFcgpyb1R2bDUzYVI2c2RIWW1saWFYVGcyV1FmNzlIQ0wvVGNBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=
root@WR768dbf76:~# 

</code></pre>
<p>If you want to use kubectl through another machine, you need to change the line <code>server: https://127.0.0.1:6443</code> to be <code>server: https://PLANETARYIP_OR_PUBLICIP/6443</code>
replace PLANETARYIP_OR_PUBLICIP with the IP you want to reach th cluster through.</p>
<h3 id="manage-workers"><a class="header" href="#manage-workers">Manage Workers</a></h3>
<p>Add or Remove workers in any <strong>Kubernetes cluster</strong>.</p>
<pre><code class="language-html">&lt;tf-deployedlist tab=&quot;k8s&quot;&gt;&lt;/tf-deployedlist&gt;
</code></pre>
<ul>
<li>
<p>Kubernetes DeployedList Weblet
<img src="weblets/./img/k8s_dl_1.png" alt=" " /></p>
</li>
<li>
<p>Manager kubernetes workers
<img src="weblets/./img/k8s_dl_2.png" alt=" " /></p>
</li>
<li>
<p>Add a new worker
<img src="weblets/./img/new_k8s4.png" alt=" " /></p>
</li>
<li>
<p>Successfully added new worker
<img src="weblets/./img/k8s_dl_4.png" alt=" " /></p>
</li>
<li>
<p>Delete a worker
<img src="weblets/./img/new_k8s4.png" alt=" " /></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="caprover"><a class="header" href="#caprover">CapRover</a></h1>
<p><img src="weblets/./img/caprover_1.png" alt=" " /></p>
<p>About CapRover</p>
<p>CapRover is an extremely easy to use app/database deployment &amp; web server manager for your NodeJS, Python, PHP, ASP.NET, Ruby, MySQL, MongoDB, Postgres, WordPress (and etc...) applications!</p>
<p>It's blazingly fast and very robust as it uses Docker, nginx, LetsEncrypt and NetData under the hood behind its simple-to-use interface.</p>
<ul>
<li>CLI for automation and scripting</li>
<li>Web GUI for ease of access and convenience</li>
<li>No lock-in! Remove CapRover and your apps keep working!</li>
<li>Docker Swarm under the hood for containerization and clustering</li>
<li>Nginx (fully customizable template) under the hood for load-balancing</li>
<li>Let's Encrypt under the hood for free SSL (HTTPS)</li>
</ul>
<p>Caprover is a very cool management app for containers based on Docker Swarm.</p>
<p>It has following benefits : </p>
<ul>
<li>easy to deploy apps (in seconds)</li>
<li>easy to create new apps</li>
<li>super good monitoring</li>
<li>can be extended over the TFGrid</li>
</ul>
<h3 id="requirements-2"><a class="header" href="#requirements-2">Requirements</a></h3>
<ul>
<li>you need an account on TF-Chain, and there needs to be TFT on the account (see getting started)</li>
<li><a href="weblets/./weblets_profile_manager.html">Make sure your profile is activated</a></li>
<li><a href="weblets/./profile_manager_unlock.html">Unlock your profile on profile manager, if already filled in before</a></li>
</ul>
<h3 id="usage"><a class="header" href="#usage">Usage</a></h3>
<p><img src="weblets/./img/new_cap1.png" alt=" " /></p>
<ul>
<li>
<p>Select a capacity package:</p>
<ul>
<li><strong>Minimum</strong>: {cpu: 1, memory: 1024, diskSize: 50 }</li>
<li><strong>Standard</strong>: {cpu: 2, memory: 1024 * 2, diskSize: 100 }</li>
<li><strong>Recommended</strong>: {cpu: 4, memory: 1024 * 4, diskSize: 250 }</li>
<li>Or choose a <strong>Custom</strong> plan</li>
</ul>
</li>
<li>
<p>Choose a node to deploy Caprover on.</p>
</li>
<li>
<p>Either use the <strong>Capacity Filter</strong>. Which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick. </p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
<li>
<p>Be very careful about the domain name: it needs to be a wildcard domain name you can configure in your chosen domain name system.</p>
</li>
</ul>
<p>Deployment will take couple of minutes.</p>
<h3 id="the-domain-name"><a class="header" href="#the-domain-name">The Domain Name</a></h3>
<ul>
<li>e.g. I picked <code>apps.openly.life</code> which is a domain name that will point to the ip address of the CapRover instance (which we only know after deployment).</li>
</ul>
<p><img src="weblets/./img/domain_name_caprover_config.png" alt=" " /></p>
<blockquote>
<p>Note how the *.apps.openly.life points to the public IPv4 address that has been returned from the deployment.</p>
</blockquote>
<h3 id="how-to-know-what-the-ip-address-is"><a class="header" href="#how-to-know-what-the-ip-address-is">How to know what the IP address is?</a></h3>
<p>Go back to your CapRover weblet and go to the deployment list. Click on <code>Show Details</code>.</p>
<p><img src="weblets/./img/caprover_detail_weblet.png" alt=" " /></p>
<ul>
<li>The public IPv4 address is visible in here</li>
<li>Now you can configure the domain name (see above, don't forget to point the wildcard domain to the public IP address)</li>
</ul>
<p>Click on details if you want to see more details</p>
<pre><code class="language-json">
{
    &quot;version&quot;: 0,
    &quot;name&quot;: &quot;caprover_leader_cr_156e44f0&quot;,
    &quot;created&quot;: 1637843368,
    &quot;status&quot;: &quot;ok&quot;,
    &quot;message&quot;: &quot;&quot;,
    &quot;flist&quot;: &quot;https://hub.grid.tf/samehabouelsaad.3bot/tf-caprover-main-a4f186da8d.flist&quot;,
    &quot;publicIP&quot;: {
        &quot;ip&quot;: &quot;185.206.122.136/24&quot;,
        &quot;gateway&quot;: &quot;185.206.122.1&quot;
    },
    &quot;planetary&quot;: false,
    &quot;yggIP&quot;: &quot;&quot;,
    &quot;interfaces&quot;: [
        {
            &quot;network&quot;: &quot;caprover_network_cr_156e44f0&quot;,
            &quot;ip&quot;: &quot;10.200.4.2&quot;
        }
    ],
    &quot;capacity&quot;: {
        &quot;cpu&quot;: 4,
        &quot;memory&quot;: 8192
    },
    &quot;mounts&quot;: [
        {
            &quot;name&quot;: &quot;data0&quot;,
            &quot;mountPoint&quot;: &quot;/var/lib/docker&quot;,
            &quot;size&quot;: 107374182400,
            &quot;state&quot;: &quot;ok&quot;,
            &quot;message&quot;: &quot;&quot;
        }
    ],
    &quot;env&quot;: {
        &quot;SWM_NODE_MODE&quot;: &quot;leader&quot;,
        &quot;CAPROVER_ROOT_DOMAIN&quot;: &quot;apps.openly.life&quot;,
        &quot;PUBLIC_KEY&quot;: &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/9RNGKRjHvViunSOXhBF7EumrWvmqAAVJSrfGdLaVasgaYK6tkTRDzpZNplh3Tk1aowneXnZffygzIIZ82FWQYBo04IBWwFDOsCawjVbuAfcd9ZslYEYB3QnxV6ogQ4rvXnJ7IHgm3E3SZvt2l45WIyFn6ZKuFifK1aXhZkxHIPf31q68R2idJ764EsfqXfaf3q8H3u4G0NjfWmdPm9nwf/RJDZO+KYFLQ9wXeqRn6u/mRx+u7UD+Uo0xgjRQk1m8V+KuLAmqAosFdlAq0pBO8lEBpSebYdvRWxpM0QSdNrYQcMLVRX7IehizyTt+5sYYbp6f11WWcxLx0QDsUZ/J&quot;
    },
    &quot;entrypoint&quot;: &quot;/sbin/zinit init&quot;,
    &quot;metadata&quot;: &quot;&quot;,
    &quot;description&quot;: &quot;caprover leader machine/node&quot;
}
</code></pre>
<h3 id="how-to-access-the-admin-interface-"><a class="header" href="#how-to-access-the-admin-interface-">How to access the admin interface ?</a></h3>
<ul>
<li>make sure your public IP address (in my case <code>185.206.122.136</code>) is filled in in the domain name record.</li>
</ul>
<blockquote>
<p>admin url: https://captain.apps.openly.life/   (note prefix captain, and the usage of our wildcard domain).
<br> 
The password is generated and visible behind the <code>Show Details</code> button of your CapRover deployment. </p>
</blockquote>
<p><img src="weblets/./img/caprover_login.png" alt=" " /></p>
<p>You should now see</p>
<p><img src="weblets/./img/captain_login+weblet_caprover_.png" alt=" " /></p>
<h3 id="how-to-work-with-caprover"><a class="header" href="#how-to-work-with-caprover">How to work with CapRover</a></h3>
<blockquote>
<p><a href="weblets/weblets/weblets_caprover_admin">see our caprover admin small tutorial</a></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="funkwhale"><a class="header" href="#funkwhale">Funkwhale</a></h1>
<p><a href="https://funkwhale.audio/">Funkwhale</a> is social platform to enjoy and share music.
Funkwhale is a community-driven project that lets you listen and share music and audio within a decentralized, open network.</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a></li>
<li>Click on the <strong>Funkwhale</strong> tab</li>
</ul>
<p><strong>Process</strong> :</p>
<p><img src="weblets/./img/new_funk1.png" alt=" " /></p>
<ul>
<li>
<p>Enter an Application Name. It's used in generating a unique subdomain on one of the gateways on the network alongside your twin ID. Ex. <strong><em>fw100myfunk</em>.gent02.dev.grid.tf</strong></p>
</li>
<li>
<p>Enter administrator information including <strong>Username</strong>, <strong>Email</strong> and <strong>Password</strong>. This admin user will have full permission on the deployed instance.</p>
</li>
<li>
<p>Select a capacity package:</p>
<ul>
<li><strong>Minimum</strong>: {cpu: 2, memory: 1024, diskSize: 50 }</li>
<li><strong>Standard</strong>: {cpu: 2, memory: 1024 * 2, diskSize: 100 }</li>
<li><strong>Recommended</strong>: {cpu: 4, memory: 1024 * 4, diskSize: 250 }</li>
<li>Or choose a <strong>Custom</strong> plan</li>
</ul>
</li>
<li>
<p>Choose a gateway node to deploy your Funkwhale instance on.</p>
</li>
<li>
<p>Select a node to deploy your Funkwhale instance on.</p>
<ul>
<li>
<p>Either use the <strong>Capacity Filter</strong>. Which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick.</p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
</ul>
</li>
</ul>
<p>After that is done you can see a list of all of your deployed instances</p>
<p><img src="weblets/./img/funkwhale2.png" alt=" " /></p>
<p>Click on <em><strong>Visit</strong></em> to go to the homepage of your Funkwhale instance!</p>
<p><img src="weblets/./img/funkwhale3.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="peertube"><a class="header" href="#peertube">Peertube</a></h1>
<p><a href="https://joinpeertube.org/">Peertube</a> aspires to be a decentralized and free/libre alternative to video broadcasting services.</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a></li>
<li>Click on the <strong>Peertube</strong> tab</li>
</ul>
<p><strong>Process</strong></p>
<p><img src="weblets/./img/new_peer1.png" alt=" " /></p>
<ul>
<li>
<p>Enter an Application Name. It's used in generating a unique subdomain on one of the gateways on the network alongside your twin ID. 
the applied format <code>&lt;solution-code&gt;&lt;twin-id&gt;&lt;solution-name&gt;.&lt;gateway-subdomain&gt;</code> Ex. <strong><em>pt100peerprod</em>.gent02.dev.grid.tf</strong></p>
</li>
<li>
<p>Enter an email and password which will be used for the admin login.</p>
</li>
<li>
<p>Select a capacity package:</p>
<ul>
<li><strong>Minimum</strong>: { cpu: 1, memory: 1024, diskSize: 100 }</li>
<li><strong>Standard</strong>: { cpu: 2, memory: 1024 * 2, diskSize: 250 }</li>
<li><strong>Recommended</strong>: { cpu: 4, memory: 1024 * 4, diskSize: 500 }</li>
<li>Or choose a <strong>Custom</strong> plan</li>
</ul>
</li>
<li>
<p>Choose a gateway node to deploy your Subsquid instance on.</p>
</li>
<li>
<p>Select a node to deploy your Peertube instance on.</p>
<ul>
<li>
<p>Either use the <strong>Capacity Filter</strong>. Which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick.</p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
</ul>
</li>
</ul>
<p>After that is done you can see a list of all of your deployed instances</p>
<p><img src="weblets/./img/weblet_peertube_listing.png" alt=" " /></p>
<p>Click on <em><strong>Visit</strong></em> to go to the homepage of your Peertube instance!</p>
<p><img src="weblets/./img/weblet_peertube_instance.png" alt=" " /></p>
<blockquote>
<p>Please note it may take sometime to be ready</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="taiga"><a class="header" href="#taiga">Taiga</a></h1>
<p><a href="https://www.taiga.io/">Taiga</a> is the project management tool for multi-functional agile teams. It has a rich feature set and at the same time it is very simple to start with through its intuitive user interface.</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a></li>
<li>Click on the <strong>Taiga</strong> tab</li>
</ul>
<p><strong>Process</strong> :</p>
<p><img src="weblets/./img/taiga1.png" alt=" " /></p>
<ul>
<li>
<p>Enter an Application Name. It's used in generating a unique subdomain on one of the gateways on the network alongside your twin ID. Ex. <strong><em>tg98taigar</em>.gent02.dev.grid.tf</strong></p>
</li>
<li>
<p>Enter administrator information including <strong>Username</strong>, <strong>Email</strong> and <strong>Password</strong>. This admin user will have full permission on the deployed instance.</p>
</li>
<li>
<p>Select a capacity package:</p>
<ul>
<li><strong>Minimum</strong>: { cpu: 2, memory: 1024 * 2, diskSize: 100 }</li>
<li><strong>Standard</strong>: { cpu: 2, memory: 1024 * 4, diskSize: 150 }</li>
<li><strong>Recommended</strong>: { cpu: 4, memory: 1024 * 4, diskSize: 250 }</li>
<li>Or choose a <strong>Custom</strong> plan</li>
</ul>
</li>
<li>
<p>Choose a gateway node to deploy your Taiga instance on.</p>
</li>
<li>
<p>Select a node to deploy your Taiga instance on.</p>
<ul>
<li>
<p>Either use the <strong>Capacity Filter</strong>. Which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick.</p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
</ul>
</li>
</ul>
<p>There's also an optional <strong>Mail Server</strong> tab if you'd like to have your Taiga instance configured with an SMTP server.</p>
<p><img src="weblets/./img/taiga4.png" alt=" " /></p>
<p>After that is done you can see a list of all of your deployed instances</p>
<p><img src="weblets/./img/taiga5.png" alt=" " /></p>
<p>Click on <em><strong>Visit</strong></em> to go to the homepage of your Funkwhale instance!</p>
<p><img src="weblets/./img/taiga6.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="owncloud"><a class="header" href="#owncloud">ownCloud</a></h1>
<p><a href="https://owncloud.com/">ownCloud</a> develops and provides open-source software for content collaboration, allowing teams to easily share and work on files seamlessly regardless of device or location.</p>
<ul>
<li>Make sure you have an activated <a href="weblets/._profile_manager.html">profile</a></li>
<li>Click on the <strong>Owncloud</strong> tab</li>
</ul>
<p><strong>Process</strong> :</p>
<p><img src="weblets/./img/owncloud1.png" alt=" " /></p>
<ul>
<li>
<p>Enter an Application Name. It's used in generating a unique subdomain on one of the gateways on the network alongside your twin ID. Ex. <strong><em>oc98newcloud</em>.gent02.dev.grid.tf</strong></p>
</li>
<li>
<p>Enter administrator information including <strong>Username</strong> and <strong>Password</strong>. This admin user will have full permission on the deployed instance.</p>
</li>
<li>
<p>Select a capacity package:</p>
<ul>
<li><strong>Minimum</strong>: { cpu: 2, memory: 1024 * 16, diskSize: 250 }</li>
<li><strong>Standard</strong>: { cpu: 2, memory: 1024 * 16, diskSize: 500 }</li>
<li><strong>Recommended</strong>: { cpu: 4, memory: 1024 * 16, diskSize: 1000 }</li>
<li>Or choose a <strong>Custom</strong> plan</li>
</ul>
</li>
<li>
<p>Choose a gateway node to deploy your Owncloud instance on.</p>
</li>
<li>
<p>Select a node to deploy your OwnCloud instance on.</p>
<ul>
<li>
<p>Either use the <strong>Capacity Filter</strong>. Which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick.</p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
</ul>
</li>
</ul>
<p>There's also an optional <strong>Mail Server</strong> tab if you'd like to have your Owncloud instance configured with an SMTP server.</p>
<p><img src="weblets/./img/owncloud4.png" alt=" " /></p>
<p>After that is done you can see a list of all of your deployed instances</p>
<p><img src="weblets/./img/owncloud5.png" alt=" " /></p>
<p>Click on <em><strong>Visit</strong></em> to go to the homepage of your Owncloud instance! If you'd like to be able to use TFConnect to login, you need to login using your admin username and password first and allow TFConnect login from settings.</p>
<p><img src="weblets/./img/owncloud6.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="discourse"><a class="header" href="#discourse">Discourse</a></h1>
<p><a href="https://www.discourse.org/">Discourse</a> is the 100% open source discussion platform built for the next decade of the Internet. Use it as a mailing list, discussion forum, long-form chat room, and more!</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a></li>
<li>Click on the <strong>Discourse</strong> tab</li>
</ul>
<p><strong>Process</strong> :</p>
<p><img src="weblets/./img/discourse1.png" alt=" " /></p>
<ul>
<li>
<p>Enter an Application Name. It's used in generating a unique subdomain on one of the gateways on the network alongside your twin ID. Ex. <strong><em>dc98newdisc</em>.gent02.dev.grid.tf</strong></p>
</li>
<li>
<p>Enter administrator information including <strong>Email</strong>. This admin will have full permission on the deployed instance.</p>
</li>
<li>
<p>Select a capacity package:</p>
<ul>
<li><strong>Minimum</strong>: {cpu: 1, memory: 1024 * 2, diskSize: 10 }</li>
<li><strong>Standard</strong>: {cpu: 2, memory: 1024 * 2, diskSize: 50 }</li>
<li><strong>Recommended</strong>: {cpu: 4, memory: 1024 * 4, diskSize: 100 }</li>
<li>Or choose a <strong>Custom</strong> plan</li>
</ul>
</li>
<li>
<p>Choose a gateway node to deploy your Discourse instance on.</p>
</li>
<li>
<p>Select a node to deploy your Discourse instance on.</p>
<ul>
<li>
<p>Either use the <strong>Capacity Filter</strong>. Which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick.</p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
</ul>
</li>
</ul>
<p>Unlike other solutions, Discourse requires that you have an SMTP server. So make sure you fill the fields in the <strong>Mail Server</strong> tab in order to deploy your instance successfully.</p>
<p><img src="weblets/./img/discourse4.png" alt=" " /></p>
<p>After that is done you can see a list of all of your deployed instances</p>
<p><img src="weblets/./img/discourse5.png" alt=" " /></p>
<p>Click on <em><strong>Visit</strong></em> to go to the homepage of your Discourse instance! </p>
<p><img src="weblets/./img/discourse6.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mattermost"><a class="header" href="#mattermost">Mattermost</a></h1>
<p><a href="https://mattermost.com/">Mattermost</a> A single point of collaboration. Designed specifically for digital operations.</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a></li>
<li>Click on the <strong>Mattermost</strong> tab</li>
</ul>
<p><strong>Process</strong> :</p>
<p><img src="weblets/./img/mattermost1.png" alt=" " /></p>
<ul>
<li>
<p>Enter an Application Name. It's used in generating a unique subdomain on one of the gateways on the network alongside your twin ID. Ex. <strong><em>matter</em>.gent02.dev.grid.tf</strong></p>
</li>
<li>
<p>Select a capacity package:</p>
<ul>
<li><strong>Minimum</strong>: {cpu: 1, memory: 1024 * 2, diskSize: 10 }</li>
<li><strong>Standard</strong>: {cpu: 2, memory: 1024 * 4, diskSize: 50 }</li>
<li><strong>Recommended</strong>: {cpu: 4, memory: 1024 * 4, diskSize: 100 }</li>
<li>Or choose a <strong>Custom</strong> plan</li>
</ul>
</li>
<li>
<p>Choose a gateway node to deploy your Mattermost instance on.</p>
</li>
<li>
<p>Select a node to deploy your MatterMost instance on.</p>
<ul>
<li>
<p>Either use the <strong>Capacity Filter</strong>. Which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick.</p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
</ul>
</li>
<li>
<p>There's also an optional <strong>Mail Server</strong> tab if you'd like to have your Mattermost instance configured with an SMTP server.</p>
<p><img src="weblets/./img/mattermost3.png" alt=" " /></p>
</li>
</ul>
<p>After that is done you can see a list of all of your deployed instances</p>
<p><img src="weblets/./img/mattermost4.png" alt=" " /></p>
<p>Click on <em><strong>Visit</strong></em> to go to the homepage of your Mattermost instance! You need to login using TFConnect so make sure you download the <em>TFConnect</em> app from your App Store.</p>
<p><img src="weblets/./img/mattermost5.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="presearch"><a class="header" href="#presearch">Presearch</a></h1>
<p><a href="https://www.presearch.io/">Presearch</a> is a community-powered, decentralized search engine that provides better results while protecting your privacy and rewarding you when you search. This weblet deploys a Presearch node. Presearch Nodes are used to process user search requests, and node operators earn Presearch PRE tokens for joining and supporting the network.</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a></li>
<li>Click on the <strong>Presearch</strong> tab</li>
</ul>
<p><strong>Process</strong></p>
<p><img src="weblets/./img/presearch1.png" alt=" " /></p>
<ul>
<li>
<p>Enter an instance name.</p>
</li>
<li>
<p>You need to sign up on Presearch in order to get your <em>Presearch Registration Code</em>. To sign up, go to <a href="https://presearch.org/">Presearch</a>, create your account and then head to your <a href="https://nodes.presearch.org/dashboard">dashboard</a> to find your registration code.</p>
</li>
<li>
<p>Choose a node to deploy your Presearch instance on.</p>
</li>
<li>
<p>Either use the <strong>Capacity Filter</strong> which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick.</p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
</ul>
<h3 id="now-what-if-you-already-have-a-presearch-node-deployed-somewhere-and-would-like-to-migrate-to-threefold"><a class="header" href="#now-what-if-you-already-have-a-presearch-node-deployed-somewhere-and-would-like-to-migrate-to-threefold">Now what if you already have a Presearch node deployed somewhere and would like to migrate to Threefold?</a></h3>
<p>We got you! All you need to do is:</p>
<ol>
<li>Login to your old server that has your node via SSH.</li>
<li>Run <code>docker cp presearch-node:/app/node/.keys presearch-node-keys</code> in order to generate your key-pair.</li>
<li>Head to the <em>Restore</em> tab in the Presearch weblet and paste your key-pair in the fields below and you'll be good to deploy!</li>
</ol>
<p><img src="weblets/./img/presearch6.png" alt=" " /></p>
<p>After that is done you can see a list of all of your deployed instances</p>
<p><img src="weblets/./img/presearch4.png" alt=" " /></p>
<p>Now head to your <a href="https://nodes.presearch.org/dashboard">dashboard</a>again and scroll down to <strong>Current Nodes</strong>, you'll see your newly created node up and connected!</p>
<p><img src="weblets/./img/presearch5.png" alt=" " /></p>
<p>You should visit Presearch's <a href="https://docs.presearch.org/">docs</a> if you want to learn more!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="casperlabs"><a class="header" href="#casperlabs">CasperLabs</a></h1>
<p><a href="https://casperlabs.io/">Casper Network</a> is a blockchain protocol built from the ground up to remain true to core Web3 principles and adapt to the needs of our evolving world.</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a></li>
<li>Click on the <strong>Casperlabs</strong> tab</li>
</ul>
<p><strong>Process</strong> :</p>
<p><img src="weblets/./img/casper1.png" alt=" " /></p>
<ul>
<li>
<p>Enter an Application Name. It's used in generating a unique subdomain on one of the gateways on the network alongside your twin ID. Ex. <strong><em>cl98casp</em>.gent02.dev.grid.tf</strong></p>
</li>
<li>
<p>Select a capacity package:</p>
<ul>
<li><strong>Minimum</strong>: {cpu: 1, memory: 1024 * 4, diskSize: 100 }</li>
<li><strong>Standard</strong>: {cpu: 2, memory: 1024 * 16, diskSize: 500 }</li>
<li><strong>Recommended</strong>: {cpu: 4, memory: 1024 * 32, diskSize: 1000 }</li>
<li>Or choose a <strong>Custom</strong> plan</li>
</ul>
</li>
<li>
<p>Choose a gateway node to deploy your CasperLabs instance on.</p>
</li>
<li>
<p>Choose a node to deploy your Casperlabs instance on.</p>
<ul>
<li>
<p>Either use the <strong>Capacity Filter</strong>. Which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick.</p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
</ul>
</li>
</ul>
<p>After that is done you can see a list of all of your deployed instances</p>
<p><img src="weblets/./img/casper4.png" alt=" " /></p>
<p>Click on <em><strong>Visit</strong></em> to go to the homepage of your Casperlabs instance! The node takes a long time in order for the RPC service to be ready so be patient!</p>
<p><img src="weblets/./img/casper5.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nodepilot"><a class="header" href="#nodepilot">NodePilot</a></h1>
<p>This is a simple instance of upstream node-pilot: https://nodepilot.tech</p>
<ul>
<li>Make sure you have an activated <a href="weblets/./weblets_profile_manager.html">profile</a></li>
<li>Click on the <strong>Node Pilot</strong> tab</li>
</ul>
<p><strong>Process</strong></p>
<p><img src="weblets/./img/nodePilot_1.png" alt=" " /></p>
<ul>
<li>
<p>Fill in the instance name: it's used to reference the node-pilot in the future.</p>
</li>
<li>
<p>Minimum CPU allowed is 8 cores and minimum memory allowed is 8192.</p>
</li>
<li>
<p>Select a node to deploy your node-pilot instance on.</p>
</li>
<li>
<p>Either use the <strong>Capacity Filter</strong>. Which simply lets you pick a <em>Farm</em> and <em>Country</em>, after clicking on <em>Apply filters and suggest nodes</em> then it lists available nodes with these preferences and you pick.</p>
</li>
<li>
<p>Or use <strong>Manual</strong> and type a specific node number to deploy on.</p>
</li>
<li>
<p>When using the <a href="https://hub.grid.tf/tf-official-vms/node-pilot-zdbfs.flist">flist</a> you get a node pilot instance ready out-of-box. You need to get a public ipv4 to get it to works.</p>
</li>
</ul>
<p>After that is done you can see a list of all of your deployed instances</p>
<p><img src="weblets/./img/nodePilot_2.png" alt=" " /></p>
<p>Click on <em><strong>Visit</strong></em> to go to the registeration page of your Node Pilot instance!</p>
<p><img src="weblets/./img/nodePilot_3.png" alt=" " /></p>
<p>You can go to <a href="https://publicip">https://publicip</a> and configure your node-pilot. You can upload a backup to the VM via ssh as well if you have a backup of a previous instance.</p>
<p>What change compared to upstream node-pilot, we have out-of-box a transparent pre-filled blockchain database for some blochain (currently Fuse and Pokt as proof-of-concept). You can start one of theses blockchain in no-time and it will be automatically nearly sync already without the requirement of the full space locally nor downloading everything and killing bandwidth.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="dashboard"><a class="header" href="#dashboard">Dashboard</a></h2>
<p>The dashboard is a unified admin interface for everything related to Threefold Grid such as:</p>
<ul>
<li>
<p>Twin/Farm Management: </p>
<ul>
<li>Twin management
<img src="dashboard/./img/dashboard_portal_twin.png" alt="twin mgmt" /></li>
<li>Farms management
<img src="dashboard/./img/dashboard_portal_farms.png" alt="farm mgmt" /></li>
</ul>
</li>
<li>
<p>TFChain DAO
<img src="dashboard/./img/dashboard_dao.png" alt="dao" /></p>
</li>
<li>
<p>Token management</p>
<ul>
<li>Swapping with Binance and Stellar
<img src="dashboard/./img/dashboard_swap.png" alt="swap" /></li>
<li>Transferring money to other tfchain user
<img src="dashboard/./img/dashboard_portal_transfer.png" alt="transfer" /></li>
</ul>
</li>
<li>
<p>Explorer:
more about the explorer <a href="dashboard/./explorer/explorer_home.html">here</a></p>
<ul>
<li>Exploring Threefold nodes
<img src="dashboard/./img/dashboard_explorer_nodes.png" alt="nodes" /></li>
<li>Exploring farms
<img src="dashboard/./img/dashboard_explorer_farms.png" alt="farms" /></li>
<li>Grid statistics
<img src="dashboard/./img/dashboard_explorer_statistics.png" alt="stats" /></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="explorer"><a class="header" href="#explorer">Explorer</a></h1>
<p>The explorer allows you to find nodes in the grid which fulfil your requirements</p>
<h2 id="current-explorer-list"><a class="header" href="#current-explorer-list">Current Explorer List</a></h2>
<ul>
<li><a href="https://dashboard.dev.grid.tf">Devnet Explorer</a></li>
<li><a href="https://dashboard.qa.grid.tf">Qanet Explorer</a></li>
<li><a href="https://dashboard.test.grid.tf">Testnet Explorer</a></li>
<li><a href="https://dashboard.grid.tf">Mainnet explorer</a> </li>
</ul>
<h2 id="explorerui"><a class="header" href="#explorerui">ExplorerUI</a></h2>
<p><img src="dashboard/explorer/../img/explorer_basics_.png" alt="explorer" /></p>
<p>Explorer UI helps exploring the capacity connected to Threefold Grid. Searching for nodes, farms, gateways .. etc. It also supports Dark mode for more comfortable navigation.</p>
<h3 id="statistics"><a class="header" href="#statistics">Statistics</a></h3>
<p><img src="dashboard/explorer/../img/explorer_basics_2.png" alt="statistics" /></p>
<p>Here you can see generic overview about</p>
<ul>
<li>number of farms</li>
<li>number of nodes</li>
<li>number of gateways</li>
<li>number of twins</li>
<li>number of contracts</li>
<li>the capacity CRU, SRU, HRU, MRU</li>
<li>the number of public IPs available</li>
</ul>
<h3 id="exploring-farms"><a class="header" href="#exploring-farms">Exploring Farms</a></h3>
<p>You can see a list of all farms with filtering options
<img src="dashboard/explorer/../img/explorer_farms.png" alt="explorer_farms" /></p>
<h3 id="farm-details"><a class="header" href="#farm-details">Farm Details</a></h3>
<p>Click on a farm shows up the farm details information
<img src="dashboard/explorer/../img/explorer_farm_details.png" alt="explorer_farm_details" /></p>
<h3 id="exploring-nodes"><a class="header" href="#exploring-nodes">Exploring Nodes</a></h3>
<p>Explorer UI allows exploring the nodes and filtering them by many filtering option
<img src="dashboard/explorer/../img/explorer_basics_.png" alt="exporer_nodes" /></p>
<p>You can see all of the node details by clicking on a node record.</p>
<p><img src="dashboard/explorer/../img/node_detail_.png" alt=" " />
<img src="dashboard/explorer/../img/node_detail_1.png" alt=" " /></p>
<hr />
<p>Lightmode is supported. Click on the icon in the upper right corner of the screen.</p>
<p><img src="dashboard/explorer/../img/explorer_darkmode.png" alt="explorer_lightmode" /></p>
<p>Can also see a map of how the capacity is distributed.
A map is available with a global overview, showing in which countries capacity is offered.</p>
<p><img src="dashboard/explorer/../img/explorer_nodes_distribution.png" alt="capacity_distribution" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="find-capacity-on-the-grid-1"><a class="header" href="#find-capacity-on-the-grid-1">Find Capacity on the Grid</a></h1>
<p><img src="dashboard/explorer/../img/explorer_basics_.png" alt=" " />
<img src="dashboard/explorer/../img/explorer_basics_2.png" alt=" " /></p>
<p>Before you can deploy on the TFGrid you need to find a node which fits your requirements, in terms of location, available resources, ... .</p>
<h2 id="use-the-explorer-you-need-for-your-chosen-tfgrid-net-1"><a class="header" href="#use-the-explorer-you-need-for-your-chosen-tfgrid-net-1">Use the explorer you need for your chosen tfgrid net</a></h2>
<p>!!!include:explorer_list</p>
<h2 id="use-filter-to-select-country--requirement-for-public-ips-1"><a class="header" href="#use-filter-to-select-country--requirement-for-public-ips-1">Use filter to select Country &amp; Requirement for Public IP's</a></h2>
<p><img src="dashboard/explorer/../img/explorer_find_country_pubip.png" alt=" " /></p>
<p>In this case 17 nodes are found with that requirement.</p>
<p>Do note you have to put</p>
<ul>
<li><code>&gt;=1</code> in the publicip's part, means at least one public ip address.</li>
</ul>
<p>Remember the ID, this is the number you will need to specify your node.</p>
<h2 id="important-1"><a class="header" href="#important-1">Important</a></h2>
<ul>
<li>make sure there is enough capacity on the node for your workload requirements</li>
<li>make sure you use the right explorer, each net has a different explorer.</li>
</ul>
<h2 id="node-details-1"><a class="header" href="#node-details-1">Node details</a></h2>
<p><img src="dashboard/explorer/../img/node_detail_.png" alt=" " />
<img src="dashboard/explorer/../img/node_detail_1.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="graphql-support-for-tfgrid"><a class="header" href="#graphql-support-for-tfgrid">GraphQL support for TFGrid</a></h1>
<p>Info on TFChain is indexed over GraphQL and is available for queries.</p>
<ul>
<li><a href="https://graphql.dev.grid.tf/graphql">Devnet GraphQL</a></li>
<li><a href="https://graphql.qa.grid.tf/graphql">Qanet GraphQL</a></li>
<li><a href="https://graphql.test.grid.tf/graphql">Testnet GraphQL</a></li>
<li><a href="https://graphql.grid.tf/graphql">Mainnet GraphQL</a></li>
</ul>
<p>The GraphQL interface used to query TF-Chain is the one offered by <a href="https://docs.subsquid.io/">Subsquid</a>. See more info on supported queries <a href="https://docs.subsquid.io/queries">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tf-chain-portal"><a class="header" href="#tf-chain-portal">TF-Chain Portal</a></h1>
<h2 id="currently-on"><a class="header" href="#currently-on">Currently on:</a></h2>
<ul>
<li><a href="https://dashboard.dev.grid.tf/">Devnet</a></li>
<li><a href="https://dashboard.qa.grid.tf/">Qanet</a></li>
<li><a href="https://dashboard.test.grid.tf/">Testnet</a></li>
<li><a href="https://dashboard.grid.tf/">Mainnet</a></li>
</ul>
<p><img src="dashboard/portal/../img/dashboard_portal_account.png" alt=" " title=":size=600" /></p>
<h2 id="installation-and-activation"><a class="header" href="#installation-and-activation">Installation and activation</a></h2>
<p>Follow the <a href="dashboard/portal/../../getstarted/tfgrid3_getstarted.html">Getting Started</a> to make sure your tfchain account set up.</p>
<h2 id="transfer-tft"><a class="header" href="#transfer-tft">Transfer TFT</a></h2>
<ul>
<li><a href="dashboard/portal/./dashboard_portal_ui_tokens.html">Manage your Tokens on TFChain</a> e.g. transfer TFT to TFChain</li>
</ul>
<h2 id="be-a-farmer-manage-your-information"><a class="header" href="#be-a-farmer-manage-your-information">Be a Farmer, manage your information</a></h2>
<ul>
<li><a href="dashboard/portal/./dashboard_portal_ui_farming.html">Activate your farmer account and nodes on TFChain</a></li>
</ul>
<h2 id="capacity-explorer"><a class="header" href="#capacity-explorer">Capacity Explorer</a></h2>
<p>In the upper right corner, you can click on <code>EXPLORER</code> to get a view of all capacity connected to TFGrid v3. For more info, see <a href="dashboard/portal/../explorer/explorer_home.html">here</a>.</p>
<h2 id="modify-you-twin-info"><a class="header" href="#modify-you-twin-info">Modify you Twin Info</a></h2>
<p>A twinIP can be edited at any moment. Simply update the field and sign afterwards.
Probably not needed right now.</p>
<h2 id="dedicated-nodes"><a class="header" href="#dedicated-nodes">Dedicated Nodes</a></h2>
<p>Rent a whole node to serve your solutions.
<a href="dashboard/portal/./dashboard_portal_dedicated_nodes.html">Dedicated Nodes</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-a-farm"><a class="header" href="#create-a-farm">Create a Farm</a></h1>
<p>If you want to start farming, you need a farmID, the ID of the farm that is owning the hardware node(s) you connect to the TFGrid.</p>
<h2 id="currently-on-1"><a class="header" href="#currently-on-1">Currently on:</a></h2>
<ul>
<li><a href="https://dashboard.dev.grid.tf/">Devnet</a></li>
<li><a href="https://dashboard.qa.grid.tf/">Qanet</a></li>
<li><a href="https://dashboard.test.grid.tf/">Testnet</a></li>
<li><a href="https://dashboard.grid.tf/">Mainnet</a></li>
</ul>
<p>Click <code>CREATE FARM</code> and choose a name.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_farm.png" alt=" " title=":size=600" /></p>
<p><img src="dashboard/portal/../img/dashboard_portal_create_farm.png" alt=" " title=":size=300" /></p>
<p>Click <code>Submit</code> and sign the action.</p>
<p>The farm is by default set up as 'DIY'. A farm can become certified through certification program.
Also a pricing policy is defined. Pricing policy is currently the same for all farms, the field is created for future use.</p>
<h2 id="add-a-public-ip-to-your-farm"><a class="header" href="#add-a-public-ip-to-your-farm">Add a public IP to your Farm</a></h2>
<p>If you have public IPv4 addresses available that can be used for usage on the TFGrid, you can add them in your farm.
Click <code>ADD IP</code>, specify the addresses, the gateway and click <code>CREATE</code>.
You can add them one by one or using range of IPs.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_ip_add.png" alt=" " title=":size=600" />
<img src="dashboard/portal/../img/dashboard_portal_ip_add_detail.png" alt=" " title=":size=300" />
<img src="dashboard/portal/../img/dashboard_portal_ip_add_detail_range.png" alt=" " title=":size=300" /></p>
<p>Deleting IPv4 addresses is also possible here. The <code>Deployed Contract ID</code> gives an indication of whether an IP is currently used. If it is 0, it is safe to remove it.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_ip_result.png" alt=" " title=":size=400" /></p>
<h2 id="add-a-stellar-address-for-payout"><a class="header" href="#add-a-stellar-address-for-payout">Add a Stellar address for payout</a></h2>
<p>In a first phase, farming of tokens still results in payout on the Stellar network. So to get the farming reward, a Stellar address needs to be provided.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_farm0.png" alt=" " title=":size=600" /></p>
<p><img src="dashboard/portal/../img/dashboard_portal_stellar.png" alt=" " title=":size=400" /></p>
<h2 id="generate-your-node-bootstrap-image"><a class="header" href="#generate-your-node-bootstrap-image">Generate your node bootstrap image</a></h2>
<p>Once you know your farmID, you can set up your node on TFGrid3. Click on <code>View bootstrap</code>.</p>
<p>After booting a node, the info will become available in your portal, including the status info along with the minting and fixup receipts.</p>
<p><img src="dashboard/portal/../img/dashboard_portal_node_info.png" alt=" " title=":size=600" /></p>
<p>Clicking on the node statistics will open up a calendar where you can view the periods the node was minting or undergoing a fixup. Clicking on the periods will show a popup with the start and end datetimes, receipt hash and the amount of TFTs minted (if it is a minting receipt).</p>
<p><img src="dashboard/portal/../img/dashboard_portal_ui_nodes_minting.png" alt=" " title=":size=600" /></p>
<p>You can also download a single node's receipts using the <code>Download Receipts</code> button within the node statistics. Moreover, you can download all of the nodes' receipts using the <code>Download Receipts</code> button on the top left corner of the farm nodes table.</p>
<h2 id="capacity-explorer-1"><a class="header" href="#capacity-explorer-1">Capacity Explorer</a></h2>
<p>In the upper right corner, you can click on <code>EXPLORER</code> to get a view of all capacity connected to TFGrid v3. For more info, see <a href="dashboard/portal/explorer_home">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="what-is-dedicated-node"><a class="header" href="#what-is-dedicated-node">What is Dedicated Node?</a></h2>
<p>A user can reserve an entire node then use it exclusively to deploy solutions.</p>
<h3 id="discription"><a class="header" href="#discription">Discription</a></h3>
<ul>
<li>Node reserved with deploying a <code>RentContract</code> on this node. node can has only one rentContract.</li>
<li>When a user create a RentContract against a node, the grid validate that there are no other active contracts on that node on the creation.</li>
<li>Once a RentContract is created, the grid can only accept contracts on this node from the tenant.</li>
<li>Only workloads from the tenant are accepted</li>
</ul>
<h3 id="billing--pricing"><a class="header" href="#billing--pricing">Billing &amp; Pricing</a></h3>
<ul>
<li>Once a node is rented, there is a fixed charge billed to the tenant regardless of deployed workloads.</li>
<li>Any subsequent NodeContract deployed on a node where a rentContract is active (and the same user is creating the nodeContracts) can be excluded from billing (apart from public ip and network usage).</li>
<li>Reseved Disounts for renting a node on TFGrid internet capacity
<ul>
<li>70% for dedicated node (TF Pricing policies)</li>
<li>a second level discount up to 60% for balance level see <a href="https://library.threefold.me/info/threefold/#/tfgrid/grid/pricing?id=discount-levels">Discount Levels</a></li>
</ul>
</li>
</ul>
<h3 id="usage-1"><a class="header" href="#usage-1">Usage</a></h3>
<ul>
<li>
<p>See list of all dedicated node on <code>Dedicated Nodes</code> tab on the portal.</p>
<p><img src="dashboard/portal/../img/dedicated_nodes.png" alt=" " /></p>
<ul>
<li>
<p>Hover on price to see discount</p>
</li>
<li>
<p>Expand row to see more info on the node:</p>
<p><img src="dashboard/portal/../img/node_info.png" alt=" " /></p>
<ul>
<li>Resources</li>
<li>Location</li>
<li>Possible Public Ips <em>this dependes on the farm it belongs to</em></li>
</ul>
</li>
<li>
<p>You can see the nodes in 3 states:</p>
<ul>
<li>Free</li>
<li>Reserved <em>Owned by current twin</em></li>
<li>Taken <em>Owned by another twin</em></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Reserve a node:</p>
<ul>
<li>
<p>If node is not rented by another twin you can simply click reserve.</p>
</li>
<li>
<p>Type your password on the polkadot pop up window.</p>
</li>
<li>
<p>Wait for the pop up message said <code>Transaction succeeded</code></p>
<p><img src="dashboard/portal/../img/rented_succeeded.png" alt=" " /></p>
</li>
</ul>
</li>
<li>
<p>Unreserve a node:</p>
<ul>
<li>Simply as reserving but another check will be done to check you don't have any active workoad on the node before unreserving.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="flist-hub"><a class="header" href="#flist-hub">Flist HUB</a></h1>
<p><img src="flist_hub/./img/hub_flist.png" alt=" " /></p>
<ul>
<li>Allows you to convert .tar.gz to an flist</li>
<li>Allows you to convert a docker image.</li>
<li>Allows you to customize an flist.</li>
</ul>
<h1 id="convert-docker-to-flist"><a class="header" href="#convert-docker-to-flist">Convert Docker To Flist</a></h1>
<ol>
<li>Upload the Docker image to Docker Hub with the following command:</li>
</ol>
<pre><code class="language-bash">docker push &lt;image_name&gt;
</code></pre>
<ol start="2">
<li>
<p>Navigate to the docker converter link: https://hub.grid.tf/docker-convert
<img src="flist_hub/./img/docker_convert.png" alt=" " /></p>
</li>
<li>
<p>Copy the name of the uploaded Docker image to the Docker Image Name field.</p>
</li>
<li>
<p>Then press the convert button.</p>
</li>
</ol>
<p>When the image is ready, some information will be displayed.</p>
<p><img src="flist_hub/./img/flist_ready.png" alt=" " /></p>
<p>To Navigate to the created Flist Either search with the newly created file name in the search tab.</p>
<p><img src="flist_hub/./img/search.png" alt=" " /></p>
<p>Or Navigate to your repository in the contributers section from the Zero-Os Hub and navigate to the newly created Flist.</p>
<p>Then press the preview button to display the Flist's url and some other data.</p>
<p><img src="flist_hub/./img/preview.png" alt=" " /></p>
<div style="break-before: page; page-break-before: always;"></div><p>Supported flists</p>
<h2 id="alpine"><a class="header" href="#alpine">alpine</a></h2>
<ul>
<li><a href="https://hub.grid.tf/tf-official-apps/threefoldtech-alpine-3.flist">Alpine</a></li>
</ul>
<h3 id="entrypoint"><a class="header" href="#entrypoint">entrypoint</a></h3>
<p><code>/entrypoint.sh</code></p>
<h3 id="env-vars"><a class="header" href="#env-vars">env vars</a></h3>
<ul>
<li><code>SSH_KEY</code></li>
</ul>
<h2 id="ubuntu"><a class="header" href="#ubuntu">ubuntu</a></h2>
<ul>
<li><a href="https://hub.grid.tf/tf-official-apps/threefoldtech-ubuntu-20.04.flist">Ubuntu</a></li>
</ul>
<h3 id="entrypoint-1"><a class="header" href="#entrypoint-1">entrypoint</a></h3>
<p><code>/init.sh</code></p>
<h3 id="env-vars-1"><a class="header" href="#env-vars-1">env vars</a></h3>
<ul>
<li><code>SSH_KEY</code></li>
</ul>
<h2 id="centos"><a class="header" href="#centos">centos</a></h2>
<ul>
<li><a href="https://hub.grid.tf/tf-official-apps/threefoldtech-centos-8.flist">CentOS</a></li>
</ul>
<h3 id="entrypoint-2"><a class="header" href="#entrypoint-2">entrypoint</a></h3>
<p><code>/entrypoint.sh</code></p>
<h3 id="env-vars-2"><a class="header" href="#env-vars-2">env vars</a></h3>
<ul>
<li><code>SSH_KEY</code></li>
</ul>
<h2 id="k3s"><a class="header" href="#k3s">k3s</a></h2>
<ul>
<li><a href="https://hub.grid.tf/ahmed_hanafy_1/ahmedhanafy725-k3s-latest.flist">K3s</a> </li>
</ul>
<h3 id="entrypoint-3"><a class="header" href="#entrypoint-3">entrypoint</a></h3>
<p><code>/sbin/zinit init</code></p>
<h3 id="env-vars-3"><a class="header" href="#env-vars-3">env vars</a></h3>
<ul>
<li><code>SSH_KEY</code></li>
<li><code>K3S_TOKEN</code></li>
<li><code>K3S_DATA_DIR</code></li>
<li><code>K3S_FLANNEL_IFACE</code></li>
<li><code>K3S_NODE_NAME</code></li>
<li><code>K3S_URL</code> <code>https://${masterIp}:6443</code></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
